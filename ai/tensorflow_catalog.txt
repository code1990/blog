>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Tensorflow：实战Google深度学习框架
第1章 深度学习简介 1
1.1 人工智能、机器学习与深度学习 2
1.2 深度学习的发展历程 7
1.3 深度学习的应用 10
1.3.1 计算机视觉 10
1.3.2 语音识别 14
1.3.3 自然语言处理 15
1.3.4 人机博弈 18
1.4 深度学习工具介绍和对比 19
小结 23
第2章 TensorFlow环境搭建 25
2.1 TensorFlow的主要依赖包 25
2.1.1 Protocol Buffer 25
2.1.2 Bazel 27
2.2 TensorFlow安装 29
2.2.1 使用Docker安装 30
2.2.2 使用pip安装 32
2.2.3 从源代码编译安装 33
2.3 TensorFlow测试样例 37
小结 38
第3章 TensorFlow入门 40
3.1 TensorFlow计算模型——计算图 40
3.1.1 计算图的概念 40
3.1.2 计算图的使用 41
3.2 TensorFlow数据模型——张量 43
3.2.1 张量的概念 43
3.2.2 张量的使用 45
3.3 TensorFlow运行模型——会话 46
3.4 TensorFlow实现神经网络 48
3.4.1 TensorFlow游乐场及神经网络简介 48
3.4.2 前向传播算法简介 51
3.4.3 神经网络参数与TensorFlow变量 54
3.4.4 通过TensorFlow训练神经网络模型 58
3.4.5 完整神经网络样例程序 62
小结 65
第4章 深层神经网络 66
4.1 深度学习与深层神经网络 66
4.1.1 线性模型的局限性 67
4.1.2 激活函数实现去线性化 70
4.1.3 多层网络解决异或运算 73
4.2 损失函数定义 74
4.2.1 经典损失函数 75
4.2.2 自定义损失函数 79
4.3 神经网络优化算法 81
4.4 神经网络进一步优化 84
4.4.1 学习率的设置 85
4.4.2 过拟合问题 87
4.4.3 滑动平均模型 90
小结 92
第5章 MNIST数字识别问题 94
5.1 MNIST数据处理 94
5.2 神经网络模型训练及不同模型结果对比 97
5.2.1 TensorFlow训练神经网络 97
5.2.2 使用验证数据集判断模型效果 102
5.2.3 不同模型效果比较 103
5.3 变量管理 107
5.4 TensorFlow模型持久化 112
5.4.1 持久化代码实现 112
5.4.2 持久化原理及数据格式 117
5.5 TensorFlow最佳实践样例程序 126
小结 132
第6章 图像识别与卷积神经网络 134
6.1 图像识别问题简介及经典数据集 135
6.2 卷积神经网络简介 139
6.3 卷积神经网络常用结构 142
6.3.1 卷积层 142
6.3.2 池化层 147
6.4 经典卷积网络模型 149
6.4.1 LeNet-5模型 150
6.4.2 Inception-v3模型 156
6.5 卷积神经网络迁移学习 160
6.5.1 迁移学习介绍 160
6.5.2 TensorFlow实现迁移学习 161
小结 169
第7章 图像数据处理 170
7.1 TFRecord输入数据格式 170
7.1.1 TFRecord格式介绍 171
7.1.2 TFRecord样例程序 171
7.2 图像数据处理 173
7.2.1 TensorFlow图像处理函数 174
7.2.2 图像预处理完整样例 183
7.3 多线程输入数据处理框架 185
7.3.1 队列与多线程 186
7.3.2 输入文件队列 190
7.3.3 组合训练数据（batching） 193
7.3.4 输入数据处理框架 196
小结 198
第8章 循环神经网络 200
8.1 循环神经网络简介 200
8.2 长短时记忆网络（LTSM）结构 206
8.3 循环神经网络的变种 212
8.3.1 双向循环神经网络和深层循环神经网络 212
8.3.2 循环神经网络的dropout 214
8.4 循环神经网络样例应用 215
8.4.1 自然语言建模 216
8.4.2 时间序列预测 225
小结 230
第9章 TensorBoard可视化 232
9.1 TensorBoard简介 232
9.2 TensorFlow计算图可视化 234
9.2.1 命名空间与TensorBoard图上节点 234
9.2.2 节点信息 241
9.3 监控指标可视化 246
小结 252
第10章 TensorFlow计算加速 253
10.1 TensorFlow使用GPU 253
10.2 深度学习训练并行模式 258
10.3 多GPU并行 261
10.4 分布式TensorFlow 268
10.4.1 分布式TensorFlow原理 269
10.4.2 分布式TensorFlow模型训练 272
10.4.3 使用Caicloud运行分布式TensorFlow 282
小结 287
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Tensorflow：实战Google深度学习框架
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow实战
1 TensorFlow基础 1
1.1 TensorFlow概要 1
1.2 TensorFlow编程模型简介 4
2 TensorFlow和其他深度学习框架的对比 18
2.1 主流深度学习框架对比 18
2.2 各深度学习框架简介 20
3 TensorFlow第一步 39
3.1 TensorFlow的编译及安装 39
3.2 TensorFlow实现SoftmaxRegression识别手写数字 46
4 TensorFlow实现自编码器及多层感知机 55
4.1 自编码器简介 55
4.2 TensorFlow实现自编码器 59
4.3 多层感知机简介 66
4.4 TensorFlow实现多层感知机 70
5 TensorFlow实现卷积神经网络 74
5.1 卷积神经网络简介 74
5.2 TensorFlow实现简单的卷积网络 80
5.3 TensorFlow实现进阶的卷积网络 83
6 TensorFlow实现经典卷积神经网络 95
6.1 TensorFlow实现AlexNet 97
6.2 TensorFlow实现VGGNet 108
6.3 TensorFlow实现GoogleInceptionNet 119
6.4 TensorFlow实现ResNet 143
6.5 卷积神经网络发展趋势 156
7 TensorFlow实现循环神经网络及Word2Vec 159
7.1 TensorFlow实现Word2Vec 159
7.2 TensorFlow实现基于LSTM的语言模型 173
7.3 TensorFlow实现BidirectionalLSTMClassifier 188
8 TensorFlow实现深度强化学习 195
8.1 深度强化学习简介 195
8.2 TensorFlow实现策略网络 201
8.3 TensorFlow实现估值网络 213
9 TensorBoard、多GPU并行及分布式并行 233
9.1 TensorBoard 233
9.2 多GPU并行 243
9.3 分布式并行 249
10 TF.Learn从入门到精通 259
10.1 分布式Estimator 259
10.2 深度学习Estimator 267
10.3 机器学习Estimator 272
10.4 DataFrame 278
10.5 监督器Monitors 279
11 TF.Contrib的其他组件 283
11.1 统计分布 283
11.2 Layer模块 285
11.3 性能分析器tfprof 293
参考文献 297
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>机器学习实战：基于Scikit-Learn和TensorFlow
前言1
第一部分 机器学习基础
第1章 机器学习概览11
什么是机器学习12
为什么要使用机器学习12
机器学习系统的种类15
监督式/无监督式学习16
批量学习和在线学习21
基于实例与基于模型的学习24
机器学习的主要挑战29
训练数据的数量不足29
训练数据不具代表性30
质量差的数据32
无关特征32
训练数据过度拟合33
训练数据拟合不足34
退后一步35
测试与验证35
练习37
第2章 端到端的机器学习项目39
使用真实数据39
观察大局40
框架问题41
选择性能指标42
检查假设45
获取数据45
创建工作区45
下载数据48
快速查看数据结构49
创建测试集52
从数据探索和可视化中获得洞见56
将地理数据可视化57
寻找相关性59
试验不同属性的组合61
机器学习算法的数据准备62
数据清理63
处理文本和分类属性65
自定义转换器67
特征缩放68
转换流水线68
选择和训练模型70
培训和评估训练集70
使用交叉验证来更好地进行评估72
微调模型74
网格搜索74
随机搜索76
集成方法76
分析最佳模型及其错误76
通过测试集评估系统77
启动、监控和维护系统78
试试看79
练习79
第3章 分类80
MNIST80
训练一个二元分类器82
性能考核83
使用交叉验证测量精度83
混淆矩阵84
精度和召回率86
精度/召回率权衡87
ROC曲线90
多类别分类器93
错误分析95
多标签分类98
多输出分类99
练习100
第4章 训练模型102
线性回归103
标准方程104
计算复杂度106
梯度下降107
批量梯度下降110
随机梯度下降112
小批量梯度下降114
多项式回归115
学习曲线117
正则线性模型121
岭回归121
套索回归123
弹性网络125
早期停止法126
逻辑回归127
概率估算127
训练和成本函数128
决策边界129
Softmax回归131
练习134
第5章 支持向量机136
线性SVM分类136
软间隔分类137
非线性SVM分类139
多项式核140
添加相似特征141
高斯RBF核函数142
计算复杂度143
SVM回归144
工作原理145
决策函数和预测146
训练目标146
二次规划148
对偶问题149
核化SVM149
在线SVM151
练习152
第6章 决策树154
决策树训练和可视化154
做出预测155
估算类别概率157
CART训练算法158
计算复杂度158
基尼不纯度还是信息熵159
正则化超参数159
回归161
不稳定性162
练习163
第7章 集成学习和随机森林165
投票分类器165
bagging和pasting168
Scikit-Learn的bagging和pasting169
包外评估170
Random Patches和随机子空间171
随机森林172
极端随机树173
特征重要性173
提升法174
AdaBoost175
梯度提升177
堆叠法181
练习184
第8章 降维185
维度的诅咒186
数据降维的主要方法187
投影187
流形学习189
PCA190
保留差异性190
主成分191
低维度投影192
使用Scikit-Learn192
方差解释率193
选择正确数量的维度193
PCA压缩194
增量PCA195
随机PCA195
核主成分分析196
选择核函数和调整超参数197
局部线性嵌入199
其他降维技巧200
练习201
第二部分 神经网络和深度学习
第9章 运行TensorFlow205
安装207
创建一个计算图并在会话中执行208
管理图209
节点值的生命周期210
TensorFlow中的线性回归211
实现梯度下降211
手工计算梯度212
使用自动微分212
使用优化器214
给训练算法提供数据214
保存和恢复模型215
用TensorBoard来可视化图和训练曲线216
命名作用域219
模块化220
共享变量222
练习225
第10章 人工神经网络简介227
从生物神经元到人工神经元227
生物神经元228
具有神经元的逻辑计算229
感知器230
多层感知器和反向传播233
用TensorFlow的高级API来训练MLP236
使用纯TensorFlow训练DNN237
构建阶段237
执行阶段240
使用神经网络241
微调神经网络的超参数242
隐藏层的个数242
每个隐藏层中的神经元数243
激活函数243
练习244
第11章 训练深度神经网络245
梯度消失/爆炸问题245
Xavier初始化和He初始化246
非饱和激活函数248
批量归一化250
梯度剪裁254
重用预训练图层255
重用TensorFlow模型255
重用其他框架的模型256
冻结低层257
缓存冻结层257
调整、丢弃或替换高层258
模型动物园258
无监督的预训练259
辅助任务中的预训练260
快速优化器261
Momentum优化261
Nesterov梯度加速262
AdaGrad263
RMSProp265
Adam优化265
学习速率调度267
通过正则化避免过度拟合269
提前停止269
1和2正则化269
dropout270
最大范数正则化273
数据扩充274
实用指南275
练习276
第12章 跨设备和服务器的分布式TensorFlow279
一台机器上的多个运算资源280
安装280
管理GPU RAM282
在设备上操作284
并行执行287
控制依赖288
多设备跨多服务器288
开启一个会话290
master和worker服务290
分配跨任务操作291
跨多参数服务器分片变量291
用资源容器跨会话共享状态292
使用TensorFlow队列进行异步通信294
直接从图中加载数据299
在TensorFlow集群上并行化神经网络305
一台设备一个神经网络305
图内与图间复制306
模型并行化308
数据并行化309
练习314
第13章 卷积神经网络31
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>机器学习实战：基于Scikit-Learn和TensorFlow
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>21个项目玩转深度学习
第1章  MNIST机器学习入门  1
1.1  MNIST数据集  2
1.1.1  简介  2
1.1.2  实验：将MNIST数据集保存为图片  5
1.1.3  图像标签的独热(one-hot)表示  6
1.2  利用TensorFlow识别MNIST  8
1.2.1  Softmax回归  8
1.2.2  两层卷积网络分类  14
1.3  总结  18
第2章  CIFAR-10与ImageNet图像识别  19
2.1  CIFAR-10数据集  20
2.1.1  CIFAR-10简介  20
2.1.2  下载CIFAR-10数据  21
2.1.3  TensorFlow的数据读取机制  23
2.1.4  实验：将CIFAR-10数据集保存为图片形式  30
2.2  利用TensorFlow训练CIFAR-10识别模型  34
2.2.1  数据增强（Data Augmentation）  34
2.2.2  CIFAR-10识别模型  36
2.2.3  训练模型  39
2.2.4  在TensorFlow中查看训练进度  39
2.2.5  测试模型效果  42
2.3  ImageNet图像识别模型  44
2.3.1  ImageNet数据集简介  44
2.3.2  历代ImageNet图像识别模型  45
2.4  总结  49
第3章  打造自己的图像识别模型  50
3.1  微调（Fine-tune）的原理  51
3.2  数据准备  52
3.3  使用TensorFlow Slim微调模型  56
3.3.1  下载TensorFlow Slim的源代码  56
3.3.2  定义新的datasets文件  57
3.3.3  准备训练文件夹  59
3.3.4  开始训练  60
3.3.5  训练程序行为  62
3.3.6  验证模型正确率  63
3.3.7  TensorBoard可视化与超参数选择  64
3.3.8  导出模型并对单张图片进行识别  65
3.4  总结  69
第4章  Deep Dream模型  70
4.1  Deep Dream的技术原理  71
4.2  TensorFlow中的Deep Dream模型实践  73
4.2.1  导入Inception模型  73
4.2.2  生成原始的Deep Dream图像  76
4.2.3  生成更大尺寸的Deep Dream图像  78
4.2.4  生成更高质量的Deep Dream图像  82
4.2.5  最终的Deep Dream模型  87
4.3  总结  90
第5章  深度学习中的目标检测  91
5.1  深度学习中目标检测的原理  92
5.1.1  R-CNN的原理  92
5.1.2  SPPNet的原理  94
5.1.3  Fast R-CNN的原理  97
5.1.4  Faster R-CNN的原理  98
5.2  TensorFlow Object Detection API  101
5.2.1  安装TensorFlow Object Detection API  101
5.2.2  执行已经训练好的模型  103
5.2.3  训练新的模型  109
5.2.4  导出模型并预测单张图片  113
5.3  总结  114
第6章  人脸检测和人脸识别  115
6.1  MTCNN的原理  116
6.2  使用深度卷积网络提取特征  121
6.2.1  三元组损失（Triplet Loss）的定义  123
6.2.2  中心损失（Center Loss）的定义  123
6.3  使用特征设计应用  125
6.4  在TensorFlow中实现人脸识别  126
6.4.1  项目环境设置  126
6.4.2  LFW人脸数据库  127
6.4.3  LFW数据库上的人脸检测和对齐  128
6.4.4  使用已有模型验证LFW数据库准确率  129
6.4.5  在自己的数据上使用已有模型  130
6.4.6  重新训练新模型  133
6.4.7  三元组损失和中心损失的定义  138
6.5  总结  140
第7章  图像风格迁移  141
7.1  图像风格迁移的原理  142
7.1.1  原始图像风格迁移的原理  142
7.1.2  快速图像风格迁移的原理  148
7.2  在TensorFlow中实现快速风格迁移  149
7.2.1  使用预训练模型  150
7.2.2  训练自己的模型  153
7.2.3  在TensorBoard中监控训练情况  154
7.2.4  项目实现细节  157
7.3  总结  162
第8章  GAN和DCGAN入门  163
8.1  GAN的原理  164
8.2  DCGAN的原理  166
8.3  在TensorFlow中用DCGAN生成图像  169
8.3.1  生成MNIST图像  170
8.3.2  使用自己的数据集训练  171
8.3.3  程序结构分析：如何将图像读入模型  173
8.3.4  程序结构分析：可视化方法  177
8.4  总结  180
第9章  pix2pix模型与自动上色技术  181
9.1  cGAN的原理  182
9.2  pix2pix模型的原理  184
9.3  TensorFlow中的pix2pix模型  187
9.3.1  执行已有的数据集  187
9.3.2  创建自己的数据集  191
9.4  使用TensorFlow为灰度图像自动上色  194
9.4.1  为食物图片上色  194
9.4.2  为动漫图片进行上色  196
9.5  总结  198
第10章  超分辨率：如何让图像变得更清晰  199
10.1  数据预处理与训练  200
10.1.1  去除错误图片  200
10.1.2  将图像裁剪到统一大小  202
10.1.3  为代码添加新的操作  202
10.2  总结  209
第11章  CycleGAN与非配对图像转换  210
11.1  CycleGAN的原理  211
11.2  在TensorFlow中用训练CycleGAN模型  213
11.2.1  下载数据集并训练  213
11.2.2  使用自己的数据进行训练  217
11.3  程序结构分析  220
11.4  总结  224
第12章  RNN基本结构与Char RNN文本生成  225
12.1  RNN的原理  226
12.1.1  经典RNN的结构  226
12.1.2  N VS 1 RNN的结构  229
12.1.3  1 VS N RNN的结构  230
12.2  LSTM的原理  231
12.3  Char RNN的原理  235
12.4  TensorFlow中的RNN实现方式  237
12.4.1  实现RNN的基本单元：RNNCell  238
12.4.2  对RNN进行堆叠：MultiRNNCell  239
12.4.3  注意点：BasicRNNCell和BasicLSTMCell的output  240
12.4.4  使用tf.nn.dynamic_rnn展开时间维度  241
12.5  使用TensorFlow实现Char RNN  242
12.5.1  定义输入数据  243
12.5.2  定义多层LSTM模型  244
12.5.3  定义损失  245
12.5.4  训练模型与生成文字  246
12.5.5  更多参数说明  250
12.5.6  运行自己的数据  250
12.6  总结  251
第13章  序列分类问题详解  252
13.1  N VS 1的RNN结构  253
13.2  数列分类问题与数据生成  254
13.3  在TensorFlow中定义RNN分类模型  258
13.3.1  定义模型前的准备工作  258
13.3.2  定义RNN分类模型  259
13.3.3  定义损失并进行训练  261
13.4  模型的推广  262
13.5  总结  263
第14章  词的向量表示：word2vec与词嵌入  264
14.1  为什么需要做词嵌入  265
14.2  词嵌入的原理  266
14.2.1  CBOW实现词嵌入的原理  266
14.2.2  Skip-Gram实现词嵌入的原理  269
14.3  在TensorFlow中实现词嵌入  270
14.3.1  下载数据集  270
14.3.2  制作词表  272
14.3.3  生成每步的训练样本  274
14.3.4  定义模型  276
14.3.5  执行训练  279
14.3.6  可视化  281
14.4  与第12章的对比  284
14.5  总结  285
第15章  在TensorFlow中进行时间序列预测  286
15.1  时间序列问题的一般形式  287
15.2  用TFTS读入时间序列数据  287
15.2.1  从Numpy数组中读入时间序列数据  288
15.2.2  从CSV文件中读入时间序列数据  291
15.3  使用AR模型预测时间序列  293
15.3.1  AR模型的训练  293
15.3.2  AR模型的验证和预测  295
15.4  使用LSTM模型预测时间序列  297
15.4.1  LSTM模型中的单变量时间序列预测  297
15.4.2  LSTM模型中的多变量时间序列预测  299
15.5  总结  301
第16章  神经网络机器翻译技术  302
16.1  Encoder-Decoder模型的原理  303
16.2  注意力机制（Attention）  305
16.3  使用TensorFlow NMT搭建神经网络翻译引擎  309
16.3.1  示例：将越南语翻译为英语  309
16.3.2  构建中英翻译引擎  313
16.4  TensorFlow NMT源码简介  317
16.5  总结  319
第17章  看图说话：将图像转换为文字  320
17.1  Image Caption技术综述  321
17.1.1  从Encoder-Decoder结构谈起  321
17.1.2  将Encoder-Decoder应用到Image Caption任务上  322
17.1.3  对Encoder-Decoder的改进1：加入Attention机制  323
17.1.4  对Encoder-Decoder的改进2：加入高层语义  325
17.2  在TensorFlow中实现Image Caption  327
17.2.1  下载代码  327
17.2.2  环境准备  328
17.2.2  编译和数据准备  328
17.2.3  训练和验证  330
17.2.4  测试单张图片  331
17.3  总结  332
第18章  强化学习入门之Q  333
18.1  强化学习中的几个重要概念  334
18.2  Q Learning的原理与实验  336
18.2.1  环境定义  336
18.2.2  Q函数  338
18.2.3  Q函数的学习策略  339
18.2.4  ?-greedy策略  341
18.2.5  简单的Q Learning示例  341
18.2.6  更复杂的情况  342
18.3  总结  343
第19章  强化学习入门之SARSA算法  344
19.1  SARSA 算法的原理  345
19.1.1  通过与Q Learning对比学习SARSA算法  345
19.1.2  off-policy与on-policy  346
19.2  SARSA 算法的实现  347
19.3  总结  348
第20章  深度强化学习：Deep Q Learning  349
20.1  DQN算法的原理  350
20.1.1  问题简介  350
20.1.2  Deep Q Network  351
20.1.3  训练方法  352
20.2  在TensorFlow中运行DQN算法  353
20.2.1  安装依赖库  353
20.2.2  训练  355
20.2.3  测试  356
20.3  在TensorFlow中DQN算法的实现分析  357
20.4  总结  360
第21章  策略梯度（Policy Gradient）算法  361
21.1  策略梯度（Policy Gradient）算法的原理  362
21.1.1  Cartpole游戏  362
21.1.2  策略网络（Policy Network）  363
21.1.3  训练策略网络  364
21.2  在TensorFlow中实现策略梯度 算法  365
21.2.1  初始化  365
21.2.2  定义策略网络  366
21.2.3  训练  367
21.3  总结  371
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>21个项目玩转深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>面向机器智能的TensorFlow实践
译者序
前言
第一部分　开启TensorFlow之旅
第1章　引言2
1.1　无处不在的数据2
1.2　深度学习2
1.3　TensorFlow：一个现代的机器学习库3
1.4　TensorFlow：技术概要3
1.5　何为TensorFlow4
1.5.1　解读来自官网的单句描述4
1.5.2　单句描述未体现的内容6
1.6　何时使用TensorFlow7
1.7　TensorFlow的优势8
1.8　使用TensorFlow所面临的挑战9
1.9　高歌猛进9
第2章　安装TensorFlow10
2.1　选择安装环境10
2.2　Jupyter Notebook与matplotlib12
2.3　创建Virtualenv环境12
2.4　TensorFlow的简易安装13
2.5　源码构建及安装实例：在64位Ubuntu Linux上安装GPU版TensorFlow14
2.5.1　安装依赖库14
2.5.2　安装Bazel15
2.5.3　安装CUDA软件（仅限NVIDIA GPU）16
2.5.4　从源码构建和安装TensorFlow18
2.6　安装Jupyter Notebook20
2.7　安装matplotlib20
2.8　测试TensorFlow、Jupyter Notebook及matplotlib21
2.9　本章小结23
第二部分　TensorFlow与机器学习基础
第3章　TensorFlow基础26
3.1　数据流图简介26
3.1.1　数据流图基础26
3.1.2　节点的依赖关系29
3.2　在TensorFlow中定义数据流图33
3.2.1　构建第一个TensorFlow数据流图33
3.2.2　张量思维39
3.2.3　张量的形状43
3.2.4　TensorFlow的Operation44
3.2.5　TensorFlow的Graph对象46
3.2.6　TensorFlow Session48
3.2.7　利用占位节点添加输入52
3.2.8　Variable对象53
3.3　通过名称作用域组织数据流图56
3.4　练习：综合运用各种组件61
3.4.1　构建数据流图63
3.4.2　运行数据流图66
3.5　本章小结71
第4章　机器学习基础72
4.1　有监督学习简介72
4.2　保存训练检查点74
4.3　线性回归76
4.4　对数几率回归78
4.5　softmax分类83
4.6　多层神经网络85
4.7　梯度下降法与误差反向传播算法88
第三部分　用TensorFlow实现更高级的深度模型
第5章　目标识别与分类96
5.1　卷积神经网络97
5.2　卷积100
5.2.1　输入和卷积核100
5.2.2　跨度102
5.2.3　边界填充104
5.2.4　数据格式104
5.2.5　深入探讨卷积核105
5.3　常见层107
5.3.1　卷积层108
5.3.2　激活函数108
5.3.3　池化层111
5.3.4　归一化113
5.3.5　高级层114
5.4　图像与TensorFlow116
5.4.1　加载图像116
5.4.2　图像格式117
5.4.3　图像操作121
5.4.4　颜色127
5.5　CNN的实现129
5.5.1　Stanford Dogs数据集129
5.5.2　将图像转为TFRecord文件130
5.5.3　加载图像133
5.5.4　模型134
5.5.5　训练136
5.5.6　用TensorBoard调试滤波器137
5.6　本章小结139
第6章　循环神经网络与自然语言处理140
6.1　循环神经网络简介140
6.1.1　时序的世界140
6.1.2　近似任意程序141
6.1.3　随时间反向传播142
6.1.4　序列的编码和解码143
6.1.5　实现第一个循环神经网络145
6.1.6　梯度消失与梯度爆炸145
6.1.7　长短时记忆网络147
6.1.8　RNN结构的变种148
6.2　词向量嵌入149
6.2.1　准备维基百科语料库151
6.2.2　模型结构155
6.2.3　噪声对比分类器156
6.2.4　训练模型156
6.3　序列分类157
6.3.1　Imdb影评数据集158
6.3.2　使用词向量嵌入159
6.3.3　序列标注模型159
6.3.4　来自最后相关活性值的softmax层161
6.3.5　梯度裁剪162
6.3.6　训练模型163
6.4　序列标注164
6.4.1　OCR数据集164
6.4.2　时间步之间共享的soft-max层166
6.4.3　训练模型169
6.4.4　双向RNN171
6.5　预测编码174
6.5.1　字符级语言建模174
6.5.2　ArXiv摘要API175
6.5.3　数据预处理177
6.5.4　预测编码模型178
6.5.5　训练模型182
6.5.6　生成相似序列185
6.6　本章小结188
第四部分　其他提示、技术与特性
第7章　产品环境中模型的部署190
7.1　搭建TensorFlow服务开发环境190
7.1.1　Docker镜像190
7.1.2　Bazel工作区191
7.2　导出训练好的模型192
7.3　定义服务器接口195
7.4　实现推断服务器197
7.5　客户端应用201
7.6　产品准备203
7.7　本章小结203
第8章　辅助函数、代码结构和类204
8.1　确保目录结构存在204
8.2　下载函数204
8.3　磁盘缓存修饰器205
8.4　属性字典206
8.5　惰性属性修饰器207
8.6　覆盖数据流图修饰器209
第9章　结语：其他资源212
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>面向机器智能的TensorFlow实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习之TensorFlow
配套学习资源
前言
第1篇 深度学习与TensorFlow基础
第1章 快速了解人工智能与TensorFlow	2
1.1 什么是深度学习	2
1.2 TensorFlow是做什么的	3
1.3 TensorFlow的特点	4
1.4 其他深度学习框架特点及介绍	5
1.5 如何通过本书学好深度学习	6
1.5.1 深度学习怎么学	6
1.5.2 如何学习本书	7
第2章 搭建开发环境	8
2.1 下载及安装Anaconda开发工具	8
2.2 在Windows平台下载及安装TensorFlow	11
2.3 GPU版本的安装方法	12
2.3.1 安装CUDA软件包	12
2.3.2 安装cuDNN库	13
2.3.3 测试显卡	14
2.4 熟悉Anaconda 3开发工具	15
2.4.1 快速了解Spyder	16
2.4.2 快速了解Jupyter Notebook	18
第3章 TensorFlow基本开发步骤——以逻辑回归拟合二维数据为例	19
3.1 实例1：从一组看似混乱的数据中找出y≈2x的规律	19
3.1.1 准备数据	20
3.1.2 搭建模型	21
3.1.3 迭代训练模型	23
3.1.4 使用模型	25
3.2 模型是如何训练出来的	25
3.2.1 模型里的内容及意义	25
3.2.2 模型内部的数据流向	26
3.3 了解TensorFlow开发的基本步骤	27
3.3.1 定义输入节点的方法	27
3.3.2 实例2：通过字典类型定义输入节点	28
3.3.3 实例3：直接定义输入节点	28
3.3.4 定义“学习参数”的变量	29
3.3.5 实例4：通过字典类型定义“学习参数”	29
3.3.6 定义“运算”	29
3.3.7 优化函数，优化目标	30
3.3.8 初始化所有变量	30
3.3.9 迭代更新参数到最优解	31
3.3.10 测试模型	31
3.3.11 使用模型	31
第4章 TensorFlow编程基础	32
4.1 编程模型	32
4.1.1 了解模型的运行机制	33
4.1.2 实例5：编写hello world程序演示session的使用	34
4.1.3 实例6：演示with session的使用	35
4.1.4 实例7：演示注入机制	35
4.1.5 建立session的其他方法	36
4.1.6 实例8：使用注入机制获取节点	36
4.1.7 指定GPU运算	37
4.1.8 设置GPU使用资源	37
4.1.9 保存和载入模型的方法介绍	38
4.1.10 实例9：保存/载入线性回归模型	38
4.1.11 实例10：分析模型内容，演示模型的其他保存方法	40
4.1.12 检查点（Checkpoint）	41
4.1.13 实例11：为模型添加保存检查点	41
4.1.14 实例12：更简便地保存检查点	44
4.1.15 模型操作常用函数总结	45
4.1.16 TensorBoard可视化介绍	45
4.1.17 实例13：线性回归的TensorBoard可视化	46
4.2 TensorFlow基础类型定义及操作函数介绍	48
4.2.1 张量及操作	49
4.2.2 算术运算函数	55
4.2.3 矩阵相关的运算	56
4.2.4 复数操作函数	58
4.2.5 规约计算	59
4.2.6 分割	60
4.2.7 序列比较与索引提取	61
4.2.8 错误类	62
4.3 共享变量	62
4.3.1 共享变量用途	62
4.3.2 使用get-variable获取变量	63
4.3.3 实例14：演示get_variable和Variable的区别	63
4.3.4 实例15：在特定的作用域下获取变量	65
4.3.5 实例16：共享变量功能的实现	66
4.3.6 实例17：初始化共享变量的作用域	67
4.3.7 实例18：演示作用域与操作符的受限范围	68
4.4 实例19：图的基本操作	70
4.4.1 建立图	70
4.4.2 获取张量	71
4.4.3 获取节点操作	72
4.4.4 获取元素列表	73
4.4.5 获取对象	73
4.4.6 练习题	74
4.5 配置分布式TensorFlow	74
4.5.1 分布式TensorFlow的角色及原理	74
4.5.2 分布部署TensorFlow的具体方法	75
4.5.3 实例20：使用TensorFlow实现分布式部署训练	75
4.6 动态图（Eager）	81
4.7 数据集（tf.data）	82
第5章 识别图中模糊的手写数字（实例21）	83
5.1 导入图片数据集	84
5.1.1 MNIST数据集介绍	84
5.1.2 下载并安装MNIST数据集	85
5.2 分析图片的特点，定义变量	87
5.3 构建模型	87
5.3.1 定义学习参数	87
5.3.2 定义输出节点	88
5.3.3 定义反向传播的结构	88
5.4 训练模型并输出中间状态参数	89
5.5 测试模型	90
5.6 保存模型	91
5.7 读取模型	92
第2篇 深度学习基础——神经网络
第6章 单个神经元	96
6.1 神经元的拟合原理	96
6.1.1 正向传播	98
6.1.2 反向传播	98
6.2 激活函数——加入非线性因素，解决线性模型缺陷	99
6.2.1 Sigmoid函数	99
6.2.2 Tanh函数	100
6.2.3 ReLU函数	101
6.2.4 Swish函数	103
6.2.5 激活函数总结	103
6.3 softmax算法——处理分类问题	103
6.3.1 什么是softmax	104
6.3.2 softmax原理	104
6.3.3 常用的分类函数	105
6.4 损失函数——用真实值与预测值的距离来指导模型的收敛方向	105
6.4.1 损失函数介绍	105
6.4.2 TensorFlow中常见的loss函数	106
6.5 softmax算法与损失函数的综合应用	108
6.5.1 实例22：交叉熵实验	108
6.5.2 实例23：one_hot实验	109
6.5.3 实例24：sparse交叉熵的使用	110
6.5.4 实例25：计算loss值	110
6.5.5 练习题	111
6.6 梯度下降——让模型逼近最小偏差	111
6.6.1 梯度下降的作用及分类	111
6.6.2 TensorFlow中的梯度下降函数	112
6.6.3 退化学习率——在训练的速度与精度之间找到平衡	113
6.6.4 实例26：退化学习率的用法举例	114
6.7 初始化学习参数	115
6.8 单个神经元的扩展——Maxout网络	116
6.8.1 Maxout介绍	116
6.8.2 实例27：用Maxout网络实现MNIST分类	117
6.9 练习题	118
第7章 多层神经网络——解决非线性问题	119
7.1 线性问题与非线性问题	119
7.1.1 实例28：用线性单分逻辑回归分析肿瘤是良性还是恶性的	119
7.1.2 实例29：用线性逻辑回归处理多分类问题	123
7.1.3 认识非线性问题	129
7.2 使用隐藏层解决非线性问题	130
7.2.1 实例30：使用带隐藏层的神经网络拟合异或操作	130
7.2.2 非线性网络的可视化及其意义	133
7.2.3 练习题	135
7.3 实例31：利用全连接网络将图片进行分类	136
7.4 全连接网络训练中的优化技巧	137
7.4.1 实例32：利用异或数据集演示过拟合问题	138
7.4.2 正则化	143
7.4.3 实例33：通过正则化改善过拟合情况	144
7.4.4 实例34：通过增大数据集改善过拟合	145
7.4.5 练习题	146
7.4.6 dropout——训练过程中，将部分神经单元暂时丢弃	146
7.4.7 实例35：为异或数据集模型添加dropout	147
7.4.8 实例36：基于退化学习率dropout技术来拟合异或数据集	149
7.4.9 全连接网络的深浅关系	150
7.5 练习题	150
第8章 卷积神经网络——解决参数太多问题	151
8.1 全连接网络的局限性	151
8.2 理解卷积神经网络	152
8.3 网络结构	153
8.3.1 网络结构描述	153
8.3.2 卷积操作	155
8.3.3 池化层	157
8.4 卷积神经网络的相关函数	158
8.4.1 卷积函数tf.nn.conv2d	158
8.4.2 padding规则介绍	159
8.4.3 实例37：卷积函数的使用	160
8.4.4 实例38：使用卷积提取图片的轮廓	165
8.4.5 池化函数tf.nn.max_pool（avg_pool）	167
8.4.6 实例39：池化函数的使用	167
8.5 使用卷积神经网络对图片分类	170
8.5.1 CIFAR介绍	171
8.5.2 下载CIFAR数据	172
8.5.3 实例40：导入并显示CIFAR数据集	173
8.5.4 实例41：显示CIFAR数据集的原始图片	174
8.5.5 cifar10_input的其他功能	176
8.5.6 在TensorFlow中使用queue	176
8.5.7 实例42：协调器的用法演示	178
8.5.8 实例43：为session中的队列加上协调器	179
8.5.9 实例44：建立一个带有全局平均池化层的卷积神经网络	180
8.5.10 练习题	183
8.6 反卷积神经网络	183
8.6.1 反卷积神经网络的应用场景	184
8.6.2 反卷积原理	184
8.6.3 实例45：演示反卷积的操作	185
8.6.4 反池化原理	188
8.6.5 实例46：演示反池化的操作	189
8.6.6 实例47：演示gradients基本用法	192
8.6.7 实例48：使用gradients对多个式子求多变量偏导	192
8.6.8 实例49：演示梯度停止的实现	193
8.7 实例50：用反卷积技术复原卷积网络各层图像	195
8.8 善用函数封装库	198
8.8.1 实例51：使用函数封装库重写CIFAR卷积网络	198
8.8.2 练习题	201
8.9 深度学习的模型训练技巧	201
8.9.1 实例52：优化卷积核技术的演示	201
8.9.2 实例53：多通道卷积技术的演示	202
8.9.3 批量归一化	204
8.9.4 实例54：为CIFAR图片分类模型添加BN	207
8.9.5 练习题	209
第9章 循环神经网络——具有记忆功能的网络	210
9.1 了解RNN的工作原理	210
9.1.1 了解人的记忆原理	210
9.1.2 RNN网络的应用领域	212
9.1.3 正向传播过程	212
9.1.4 随时间反向传播	213
9.2 简单RNN	215
9.2.1 实例55：简单循环神经网络实现——裸写一个退位减法器	215
9.2.2 实例56：使用RNN网络拟合回声信号序列	220
9.3 循环神经网络（RNN）的改进	225
9.3.1 LSTM网络介绍	225
9.3.2 窥视孔连接（Peephole）	228
9.3.3 带有映射输出的STMP	230
9.3.4 基于梯度剪辑的cell	230
9.3.5 GRU网络介绍	230
9.3.6 Bi-RNN网络介绍	231
9.3.7 基于神经网络的时序类分类CTC	232
9.4 TensorFlow实战RNN	233
9.4.1 TensorFlow中的cell类	233
9.4.2 通过cell类构建RNN	234
9.4.3 实例57：构建单层LSTM网络对MNIST数据集分类	239
9.4.4 实例58：构建单层GRU网络对MNIST数据集分类	240
9.4.5 实例59：创建动态单层RNN网络对MNIST数据集分类	240
9.4.6 实例60：静态多层LSTM对MNIST数据集分类	241
9.4.7 实例61：静态多层RNN-LSTM连接GRU对MNIST数据集分类	242
9.4.8 实例62：动态多层RNN对MNIST数据集分类	242
9.4.9 练习题	243
9.4.10 实例63：构建单层动态双向RNN对MNIST数据集分类	243
9.4.11 实例64：构建单层静态双向RNN对MNIST数据集分类	244
9.4.12 实例65：构建多层双向RNN对MNIST数据集分类	246
9.4.13 实例66：构建动态多层双向RNN对MNIST数据集分类	247
9.4.14 初始化RNN	247
9.4.15 优化RNN	248
9.4.16 实例67：在GRUCell中实现LN	249
9.4.17 CTC网络的loss——ctc_loss	251
9.4.18 CTCdecoder	254
9.5 实例68：利用BiRNN实现语音识别	255
9.5.1 语音识别背景	255
9.5.2 获取并整理样本	256
9.5.3 训练模型	265
9.5.4 练习题	272
9.6 实例69：利用RNN训练语言模型	273
9.6.1 准备样本	273
9.6.2 构建模型	275
9.7 语言模型的系统学习	279
9.7.1 统计语言模型	279
9.7.2 词向量	279
9.7.3 word2vec	281
9.7.4 实例70：用CBOW模型训练自己的word2vec	283
9.7.5 实例71：使用指定侯选采样本训练word2vec	293
9.7.6 练习题	296
9.8 处理Seq2Seq任务	296
9.8.1 Seq2Seq任务介绍	296
9.8.2 Encoder-Decoder框架	297
9.8.3 实例72：使用basic_rnn_seq2seq拟合曲线	298
9.8.4 实例73：预测当天的股票价格	306
9.8.5 基于注意力的Seq2Seq	310
9.8.6 实例74：基于Seq2Seq注意力模型实现中英文机器翻译	313
9.9 实例75：制作一个简单的聊天机器人	339
9.9.1 构建项目框架	340
9.9.2 准备聊天样本	340
9.9.3 预处理样本	340
9.9.4 训练样本	341
9.9.5 测试模型	342
9.10 时间序列的高级接口TFTS	344
第10章 自编码网络——能够自学习样本特征的网络	346
10.1 自编码网络介绍及应用	346
10.2 最简单的自编码网络	347
10.3 自编码网络的代码实现	347
10.3.1 实例76：提取图片的特征，并利用特征还原图片	347
10.3.2 线性解码器	351
10.3.3 实例77：提取图片的二维特征，并利用二维特征还原图片	351
10.3.4 实例78：实现卷积网络的自编码	356
10.3.5 练习题	358
10.4 去噪自编码	359
10.5 去噪自编码网络的代码实现	359
10.5.1 实例79：使用去噪自编码网络提取MNIST特征	359
10.5.2 练习题	363
10.6 栈式自编码	364
10.6.1 栈式自编码介绍	364
10.6.2 栈式自编码在深度学习中的意义	365
10.7 深度学习中自编码的常用方法	366
10.7.1 代替和级联	366
10.7.2 自编码的应用场景	366
10.8 去噪自编码与栈式自编码的综合实现	366
10.8.1 实例80：实现去噪自编码	367
10.8.2 实例81：添加模型存储支持分布训练	375
10.8.3 小心分布训练中的“坑”	376
10.8.4 练习题	377
10.9 变分自编码	377
10.9.1 什么是变分自编码	377
10.9.2 实例82：使用变分自编码模拟生成MNIST数据	377
10.9.3 练习题	384
10.10 条件变分自编码	385
10.10.1 什么是条件变分自编码	385
10.10.2 实例83：使用标签指导变分自编码网络生成MNIST数据	385
第3篇 深度学习进阶
第11章 深度神经网络	392
11.1 深度神经网络介绍	392
11.1.1 深度神经网络起源	392
11.1.2 经典模型的特点介绍	393
11.2 GoogLeNet模型介绍	394
11.2.1 MLP卷积层	394
11.2.2 全局均值池化	395
11.2.3 Inception 原始模型	396
11.2.4 Inception v1模型	396
11.2.5 Inception v2模型	397
11.2.6 Inception v3模型	397
11.2.7 Inception v4模型	399
11.3 残差网络（ResNet）	399
11.3.1 残差网络结构	399
11.3.2 残差网络原理	400
11.4 Inception-ResNet-v2结构	400
11.5 TensorFlow中的图片分类模型库——slim	400
11.5.1 获取models中的slim模块代码	401
11.5.2 models中的Slim目录结构	401
11.5.3 slim中的数据集处理	403
11.5.4 实例84：利用slim读取TFRecord中的数据	405
11.5.5 在slim中训练模型	407
11.6 使用slim中的深度网络模型进行图像的识别与检测	410
11.6.1 实例85：调用Inception_ResNet_v2模型进行图像识别	410
11.6.2 实例86：调用VGG模型进行图像检测	413
11.7 实物检测模型库——Object Detection API	417
11.7.1 准备工作	418
11.7.2 实例87：调用Object Detection API进行实物检测	421
11.8 实物检测领域的相关模型	425
11.8.1 RCNN基于卷积神经网络特征的区域方法	426
11.8.2 SPP-Net：基于空间金字塔池化的优化RCNN方法	426
11.8.3 Fast-R-CNN快速的RCNN模型	426
11.8.4 YOLO：能够一次性预测多个位置和类别的模型	427
11.8.5 SSD：比YOLO更快更准的模型	428
11.8.6 YOLO2：YOLO的升级版模型	428
11.9 机器自己设计的模型（NASNet）	428
第12章 对抗神经网络（GAN）	430
12.1 GAN的理论知识	430
12.1.1 生成式模型的应用	431
12.1.2 GAN的训练方法	431
12.2 DCGAN——基于深度卷积的GAN	432
12.3 InfoGAN和ACGAN：指定类别生成模拟样本的GAN	432
12.3.1 InfoGAN：带有隐含信息的GAN	432
12.3.2 AC-GAN：带有辅助分类信息的GAN	433
12.3.3 实例88：构建InfoGAN生成MNIST模拟数据	434
12.3.4 练习题	440
12.4 AEGAN：基于自编码器的GAN	441
12.4.1 AEGAN原理及用途介绍	441
12.4.2 实例89：使用AEGAN对MNIST数据集压缩特征及重建	442
12.5 WGAN-GP：更容易训练的GAN	447
12.5.1 WGAN：基于推土机距离原理的GAN	448
12.5.2 WGAN-GP：带梯度惩罚项的WGAN	449
12.5.3 实例90：构建WGAN-GP生成MNIST数据集	451
12.5.4 练习题	455
12.6 LSGAN（最小乘二GAN）：具有WGAN 同样效果的GAN	455
12.6.1 LSGAN介绍	455
12.6.2 实例91：构建LSGAN生成MNIST模拟数据	456
12.7 GAN-cls：具有匹配感知的判别器	457
12.7.1 GAN-cls的具体实现	458
12.7.2 实例92：使用GAN-cls技术实现生成标签匹配的模拟数据	458
12.8 SRGAN——适用于超分辨率重建的GAN	461
12.8.1 超分辨率技术	461
12.8.2 实例93：ESPCN实现MNIST数据集的超分辨率重建	463
12.8.3 实例94：ESPCN实现flowers数据集的超分辨率重建	466
12.8.4 实例95：使用残差网络的ESPCN	472
12.8.5 SRGAN的原理	477
12.8.6 实例96：使用SRGAN实现flowers数据集的超分辨率修复	477
12.9 GAN网络的高级接口TFGAN	485
12.10 总结	486
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习之TensorFlow
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow：实战Google深度学习框架（第2版）
第1章 深度学习简介
1.1 人工智能、机器学习与深度学习
1.2 深度学习的发展历程
1.3 深度学习的应用
1.3.1 计算机视觉
1.3.2 语音识别
1.3.3 自然语言处理
1.3.4 人机博弈
1.4 深度学习工具介绍和对比
小结
第2章 TensorFlow环境搭建
2.1 TensorFlow的主要依赖包
2.1.1 Protocol Buffer
2.1.2 Bazel
2.2 TensorFlow安装
2.2.1 使用Docker安装
2.2.2 使用pip安装
2.2.3 从源代码编译安装
2.3 TensorFlow测试样例
小结
第3章 TensorFlow入门
3.1 TensorFlow计算模型——计算图
3.1.1 计算图的概念
3.1.2 计算图的使用
3.2 TensorFlow数据模型——张量
3.2.1 张量的概念
3.2.2 张量的使用
3.3 TensorFlow运行模型——会话
3.4 TensorFlow实现神经网络
3.4.1 TensorFlow游乐场及神经网络简介
3.4.2 前向传播算法简介
3.4.3 神经网络参数与TensorFlow变量
3.4.4 通过TensorFlow训练神经网络模型
3.4.5 完整神经网络样例程序
小结
第4章 深层神经网络
4.1 深度学习与深层神经网络
4.1.1 线性模型的局限性
4.1.2 激活函数实现去线性化
4.1.3 多层网络解决异或运算
4.2 损失函数定义
4.2.1 经典损失函数
4.2.2 自定义损失函数
4.3 神经网络优化算法
4.4 神经网络进一步优化
4.4.1 学习率的设置
4.4.2 过拟合问题
4.4.3 滑动平均模型
小结
第5章 MNIST数字识别问题
5.1 MNIST数据处理
5.2 神经网络模型训练及不同模型结果对比
5.2.1 TensorFlow训练神经网络
5.2.2 使用验证数据集判断模型效果
5.2.3 不同模型效果比较
5.3 变量管理
5.4 TensorFlow模型持久化
5.4.1 持久化代码实现
5.4.2 持久化原理及数据格式
5.5 TensorFlow最佳实践样例程序
小结
第6章 图像识别与卷积神经网络
6.1 图像识别问题简介及经典数据集
6.2 卷积神经网络简介
6.3 卷积神经网络常用结构
6.3.1 卷积层
6.3.2 池化层
6.4 经典卷积网络模型
6.4.1 LeNet-5模型
6.4.2 Inception-v3模型
6.5 卷积神经网络迁移学习
6.5.1 迁移学习介绍
6.5.2 TensorFlow实现迁移学习
小结
第7章 图像数据处理
7.1 TFRecord输入数据格式
7.1.1 TFRecord格式介绍
7.1.2 TFRecord样例程序
7.2 图像数据处理
7.2.1 TensorFlow图像处理函数
7.2.2 图像预处理完整样例
7.3 多线程输入数据处理框架
7.3.1 队列与多线程
7.3.2 输入文件队列
7.3.3 组合训练数据（batching）
7.3.4 输入数据处理框架
7.4 数据集（Dataset）
7.4.1 数据集的基本使用方法
7.4.2 数据集的高层操作
小结
第8章 循环神经网络
8.1 循环神经网络简介
8.2 长短时记忆网络（LSTM）结构
8.3 循环神经网络的变种
8.3.1 双向循环神经网络和深层循环神经网络
8.3.2 循环神经网络的dropout
8.4 循环神经网络样例应用
小结
第9章 自然语言处理
9.1 语言模型的背景知识
9.1.1 语言模型简介
9.1.2 语言模型的评价方法
9.2 神经语言模型
9.2.1 PTB数据集的预处理
9.2.2 PTB数据的batching方法
9.2.3 基于循环神经网络的神经语言模型
9.3 神经网络机器翻译
9.3.1 机器翻译背景与Seq2Seq模型介绍
9.3.2 机器翻译文本数据的预处理
9.3.3 Seq2Seq模型的代码实现
9.3.4 注意力机制
小结
第10章 TensorFlow高层封装
10.1 TensorFlow高层封装总览
10.2 Keras介绍
10.2.1 Keras基本用法
10.2.2 Keras高级用法
10.3 Estimator介绍
10.3.1 Estimator基本用法
10.3.2 Estimator自定义模型
10.3.3 使用数据集（Dataset）作为Estimator输入
小结
第11章 TensorBoard可视化
11.1 TensorBoard简介
11.2 TensorFlow计算图可视化
11.2.1 命名空间与TensorBoard图上节点
11.2.2 节点信息
11.3 监控指标可视化
11.4 高维向量可视化
小结
第12章 TensorFlow计算加速
12.1 TensorFlow使用GPU
12.2 深度学习训练并行模式
12.3 多GPU并行
12.4 分布式TensorFlow
12.4.1 分布式TensorFlow原理
12.4.2 分布式TensorFlow模型训练
小结
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow：实战Google深度学习框架（第2版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow技术解析与实战
第一篇　基础篇
第1章　人工智能概述 2
1.1　什么是人工智能 2
1.2　什么是深度学习 5
1.3　深度学习的入门方法 7
1.4　什么是TensorFlow 11
1.5　为什么要学TensorFlow 12
1.5.1　TensorFlow的特性 14
1.5.2　使用TensorFlow的公司 15
1.5.3　TensorFlow的发展 16
1.6　机器学习的相关赛事 16
1.6.1　ImageNet的ILSVRC 17
1.6.2　Kaggle 18
1.6.3　天池大数据竞赛 19
1.7　国内的人工智能公司 20
1.8　小结 22
第2章　TensorFlow环境的准备 23
2.1　下载TensorFlow 1.1.0 23
2.2　基于pip的安装 23
2.2.1　Mac OS环境准备 24
2.2.2　Ubuntu/Linux环境准备 25
2.2.3　Windows环境准备 25
2.3　基于Java的安装 28
2.4　从源代码安装 29
2.5　依赖的其他模块 30
2.5.1　numpy 30
2.5.2　matplotlib 31
2.5.3　jupyter 31
2.5.4　scikit-image 32
2.5.5　librosa 32
2.5.6　nltk 32
2.5.7　keras 33
2.5.8　tflearn 33
2.6　小结 33
第3章　可视化TensorFlow 34
3.1　PlayGround 34
3.1.1　数据 35
3.1.2　特征 36
3.1.3　隐藏层 36
3.1.4　输出 37
3.2　TensorBoard 39
3.2.1　SCALARS面板 40
3.2.2　IMAGES面板 41
3.2.3　AUDIO面板 42
3.2.4　GRAPHS面板 42
3.2.5　DISTRIBUTIONS面板 43
3.2.6　HISTOGRAMS面板 43
3.2.7　EMBEDDINGS面板 44
3.3　可视化的例子 44
3.3.1　降维分析 44
3.3.2　嵌入投影仪 48
3.4　小结 51
第4章　TensorFlow基础知识 52
4.1　系统架构 52
4.2　设计理念 53
4.3　编程模型 54
4.3.1　边 56
4.3.2　节点 57
4.3.3　其他概念 57
4.4　常用API 60
4.4.1　图、操作和张量 60
4.4.2　可视化 61
4.5　变量作用域 62
4.5.1　variable_scope示例 62
4.5.2　name_scope示例 64
4.6　批标准化 64
4.6.1　方法 65
4.6.2　优点 65
4.6.3　示例 65
4.7　神经元函数及优化方法 66
4.7.1　激活函数 66
4.7.2　卷积函数 69
4.7.3　池化函数 72
4.7.4　分类函数 73
4.7.5　优化方法 74
4.8　模型的存储与加载 79
4.8.1　模型的存储与加载 79
4.8.2　图的存储与加载 82
4.9　队列和线程 82
4.9.1　队列 82
4.9.2　队列管理器 85
4.9.3　线程和协调器 86
4.10　加载数据 87
4.10.1　预加载数据 87
4.10.2　填充数据 87
4.10.3　从文件读取数据 88
4.11　实现一个自定义操作 92
4.11.1　步骤 92
4.11.2　最佳实践 93
4.12　小结 101
第5章　TensorFlow源代码解析 102
5.1　TensorFlow的目录结构 102
5.1.1　contirb 103
5.1.2　core 104
5.1.3　examples 105
5.1.4　g3doc 105
5.1.5　python 105
5.1.6　tensorboard 105
5.2　TensorFlow源代码的学习方法 106
5.3　小结 108
第6章　神经网络的发展及其TensorFlow实现 109
6.1　卷积神经网络 109
6.2　卷积神经网络发展 110
6.2.1　网络加深 111
6.2.2　增强卷积层的功能 115
6.2.3　从分类任务到检测任务 120
6.2.4　增加新的功能模块 121
6.3　MNIST的AlexNet实现 121
6.3.1　加载数据 121
6.3.2　构建网络模型 122
6.3.3　训练模型和评估模型 124
6.4　循环神经网络 125
6.5　循环神经网络发展 126
6.5.1　增强隐藏层的功能 127
6.5.2　双向化及加深网络 129
6.6　TensorFlow Model Zoo 131
6.7　其他研究进展 131
6.7.1　强化学习 132
6.7.2　深度森林 132
6.7.3　深度学习与艺术 132
6.8　小结 133
第7章　TensorFlow的高级框架 134
7.1　TFLearn 134
7.1.1　加载数据 134
7.1.2　构建网络模型 135
7.1.3　训练模型 135
7.2　Keras 135
7.2.1　Keras的优点 136
7.2.2　Keras的模型 136
7.2.3　Keras的使用 137
7.3　小结 141
第二篇　实战篇
第8章　第一个TensorFlow程序 144
8.1　TensorFlow的运行方式 144
8.1.1　生成及加载数据 144
8.1.2　构建网络模型 145
8.1.3　训练模型 145
8.2　超参数的设定 146
8.3　小结 147
第9章　TensorFlow在MNIST中的应用 148
9.1　MNIST数据集简介 148
9.1.1　训练集的标记文件 148
9.1.2　训练集的图片文件 149
9.1.3　测试集的标记文件 149
9.1.4　测试集的图片文件 150
9.2　MNIST的分类问题 150
9.2.1　加载数据 150
9.2.2　构建回归模型 151
9.2.3　训练模型 151
9.2.4　评估模型 152
9.3　训练过程的可视化 152
9.4　MNIST的卷积神经网络 156
9.4.1　加载数据 157
9.4.2　构建模型 157
9.4.3　训练模型和评估模型 159
9.5　MNIST的循环神经网络 161
9.5.1　加载数据 161
9.5.2　构建模型 161
9.5.3 训练数据及评估模型 163
9.6　MNIST的无监督学习 164
9.6.1　自编码网络 164
9.6.2　TensorFlow的自编码网络实现 165
9.7　小结 169
第10章　人脸识别 170
10.1　人脸识别简介 170
10.2　人脸识别的技术流程 171
10.2.1　人脸图像采集及检测 171
10.2.2　人脸图像预处理 171
10.2.3　人脸图像特征提取 171
10.2.4　人脸图像匹配与识别 172
10.3　人脸识别的分类 172
10.3.1　人脸检测 172
10.3.2　人脸关键点检测 173
10.3.3　人脸验证 174
10.3.4　人脸属性检测 174
10.4　人脸检测 175
10.4.1　LFW数据集 175
10.4.2　数据预处理 175
10.4.3　进行检测 176
10.5　性别和年龄识别 178
10.5.1　数据预处理 179
10.5.2　构建模型 181
10.5.3　训练模型 182
10.5.4　验证模型 184
10.6　小结 185
第11章　自然语言处理 186
11.1　模型的选择 186
11.2　英文数字语音识别 187
11.2.1　定义输入数据并预处理数据 188
11.2.2　定义网络模型 188
11.2.3　训练模型 188
11.2.4　预测模型 189
11.3　智能聊天机器人 189
11.3.1　原理 190
11.3.2　最佳实践 191
11.4　小结 200
第12章　图像与语音的结合 201
12.1　看图说话模型 201
12.1.1　原理 202
12.1.2　最佳实践 203
12.2　小结 205
第13章　生成式对抗网络 206
13.1　生成式对抗网络的原理 206
13.2　生成式对抗网络的应用 207
13.3　生成式对抗网络的实现 208
13.4　生成式对抗网络的改进 214
13.5　小结 214
第三篇　提高篇
第14章　分布式TensorFlow 216
14.1　分布式原理 216
14.1.1　单机多卡和分布式 216
14.1.2　分布式部署方式 217
14.2　分布式架构 218
14.2.1　客户端、主节点和工作节点的关系 218
14.2.2　客户端、主节点和工作节点的交互过程 220
14.3　分布式模式 221
14.3.1　数据并行 221
14.3.2　同步更新和异步更新 222
14.3.3　模型并行 224
14.4　分布式API 225
14.5　分布式训练代码框架 226
14.6　分布式最佳实践 227
14.7　小结 235
第15章　TensorFlow线性代数编译框架XLA 236
15.1　XLA的优势 236
15.2　XLA的工作原理 237
15.3　JIT编译方式 238
15.3.1　打开JIT编译 238
15.3.2　将操作符放在XLA设备上 238
15.4　JIT编译在MNIST上的实现 239
15.5　小结 240
第16章　TensorFlow Debugger 241
16.1　Debugger的使用示例 241
16.2　远程调试方法 245
16.3　小结 245
第17章　TensorFlow和Kubernetes结合 246
17.1　为什么需要Kubernetes 246
17.2　分布式TensorFlow在Kubernetes中的运行 247
17.2.1　部署及运行 247
17.2.2　其他应用 253
17.3　小结 254
第18章　TensorFlowOnSpark 255
18.1　TensorFlowOnSpark的架构 255
18.2　TensorFlowOnSpark在MNIST上的实践 257
18.3　小结 261
第19章　TensorFlow移动端应用 262
19.1　移动端应用原理 262
19.1.1　量化 263
19.1.2　优化矩阵乘法运算 266
19.2　iOS系统实践 266
19.2.1　环境准备 266
19.2.2　编译演示程序并运行 267
19.2.3　自定义模型的编译及运行 269
19.3　Android系统实践 273
19.3.1　环境准备 274
19.3.2　编译演示程序并运行 275
19.3.3　自定义模型的编译及运行 277
19.4　树莓派实践 278
19.5　小结 278
第20章　TensorFlow的其他特性 279
20.1　TensorFlow Serving 279
20.2　TensorFlow Flod 280
20.3　TensorFlow计算加速 281
20.3.1　CPU加速 281
20.3.2　TPU加速和FPGA加速 282
20.4　小结 283
第21章　机器学习的评测体系 284
21.1　人脸识别的性能指标 284
21.2　聊天机器人的性能指标 284
21.3　机器翻译的评价方法 286
21.3.1　BLEU 286
21.3.2　METEOR 287
21.4　常用的通用评价指标 287
21.4.1　ROC和AUC 288
21.4.2　AP和mAP 288
21.5　小结 288
附录A　公开数据集 289
附录B　项目管理经验小谈 292
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow技术解析与实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习原理与TensorFlow实践
1    深度学习简介  1
1.1  深度学习介绍  1
1.2  深度学习的趋势  7
1.3  参考资料  10
2    TensorFlow系统介绍  12
2.1  TensorFlow诞生的动机  12
2.2  TensorFlow系统简介  14
2.3  TensorFlow基础概念  16
2.3.1  计算图  16
2.3.2  Session会话  18
2.4  系统架构  19
2.5  源码结构  21
2.5.1  后端执行引擎  22
2.5.2  前端语言接口  24
2.6  小结  24
2.7  参考资料  25
3    Hello TensorFlow  26
3.1  环境准备  26
3.1.1  Mac OS安装  27
3.1.2  Linux GPU服务器安装  28
3.1.3  常用Python库  32
3.2  Titanic题目实战  34
3.2.1  Kaggle平台介绍  34
3.2.2  Titanic题目介绍  35
3.2.3  数据读入及预处理  38
3.2.4  构建计算图  40
3.2.5  构建训练迭代过程  44
3.2.6  执行训练  46
3.2.7  存储和加载模型参数  47
3.2.8  预测测试数据结果  50
3.3  数据挖掘的技巧  51
3.3.1  数据可视化  52
3.3.2  特征工程  54
3.3.3  多种算法模型  57
3.4  TensorBoard可视化  58
3.4.1  记录事件数据  58
3.4.2  启动TensorBorad服务  60
3.5  数据读取  62
3.5.1  数据文件格式  63
3.5.2  TFRecord  63
3.6  SkFlow、TFLearn与TF-Slim  67
3.7  小结  69
3.8  参考资料  69
4    CNN“看懂”世界  71
4.1  图像识别的难题  72
4.2  CNNs的基本原理  74
4.2.1  卷积的数学意义  75
4.2.2  卷积滤波  77
4.2.3  CNNs中的卷积层  81
4.2.4  池化（Pooling）  83
4.2.5  ReLU  84
4.2.6  多层卷积  86
4.2.7  Dropout  86
4.3  经典CNN模型  87
4.3.1  AlexNet  88
4.3.2  VGGNets  95
4.3.3  GoogLeNet & Inception  98
4.3.4  ResNets  106
4.4  图像风格转换  109
4.4.1  量化的风格  109
4.4.2  风格的滤镜  116
4.5  小结  120
4.6  参考资料  121
5    RNN“能说会道”  123
5.1  文本理解和文本生成问题  124
5.2  标准RNN模型  128
5.2.1  RNN模型介绍  128
5.2.2  BPTT算法  130
5.2.3  灵活的RNN结构  132
5.2.4  TensorFlow实现正弦序列预测  135
5.3  LSTM模型  138
5.3.1  长期依赖的难题  138
5.3.2  LSTM基本原理  139
5.3.3  TensorFlow构建LSTM模型  142
5.4  更多RNN的变体  144
5.5  语言模型  146
5.5.1  NGram语言模型  146
5.5.2  神经网络语言模型  148
5.5.3  循环神经网络语言模型  150
5.5.4  语言模型也能写代码  152
5.5.5  改进方向  163
5.6  对话机器人  164
5.6.1  对话机器人的发展  165
5.6.2  基于seq2seq的对话机器人  169
5.7  小结  181
5.8  参考资料  182
6    CNN+LSTM看图说话  183
6.1  CNN+LSTM网络模型与图像检测问题  184
6.1.1  OverFeat和Faster R-CNN图像检测算法介绍  185
6.1.2  遮挡目标图像检测方法  187
6.1.3  ReInspect算法实现及模块说明  188
6.1.4  ReInspect算法的实验数据与结论  204
6.2  CNN+LSTM网络模型与图像摘要问题  207
6.2.1  图像摘要问题  208
6.2.2  NIC图像摘要生成算法  209
6.2.3  NIC图像摘要生成算法实现说明  214
6.2.4  NIC算法的实验数据与结论  243
6.3  小结  249
6.4  参考资料  250
7    损失函数与优化算法  253
7.1  目标函数优化策略  254
7.1.1  梯度下降算法  254
7.1.2  RMSProp优化算法  256
7.1.3  Adam优化算法  257
7.1.4  目标函数优化算法小结  258
7.2  类别采样（Candidate Sampling）损失函数  259
7.2.1  softmax类别采样损失函数  261
7.2.2  噪声对比估计类别采样损失函数  281
7.2.3  负样本估计类别采样损失函数  286
7.2.4  类别采样logistic损失函数  286
7.3  小结  287
7.4  参考资料  288
结语  289
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习原理与TensorFlow实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow学习指南
前言1
第1章 引言5
1.1 走入深度学习5
1.2 TensorFlow：名字中的含义8
1.3 高层次概览9
1.4 本章总结11
第2章 随之“流”动：启动与运行TensorFlow12
2.1 安装TensorFlow12
2.2 Hello World14
2.3 MNIST16
2.4 softmax回归17
2.5 本章总结24
第3章 理解TensorFlow基础知识25
3.1 计算图25
3.2 图、会话和提取数据26
3.3 流动的张量32
3.4 变量、占位符和简单的优化41
3.5 本章总结52
第4章 卷积神经网络53
4.1 卷积神经网络简介53
4.2 MNIST：第二轮55
4.3 CIFAR1063
4.4 本章总结71
第5章 文本I：文本及序列的处理，以及TensorBoard可视化72
5.1 序列数据的重要性72
5.2 循环神经网络简介73
5.3 处理RNN的文本序列87
5.4 本章总结97
第6章 文本II：词向量、高级RNN和词嵌入可视化99
6.1 词嵌入介绍99
6.2 word2vec101
6.3 预训练词嵌入，高级RNN110
6.4 本章总结116
第7章 TensorFlow抽象与简化117
7.1 本章概述117
7.2 contrib.learn121
7.3 TFLearn136
7.4 本章总结156
第8章 队列、线程和数据读取158
8.1 输入管道158
8.2 TFRecord159
8.3 队列162
8.4 完全多线程的输入管道168
8.5 本章总结172
第9章 分布式 TensorFlow173
9.1 分布式计算173
9.2 TensorFlow 元素175
9.3 分布式示例180
9.4 本章总结187
第10章 用TensorFlow导出和提供服务模型188
10.1 保存和导出模型188
10.2 TensorFlow Serving简介199
10.3 本章总结209
附录A 模型构建和使用TensorFlow Serving的建议210
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow学习指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow进阶指南：基础、算法与应用
第1章  人工智能与深度学习  1
1.1  人工智能与机器学习  1
1.2  无处不在的深度学习  6
1.3  如何入门深度学习  7
1.4  主流深度学习框架介绍  13
第2章  搭建TensorFlow环境  15
2.1  基于pip安装  15
2.1.1  基于Windows环境安装TensorFlow  15
2.1.2  基于Linux环境安装TensorFlow  22
2.2  基于Java安装TensorFlow  24
2.3  安装TensorFlow的常用依赖模块  27
2.4  Hello TensorFlow  30
2.4.1  MNIST数据集  30
2.4.2  编写训练程序  32
2.5  小结  35
第3章  TensorFlow基础  36
3.1  TensorFlow的系统架构  36
3.1.1  Client  37
3.1.2  Distributed Master  38
3.1.3  Worker Service  39
3.1.4  Kernel Implements  39
3.2  TensorFlow的数据结构——张量  39
3.2.1  什么是张量  39
3.2.2  张量的阶  40
3.2.3  张量的形状  40
3.2.4  数据类型  41
3.3  TensorFlow的计算模型——图  42
3.3.1  计算图基础  42
3.3.2  计算图的组成  43
3.3.3  计算图的使用  45
3.3.4  小结  48
3.4  TensorFlow中的会话——Session  48
第4章  TensorFlow中常用的激活函数与神经网络  50
4.1  激活函数的概念  50
4.2  常用的激活函数  51
4.2.1  Sigmoid函数  51
4.2.2  Tanh函数  53
4.2.3  ReLU函数  55
4.2.4  Softplus函数  57
4.2.5  Softmax函数  58
4.2.6  小结  59
4.3  损失函数的概念  60
4.4  损失函数的分类  63
4.5  常用的损失函数  65
4.5.1  0-1损失函数  65
4.5.2  Log损失函数  66
4.5.3  Hinge损失函数  69
4.5.4  指数损失  70
4.5.5  感知机损失  70
4.5.6  平方（均方）损失函数  71
4.5.7  绝对值损失函数  71
4.5.8  自定义损失函数  71
4.6  正则项  72
4.6.1  L0范数和L1范数  72
4.6.2  L2范数  73
4.6.3  核范数  74
4.7  规则化参数  76
4.8  易混淆的概念  76
4.9  神经网络的优化方法  77
4.9.1  梯度下降算法  77
4.9.2  随机梯度下降算法  79
4.9.3  其他的优化算法  80
4.9.4  小结  84
4.10  生成式对抗网络（GAN）  84
4.10.1  CGAN  96
4.10.2  DCGAN  97
4.10.3  WGAN  98
4.10.4  LSGAN  99
4.10.5  BEGAN  100
第5章  卷积神经网络  102
5.1  神经网络简介  102
5.1.1  神经元与神经网络  102
5.1.2  感知器（单层神经网络）与多层感知器  104
5.2  图像识别问题  108
5.3  常用的图像库介绍  111
5.4  卷积神经网络简介  114
5.4.1  CNN的基本原理与卷积核  115
5.4.2  池化  116
5.4.3  再探ReLU  118
5.5  CNN模型  119
5.5.1  LeNet-5模型  119
5.5.2  AlexNet模型  123
5.5.3  Inception模型  130
5.6  用CNN实现MNIST训练  147
第6章  循环神经网络  152
6.1  初识循环神经网络  152
6.1.1  前馈神经网络  153
6.1.2  神经网络中的时序信息  159
6.2  详解循环神经网络  160
6.3  RNN的变种——双向RNN  163
6.4  One-Hot Encoding  166
6.5  词向量和word2vec  167
6.5.1  CBOW模型  168
6.5.2  Skip-Gram模型  169
6.6  梯度消失问题和梯度爆炸问题  170
6.6.1  梯度下降  171
6.6.2  解决梯度消失和梯度爆炸问题的方法  173
6.7  RNN的变种——LSTM  180
6.8  写诗机器人  190
第7章  TensorFlow的可视化  197
7.1  TensorBoard简介  197
7.2  生成和使用TensorBoard  201
7.3  TensorBoard的面板展示  209
7.4  小结  224
第8章  TensorFlow中的数据操作  225
8.1  制作TFRecords数据集  225
8.2  Dataset API介绍  231
8.3  TensorFlow中的队列  234
第9章  支持向量机（SVM）  241
9.1  什么是支持向量机  241
9.2  计算最优超平面  243
9.3  TensorFlow实现线性SVM  244
9.4  非线性SVM介绍  248
9.5  使用TensorFlow实现非线性SVM分类器  251
第10章  TensorFlow结合Flask发布MNIST模型  259
10.1  Flask框架介绍  259
10.2  训练MNIST模型  260
10.3  小结  276
第11章  TensorFlow模型的发布与部署  277
11.1  TensorFlow Serving的前导知识  277
11.2  TensorFlow Serving 模型打包  281
11.3  TensorFlow Serving模型的部署和调用  285
第12章  TensorFlow Lite牛刀小试  286
12.1  什么是TensorFlow Lite  286
12.2  如何使用TensorFlow Lite模型  288
12.3  TensorFlow Lite与Android结合实现图像识别  291
第13章  TensorFlow GPU  297
13.1  什么是GPU  297
13.2  GPU的选择  298
13.3  搭建TensorFlow GPU  300
13.3.1  在Windows上搭建TensorFlow GPU  300
13.3.2  在Linux上搭建TensorFlow GPU  308
13.4  使用TensorFlow GPU进行训练  312
第14章  TensorFlow与目标检测  318
14.1  传统目标检测方法  318
14.2  RCNN介绍  320
14.3  Fast-RCNN  322
14.4  Faster-RCNN  326
14.5  YOLO  329
附录A  TensorFlow历代版本更新内容  355
A.1  TensorFlow 1.3版本更新内容  355
A.2  TensorFlow 1.4版本更新内容  356
A.3  TensorFlow 1.5版本更新内容  357
A.4  TensorFlow 1.6版本更新内容  357
A.5  TensorFlow 1.7版本更新内容  358
A.6  TensorFlow 1.8版本更新内容  358
A.7  TensorFlow 1.9版本更新内容  359
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow进阶指南：基础、算法与应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>强化学习精要
第一部分强化学习入门与基础知识
1 引言2
1.1 强化学习的概念    2
1.1.1 巴浦洛夫的狗   3
1.1.2 俄罗斯方块    4
1.2 站在被实验者的角度看问题   5
1.3 强化学习效果的评估   8
1.3.1 不断试错    8
1.3.2 看重长期回报   8
1.4 强化学习与监督学习   9
1.4.1 强化学习与监督学习的本质   9
1.4.2 模仿学习    10
1.5 强化学习的实验环境   11
1.5.1 Arcade Learning Environment   12
1.5.2 Box2D    12
1.5.3 MuJoCo   13
1.5.4 Gym    14
1.6 本书的主要内容    15
1.7 参考资料    16
2 数学与机器学习基础17
2.1 线性代数基础    17
2.2 对称矩阵的性质    21
2.2.1 特征值与特征向量   21
2.2.2 对称矩阵的特征值和特征向量   22
2.2.3 对称矩阵的对角化   23
2.3 概率论    24
2.3.1 概率与分布    24
2.3.2 最大似然估计   27
2.4 重要性采样    29
2.5 信息论基础    33
2.6 KL 散度    35
2.7 凸函数及其性质    39
2.8 机器学习的基本概念   41
2.9 机器学习的目标函数   43
2.10 总结    45
3 优化算法47
3.1 梯度下降法    47
3.1.1 什么是梯度下降法   47
3.1.2 优雅的步长    48
3.2 动量算法    53
3.3 共轭梯度法    59
3.3.1 精妙的约束    59
3.3.2 共轭    60
3.3.3 优化步长的确定   63
3.3.4 Gram-Schmidt 方法   64
3.3.5 共轭梯度    65
3.4 自然梯度法    69
3.4.1 基本概念    69
3.4.2 Fisher 信息矩阵   71
3.4.3 自然梯度法目标公式   76
3.5 总结    77
4 TensorFlow 入门78
4.1 TensorFlow 的基本使用方法   78
4.2 TensorFlow 原理介绍   82
4.2.1 创建变量的scope   83
4.2.2 创建一个Variable 背后的故事   89
4.2.3 运算操作    94
4.2.4 tf.gradients    96
4.2.5 Optimizer    102
4.2.6 TensorFlow 的反向传播技巧   106
4.2.7 arg_scope 的使用   109
4.3 TensorFlow 的分布式训练   113
4.3.1 基于MPI 的数据并行模型   114
4.3.2 MPI 的实现：mpi_adam   121
4.4 基于TensorFlow 实现经典网络结构   122
4.4.1 多层感知器    122
4.4.2 卷积神经网络   124
4.4.3 循环神经网络   126
4.5 总结    129
4.6 参考资料    129
5 Gym 与Baselines 130
5.1 Gym    130
5.1.1 Gym 的安装    130
5.1.2 Gym 的基本使用方法   132
5.1.3 利用Gym 框架实现一个经典的棋类游戏：蛇棋  134
5.2 Baselines    138
5.2.1 Baselines 中的Python 3 新特性   139
5.2.2 tf_util    141
5.2.3 对Gym 平台的扩展   142
5.3 总结    144
6 强化学习基本算法145
6.1 马尔可夫决策过程    145
6.1.1 MDP：策略与环境模型   145
6.1.2 值函数与Bellman 公式   147
6.1.3 “表格式”Agent   151
6.2 策略迭代    153
6.2.1 策略迭代法    153
6.2.2 策略提升的证明   159
6.2.3 策略迭代的效果展示   160
6.3 价值迭代    162
6.3.1 N 轮策略迭代   162
6.3.2 从动态规划的角度谈价值迭代   165
6.3.3 价值迭代的实现   167
6.4 泛化迭代    168
6.4.1 两个极端    168
6.4.2 广义策略迭代法   169
6.4.3 泛化迭代的实现   170
6.5 总结    171
第二部分最优价值算法
7 Q-Learning 基础173
7.1 状态转移概率：从掌握到放弃   173
7.2 蒙特卡罗方法    174
7.3 探索与利用    178
7.4 蒙特卡罗的方差问题   181
7.5 时序差分法与SARSA    183
7.6 Q-Learning    186
7.7 Q-Learning 的收敛性分析   189
7.8 从表格形式到价值模型   193
7.9 Deep Q Network    195
7.10 总结    202
7.11 参考资料    202
8 DQN 的改进算法203
8.1 Double Q-Learning    203
8.2 Priority Replay Buffer    204
8.3 Dueling DQN    209
8.4 解决DQN 的冷启动问题   211
8.5 Distributional DQN    214
8.5.1 输出价值分布   214
8.5.2 分布的更新    216
8.6 Noisy Network    218
8.7 Rainbow    221
8.7.1 Rainbow 的模型特点   221
8.7.2 Deep Q Network 的实现   223
8.8 总结    227
8.9 参考资料    227
第三部分基于策略梯度的算法
9 基于策略梯度的算法229
9.1 策略梯度法    229
9.1.1 算法推导    230
9.1.2 算法分析    233
9.1.3 算法改进    234
9.2 Actor-Critic 算法    236
9.2.1 降低算法的方差   236
9.2.2 A3C 算法    238
9.2.3 A2C 算法实战   240
9.3 总结    243
9.4 参考资料    243
10 使策略单调提升的优化算法244
10.1 TRPO    244
10.1.1 策略的差距    245
10.1.2 策略提升的目标公式   247
10.1.3 TRPO 的目标定义   248
10.1.4 自然梯度法求解   251
10.1.5 TRPO 的实现   254
10.2 GAE    256
10.2.1 GAE 的公式定义   256
10.2.2 基于GAE 和TRPO 的值函数优化  259
10.2.3 GAE 的实现    260
10.3 PPO    261
10.3.1 PPO 介绍    261
10.3.2 PPO 算法实践   263
10.4 总结    264
10.5 参考资料    264
11 Off-Policy 策略梯度法265
11.1 Retrace    266
11.1.1 Retrace 的基本概念   266
11.1.2 Retrace 的算法实现   267
11.2 ACER    270
11.2.1 Off-Policy Actor-Critic   270
11.2.2 ACER 算法    272
11.2.3 ACER 的实现   276
11.3 DPG    279
11.3.1 连续空间的策略优化   279
11.3.2 策略模型参数的一致性   280
11.3.3 DDPG 算法    283
11.3.4 DDPG 的实现   286
11.4 总结    289
11.5 参考资料    289
第四部分其他强化学习算法
12 稀疏回报的求解方法291
12.1 稀疏回报的困难    291
12.2 层次强化学习    294
12.3 HER    298
12.3.1 渐进式学习    299
12.3.2 HER 的实现    301
12.4 总结    304
12.5 参考资料    304
13 Model-based 方法305
13.1 AlphaZero    305
13.1.1 围棋游戏    305
13.1.2 Alpha-Beta 树   307
13.1.3 MCTS    309
13.1.4 策略价值模型   312
13.1.5 模型的对决    316
13.2 iLQR    316
13.2.1 线性模型的求解法   317
13.2.2 非线性模型的解法   322
13.2.3 iLQR 的实现    325
13.3 总结    328
13.4 参考资料    328
第五部分反向强化学习
14 反向强化学习入门330
14.1 基本概念    330
14.2 从最优策略求解回报   332
14.2.1 求解回报的目标函数   332
14.2.2 目标函数的约束   334
14.3 求解线性规划    335
14.3.1 线性规划的求解过程   335
14.3.2 实际案例    337
14.4 无限状态下的求解    338
14.5 从样本中学习    342
14.6 总结    344
14.7 参考资料    344
15 反向强化学习算法2.0 345
15.1 最大熵模型    345
15.1.1 指数家族    346
15.1.2 最大熵模型的推导   349
15.1.3 最大熵模型的实现   354
15.2 最大熵反向强化学习   356
15.3 GAIL    361
15.3.1 GAN 的基本概念   361
15.3.2 GAN 的训练分析   363
15.4 GAIL 实现    367
15.5 总结    370
15.6 参考资料    370
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>强化学习精要
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度學習快速入門
Chapter 1 TensorFlow：基本概念
機器學習與深度學習的基礎
TensorFlow：總體概述
Python的基礎
安裝TensorFlow
第一次實地操作
資料流圖形
TensorFlow程式設計模型
如何使用TensorBoard
總結
Chapter 2 用TensorFlow求解數學問題
張量資料結構
複數及碎形（fractals）
計算梯度（gradient）
隨機數值
總結
Chapter 3 機器學習簡介與應用
線性迴歸演算法
分類（Classifiers）
資料群集（Data clustering）
總結
Chapter 4 類神經網路簡介
什麼是類神經網路？
單層感知器
邏輯斯迴歸（logistic regression）
多層感知器
多層感知器函數近似（function approximation）
總結
Chapter 5 深度學習
深度學習技術
卷積神經網路CNN
CNN架構
CNN的TensorFlow實作
遞迴神經網路RNN
RNN架構
LSTM網路
使用TensorFlow進行自然語言處理
總結
Chapter 6 GPU程式設計和TensorFlow服務
GPU程式設計
TensorFlow服務（TensorFlow Serving）
如何安裝TensorFlow Serving
如何使用TensorFlow Serving
訓練和輸出模型
執行session
載入與輸出一個TensorFlow模型
測試伺服器
總結
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度學習快速入門
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解TensorFlow：架构设计与实现原理
第 一部分　基础篇
第　1章 TensorFlow系统概述　2
1.1　简介　2
1.1.1　产生背景　2
1.1.2　独特价值　3
1.1.3　版本变迁　4
1.1.4　与其他主流深度学习框架的对比　6
1.2　设计目标　7
1.2.1　灵活通用的深度学习库　8
1.2.2　端云结合的人工智能引擎　9
1.2.3　高性能的基础平台软件　10
1.3　基本架构　12
1.3.1　工作形态　12
1.3.2　组件结构　13
1.4　小结　14
第　2章 TensorFlow环境准备　15
2.1　安装　15
2.1.1　TensorFlow安装概述　15
2.1.2　使用Anaconda安装　17
2.1.3　使用原生pip安装　17
2.1.4　使用virtualenv安装　18
2.1.5　使用Docker安装　19
2.1.6　使用源代码编译安装　20
2.1.7　Hello TensorFlow　22
2.2　依赖项　23
2.2.1　Bazel软件构建工具　24
2.2.2　Protocol Buffers数据结构序列化工具　25
2.2.3　Eigen线性代数计算库　27
2.2.4　CUDA统一计算设备架构　28
2.3　源代码结构　29
2.3.1　根目录　29
2.3.2　tensorflow目录　30
2.3.3　tensorflow/core目录　31
2.3.4　tensorflow/python目录　32
2.3.5　安装目录　33
2.4　小结　33
第3章　TensorFlow基础概念　34
3.1　编程范式：数据流图　34
3.1.1　声明式编程与命令式编程　34
3.1.2　声明式编程在深度学习应用上的优势　35
3.1.3　TensorFlow数据流图的基本概念　38
3.2　数据载体：张量　40
3.2.1　张量：Tensor　40
3.2.2　稀疏张量：SparseTensor　44
3.3　模型载体：操作　46
3.3.1　计算节点：Operation　46
3.3.2　存储节点：Variable　49
3.3.3　数据节点：Placeholder　53
3.4　运行环境：会话　55
3.4.1　普通会话：Session　55
3.4.2　交互式会话：InteractiveSession　59
3.4.3　扩展阅读：会话实现原理　59
3.5　训练工具：优化器　61
3.5.1　损失函数与优化算法　61
3.5.2　优化器概述　64
3.5.3　使用minimize方法训练模型　66
3.5.4　扩展阅读：模型训练方法进阶　68
3.6　一元线性回归模型的最佳实践　72
3.7　小结　76
第二部分　关键模块篇
第4章　TensorFlow数据处理方法　78
4.1　输入数据集　78
4.1.1　使用输入流水线并行读取数据　78
4.1.2　创建批样例数据的方法　86
4.1.3　填充数据节点的方法　87
4.1.4　处理CIFAR-10数据集的最佳实践　88
4.1.5　扩展阅读：MNIST数据集　91
4.2　模型参数　92
4.2.1　模型参数的典型使用流程　92
4.2.2　使用tf.Variable创建、初始化和更新模型参数　92
4.2.3　使用tf.train.Saver保存和恢复模型参数　98
4.2.4　使用变量作用域处理复杂模型　100
4.3　命令行参数　103
4.3.1　使用argparse解析命令行参数　103
4.3.2　使用tf.app.flags解析命令行参数　108
4.4　小结　111
第5章　TensorFlow编程框架　112
5.1　单机程序编程框架　112
5.1.1　概述　112
5.1.2　创建单机数据流图　114
5.1.3　创建并运行单机会话　116
5.2　分布式程序编程框架　118
5.2.1　PS-worker架构概述　118
5.2.2　分布式程序编程框架概述　120
5.2.3　创建TensorFlow集群　121
5.2.4　将操作放置到目标设备　124
5.2.5　数据并行模式　124
5.2.6　同步训练机制　125
5.2.7　异步训练机制　130
5.2.8　使用Supervisor管理模型训练　131
5.2.9　分布式同步训练的最佳实践　133
5.3　小结　137
第6章　TensorBoard可视化工具　138
6.1　概述　138
6.2　可视化数据流图　142
6.2.1　名字作用域与抽象节点　142
6.2.2　可视化数据流图的最佳实践　144
6.2.3　扩展阅读：汇总数据和事件数据　145
6.2.4　扩展阅读：揭秘tf.summary.FileWriter工作原理　147
6.3　可视化学习过程　149
6.3.1　汇总操作概述　149
6.3.2　使用tf.summary.scalar生成折线图　150
6.3.3　使用tf.summary.histogram生成数据分布图　152
6.3.4　使用tf.summary.image生成图像　154
6.3.5　使用tf.summary.audio生成音频　155
6.3.6　可视化MNIST softmax模型学习过程的最佳实践　156
6.4　可视化高维数据　158
6.4.1　使用TensorBoard可视化高维数据　158
6.4.2　可视化MNIST数据集的最佳实践　160
6.5　小结　163
第7章　模型托管工具：TensorFlow Serving　164
7.1　概述　164
7.2　系统架构　165
7.3　安装　167
7.3.1　使用APT安装ModelServer　168
7.3.2　使用源码编译安装ModelServer　169
7.4　最佳实践　170
7.4.1　导出模型　170
7.4.2　发布模型服务　173
7.4.3　更新线上模型服务　174
7.5　小结　175
第三部分　算法模型篇
第8章　深度学习概述　178
8.1　深度学习的历史　178
8.1.1　感知机模型与神经网络　178
8.1.2　神经网络的寒冬与复苏　179
8.1.3　神经网络的发展与第二次寒冬　181
8.1.4　深度学习时代的到来　183
8.2　深度学习的主要应用　184
8.2.1　计算机视觉　185
8.2.2　自然语言处理　186
8.2.3　强化学习　188
8.3　深度学习与TensorFlow　190
8.4　小结　191
第9章　CNN模型　192
9.1　CNN　192
9.1.1　CNN简介　192
9.1.2　卷积层　193
9.1.3　激活层　195
9.1.4　池化层　195
9.1.5　全连接层　196
9.1.6　Dropout层　196
9.1.7　BN层　197
9.1.8　常用的CNN图像分类模型　197
9.2　TensorFlow-Slim　204
9.2.1　TensorFlow-Slim总体结构　204
9.2.2　datasets包和data包　205
9.2.3　preprocessing包　207
9.2.4　deployment包　207
9.2.5　nets包　209
9.2.6　TensorFlow-Slim最佳实践　212
9.3　应用　216
9.3.1　物体检测　216
9.3.2　图像分割　221
9.4　小结　222
第　10章 GAN模型　223
10.1　原理、特点及应用　223
10.1.1　原理　224
10.1.2　特点　225
10.1.3　应用　226
10.2　GAN模型的改进　228
10.2.1　CGAN模型　228
10.2.2　LAPGAN模型　229
10.2.3　DCGAN模型　230
10.2.4　InfoGAN模型　230
10.2.5　LSGAN模型　231
10.2.6　WGAN模型　232
10.3　最佳实践　233
10.4　小结　238
第　11章 RNN模型　239
11.1　基本RNN单元及其变种　239
11.1.1　RNN模型简介　239
11.1.2　基本RNN单元　240
11.1.3　LSTM单元　242
11.1.4　GRU单元　243
11.1.5　双向RNN单元　244
11.1.6　带有其他特性的RNN单元　245
11.2　RNN模型　247
11.2.1　PTB-LSTM语言模型　247
11.2.2　Seq2Seq模型　251
11.3　小结　254
第四部分　核心揭秘篇
第　12章 TensorFlow运行时核心设计与实现　256
12.1　运行时框架概述　256
12.2　关键数据结构　257
12.2.1　张量相关数据结构　258
12.2.2　设备相关数据结构　260
12.2.3　数据流图相关的数据结构　263
12.3　公共基础机制　266
12.3.1　内存分配　266
12.3.2　线程管理　268
12.3.3　多语言接口　269
12.3.4　XLA编译技术　270
12.3.5　单元测试框架　271
12.4　外部环境接口　272
12.4.1　加速器硬件接口　272
12.4.2　系统软件接口　275
12.5　小结　276
第　13章 通信原理与实现　277
13.1　概述　277
13.2　进程内通信　278
13.2.1　通信接口　278
13.2.2　会合点机制　280
13.2.3　异构设备内存访问　282
13.3　进程间通信　283
13.3.1　gRPC通信机制　284
13.3.2　控制通信　286
13.3.3　数据通信　290
13.4　RDMA通信模块　294
13.4.1　模块结构　295
13.4.2　消息语义　296
13.4.3　通信流程　297
13.5　小结　300
第　14章 数据流图计算原理与实现　301
14.1　概述　301
14.2　数据流图创建　302
14.2.1　流程与抽象　303
14.2.2　全图构造　305
14.2.3　子图提取　306
14.2.4　图切分　307
14.2.5　图优化　308
14.3　单机会话运行　308
14.3.1　流程与抽象　309
14.3.2　执行器获取　311
14.3.3　输入数据填充　312
14.3.4　图运行　313
14.3.5　输出数据获取　315
14.3.6　张量保存　315
14.4　分布式会话运行　315
14.4.1　主-从模型　316
14.4.2　主要抽象　317
14.4.3　client创建会话　319
14.4.4　client请求图运行　320
14.4.5　master驱动图运行　321
14.4.6　worker实施图运行　323
14.5　操作节点执行　325
14.5.1　核函数抽象　325
14.5.2　CPU上的执行流程　326
14.5.3　CUDA GPU上的执行流程　326
14.6　小结　327
第五部分　生态发展篇
第　15章 TensorFlow生态环境　330
15.1　生态环境概况　330
15.1.1　社区托管组件　330
15.1.2　第三方项目　333
15.2　深度神经网络库Keras　334
15.2.1　概述　334
15.2.2　模型概述　335
15.2.3　顺序模型　336
15.2.4　函数式模型　338
15.3　TensorFlow与Kubernetes生态的结合　340
15.4　TensorFlow与Spark生态的结合　344
15.5　TensorFlow通信优化技术　345
15.6　TPU及神经网络处理器　348
15.7　NNVM模块化深度学习组件　349
15.8　TensorFlow未来展望——TFX　351
15.9　小结　353
附录A　354
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解TensorFlow：架构设计与实现原理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从机器学习到深度学习
第1章  机器学习基础  1
1.1  引言  1
1.1.1  为什么使用机器学习  2
1.1.2  机器学习与数据挖掘  4
1.1.3  机器学习与人工智能  5
1.2  机器学习的一般流程  7
1.2.1  定义问题  7
1.2.2  收集数据  8
1.2.3  比较算法与模型  9
1.2.4  应用模型  10
1.3  学习策略  10
1.3.1  有监督学习  11
1.3.2  无监督学习  14
1.3.3  强化学习  16
1.3.4  综合模型与工具  18
1.4  评估理论  19
1.4.1  划分数据集  19
1.4.2  交叉验证  21
1.4.3  评估指标  22
1.4.4  拟合不足与过度拟合  25
1.5  本章内容回顾  26
第2章  Python基础工具  27
2.1  Numpy  28
2.1.1  Numpy与Scipy的分工  28
2.1.2  ndarray构造  29
2.1.3  数据类型  32
2.1.4  访问与修改  33
2.1.5  轴  35
2.1.6  维度操作  38
2.1.7  合并与拆分  40
2.1.8  增与删  41
2.1.9  全函数  42
2.1.10  广播  42
2.2  Matplot  43
2.2.1  点线图  44
2.2.2  子视图  50
2.2.3  图像  53
2.2.4  等值图  57
2.2.5  三维绘图  58
2.2.6  从官网学习  59
2.3  Scipy  60
2.3.1  数学与物理常数  61
2.3.2  特殊函数库  62
2.3.3  积分  64
2.3.4  优化  65
2.3.5  插值  67
2.3.6  离散傅里叶  68
2.3.7  卷积  70
2.3.8  线性分析  71
2.3.9  概率统计  73
2.4  本章内容回顾  77
第3章  有监督学习：分类与回归  79
3.1  线性回归  80
3.1.1  何谓线性模型  80
3.1.2  最小二乘法  81
3.1.3  最小二乘法的不足  82
3.1.4  岭回归  85
3.1.5  Lasso回归  87
3.2  梯度下降  90
3.2.1  假设函数与损失函数  90
3.2.2  随机梯度下降  92
3.2.3  实战：SGDRegressor和SGDClassifier  93
3.2.4  增量学习  94
3.3  支持向量机  95
3.3.1  最优超平面  95
3.3.2  软间隔  97
3.3.3  线性不可分问题  98
3.3.4  核函数  99
3.3.5  实战：scikit-learn中的SVM  100
3.4  朴素贝叶斯分类  101
3.4.1  基础概率  102
3.4.2  贝叶斯分类原理  103
3.4.3  高斯朴素贝叶斯  105
3.4.4  多项式朴素贝叶斯  106
3.4.5  伯努利朴素贝叶斯  107
3.5  高斯过程  107
3.5.1  随机过程  108
3.5.2  无限维高斯分布  109
3.5.3  实战：gaussian_process工具包  111
3.6  决策树  114
3.6.1  最易于理解的模型  114
3.6.2  熵的作用  115
3.6.3  实战：DecisionTreeClassifier与DecisionTreeRegressor  117
3.6.4  树的可视化  118
3.7  集成学习  119
3.7.1  偏差与方差  120
3.7.2  随机森林  121
3.7.3  自适应增强  124
3.8  综合话题  126
3.8.1  参数与非参数学习  127
3.8.2  One-Vs-All与One-Vs-One  127
3.8.3  评估工具  129
3.8.4  超参数调试  131
3.8.5  多路输出  134
3.9  本章内容回顾  134
第4章  无监督学习：聚类  136
4.1  动机  137
4.2  K-means  138
4.2.1  算法  139
4.2.2  实战：scikit-learn聚类调用  141
4.2.3  如何选择K值  144
4.3  近邻算法  145
4.3.1  生活化的理解  145
4.3.2  有趣的迭代  146
4.3.3  实战：AffinityPropagation类  147
4.4  高斯混合模型  149
4.4.1  中心极限定理  150
4.4.2  最大似然估计  151
4.4.3  几种协方差矩阵类型  152
4.4.4  实战：GaussianMixture类  154
4.5  密度聚类  156
4.5.1  凸数据集  157
4.5.2  密度算法  158
4.5.3  实战：DBSCAN类  159
4.6  BIRCH  160
4.6.1  层次模型综述  161
4.6.2  聚类特征树  162
4.6.3  实战：BIRCH相关调用  164
4.7  距离计算  166
4.7.1  闵氏距离  166
4.7.2  马氏距离  167
4.7.3  余弦相似度  168
4.7.4  时间序列比较  169
4.7.5  杰卡德相似度  169
4.8  聚类评估  170
4.9  本章内容回顾  172
第5章  无监督学习：数据降维  173
5.1  主成分分析  174
5.1.1  寻找方差最大维度  174
5.1.2  用PCA降维  177
5.1.3  实战：用PCA寻找主成分  178
5.2  线性判别分析  181
5.2.1  双重标准  181
5.2.2  实战：使用LinearDiscriminantAnalysis  183
5.3  多维标度法  185
5.3.1  保留距离信息的线性变换  185
5.3.2  MDS的重要变形  187
5.3.3  实战：使用MDS类  188
5.4  流形学习之Isomap  189
5.4.1  什么是流形  190
5.4.2  测地线距离  192
5.4.3  实战：使用Isomap类  193
5.5  流形学习之局部嵌入  195
5.5.1  局部线性嵌入  195
5.5.2  拉普拉斯特征映射（LE）  198
5.5.3  调用介绍  200
5.5.4  谱聚类  201
5.6  流形学习之t-SNE  203
5.6.1  用Kullback-Leiber衡量分布相似度  203
5.6.2  为什么是t-分布  205
5.6.3  实战：使用TSNE类  206
5.7  实战：降维模型之比较  207
5.8  本章内容回顾  210
第6章  隐马尔可夫模型  212
6.1  场景建模  213
6.1.1  两种状态链  213
6.1.2  两种概率  215
6.1.3  三种问题  217
6.1.4  hmmLearn介绍  218
6.2  离散型分布算法与应用  222
6.2.1  前向算法与后向算法  222
6.2.2  MultinomialNB求估计问题  226
6.2.3  Viterbi算法  227
6.2.4  MultinomialNB求解码问题  229
6.2.5  EM算法  232
6.2.6  Baum-Welch算法  233
6.2.7  用hmmLearn训练数据  235
6.3  连续型概率分布  236
6.3.1  多元高斯分布  237
6.3.2  GaussianHMM  239
6.3.3  GMMHMM  240
6.4  实战：股票预测模型  241
6.4.1  数据模型  241
6.4.2  目标  243
6.4.3  训练模型  243
6.4.4  分析模型参数  245
6.4.5  可视化短线预测  247
6.5  本章内容回顾  250
第7章  贝叶斯网络  251
7.1  什么是贝叶斯网络  252
7.1.1  典型贝叶斯问题  252
7.1.2  静态结构  253
7.1.3  联合/边缘/条件概率换算  256
7.1.4  链式法则与变量消元  258
7.2  网络构建  259
7.2.1  网络参数估计  260
7.2.2  启发式搜索  261
7.2.3  Chow-Liu Tree算法  262
7.3  近似推理  263
7.3.1  蒙特卡洛方法  264
7.3.2  马尔可夫链收敛定理  265
7.3.3  MCMC推理框架  267
7.3.4  Gibbs采样  268
7.3.5  变分贝叶斯  268
7.4  利用共轭建模  270
7.4.1  共轭分布  270
7.4.2  隐含变量与显式变量  272
7.5  实战：胸科疾病诊断  274
7.5.1  诊断需求  274
7.5.2  Python概率工具包  275
7.5.3  建立模型  276
7.5.4  MCMC采样分析  278
7.5.5  近似推理  281
7.6  本章内容回顾  282
第8章  自然语言处理  284
8.1  文本建模  285
8.1.1  聊天机器人原理  285
8.1.2  词袋模型  286
8.1.3  访问新闻资源库  287
8.1.4  TF-IDF  290
8.1.5  实战：关键词推举  290
8.2  词汇处理  294
8.2.1  中文分词  294
8.2.2  Word2vec  296
8.2.3  实战：寻找近似词  298
8.3  主题模型  303
8.3.1  三层模型  303
8.3.2  非负矩阵分解  304
8.3.3  潜在语意分析  305
8.3.4  隐含狄利克雷分配  307
8.3.5  实战：使用工具包  309
8.4  实战：用LDA分析新闻库  311
8.4.1  文本预处理  311
8.4.2  训练与显示  313
8.4.3  困惑度调参  315
8.5  本章内容回顾  317
第9章  深度学习  319
9.1  神经网络基础  320
9.1.1  人工神经网络  320
9.1.2  神经元与激活函数  321
9.1.3  反向传播  323
9.1.4  万能网络  325
9.2  TensorFlow核心应用  328
9.2.1  张量  329
9.2.2  开发架构  331
9.2.3  数据管理  332
9.2.4  评估器  335
9.2.5  图与会话  338
9.2.6  逐代（epoch）训练  341
9.2.7  图与统计可视化  343
9.3  卷积神经网络  349
9.3.1  给深度学习一个理由  349
9.3.2  CNN结构发展  351
9.3.3  卷积层  354
9.3.4  池化层  356
9.3.5  ReLU与Softmax  357
9.3.6  Inception与ResNet  359
9.4  优化  362
9.4.1  批次规范化  362
9.4.2  剪枝  364
9.4.3  算法选择  366
9.5  循环神经网络与递归神经网络  367
9.5.1  循环神经网络  368
9.5.2  长短期记忆（LSTM）  371
9.5.3  递归神经网络  374
9.6  前沿精选  377
9.6.1  物件检测模型  377
9.6.2  密连卷积网络  381
9.6.3  胶囊网络  382
9.7  CNN实战：图像识别  385
9.7.1  开源图像库CIFAR  385
9.7.2  项目介绍  388
9.7.3  构建Graph  389
9.7.4  优化与训练  392
9.7.5  运行  394
9.8  RNN实战：写诗机器人  397
9.8.1  语言模型  397
9.8.2  LSTM开发步骤1：网络架构  401
9.8.3  LSTM开发步骤2：数据加载  402
9.8.4  LSTM开发步骤3：搭建TensorFlow Graph  403
9.8.5  LSTM开发步骤4：解析LSTM RNN  404
9.8.6  LSTM开发步骤5：LSTM中的参数  406
9.8.7  LSTM开发步骤6：用sequence_loss计算RNN损失值  406
9.8.8  LSTM开发步骤7：学习速度可调优化器  407
9.8.9  LSTM开发步骤8：训练  408
9.8.10  开始写唐诗  410
9.8.11  写唐诗步骤1：用唐诗语料训练语言模型  410
9.8.12  写唐诗步骤2：作诗  412
9.8.13  写唐诗步骤3：作品举例  414
9.9  本章内容回顾  415
第10章  强化学习  418
10.1  场景与原理  419
10.1.1  借AlphaGo谈人工智能  419
10.1.2  基于价值的算法Q-Learning与Sarsa  421
10.1.3  基于策略的算法  424
10.1.4  基于模型的算法  426
10.2  OpenAI Gym  427
10.2.1  环境调用  428
10.2.2  实战：用Q-Learning开发走迷宫机器人  432
10.3  深度强化学习  435
10.3.1  DQN及改进  435
10.3.2  DPN、DDPG及A3C  436
10.3.3  实战：用DPN训练月球定点登陆  439
10.4  博弈原理  444
10.4.1  深度搜索与广度搜索  444
10.4.2  完美决策  446
10.4.3  蒙特卡洛搜索树  448
10.5  实战：中国象棋版AlphaGo Zero  449
10.5.1  开源版本AlphaGo Zero  450
10.5.2  盘面建模  452
10.5.3  左右互搏  457
10.5.4  MCTS详解  464
10.5.5  DDPG详解  468
10.5.6  运行展示：训练  473
10.5.7  运行展示：查看统计  475
10.5.8  运行展示：当头炮、把马跳  475
10.5.9  运行展示：人机博弈  476
10.6  本章内容回顾  477
第11章  模型迁移  478
11.1  走向移动端  478
11.1.1  Android上的TensorFlow  479
11.1.2  iOS上的CoreML  480
11.2  迁移学习  483
11.2.1  动机  483
11.2.2  训练流程  484
11.3  案例实战：基于TensorFlow Hub的迁移学习开发  485
11.3.1  下载并训练  485
11.3.2  检验学习成果  486
11.3.3  迁移学习开发  487
11.4  本章内容回顾  488
后记  489
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从机器学习到深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow机器学习项目实战
第1章　探索和转换数据 1
1.1　TensorFlow的主要数据结构—
张量 1
1.1.1　张量的属性—阶、形状和
类型 1
1.1.2　创建新的张量 3
1.1.3　动手工作—与TensorFlow
交互 4
1.2　处理计算工作流—TensorFlow
的数据流图 5
1.2.1　建立计算图 5
1.2.2　数据供给 6
1.2.3　变量 6
1.2.4　保存数据流图 6
1.3　运行我们的程序—会话 8
1.4　基本张量方法 8
1.4.1　简单矩阵运算 8
1.4.2　序列 11
1.4.3　张量形状变换 12
1.4.4　数据流结构和结果可视化—
TensorBoard 14
1.5　从磁盘读取信息 18
1.5.1　列表格式—CSV 18
1.5.2　读取图像数据 19
1.5.3　加载和处理图像 20
1.5.4　读取标准TensorFlow格式 21
1.6　小结 21
第2章　聚类 22
2.1　从数据中学习—无监督学习 22
2.2　聚类的概念 22
2.3　k均值 23
2.3.1　k均值的机制 23
2.3.2　算法迭代判据 23
2.3.3　k均值算法拆解 24
2.3.4　k均值的优缺点 25
2.4　k最近邻 25
2.4.1　k最近邻算法的机制 26
2.4.2　k-nn的优点和缺点 26
2.5　有用的库和使用示例 27
2.5.1　matplotlib绘图库 27
2.5.2　scikit-learn数据集模块 28
2.5.3　人工数据集类型 28
2.6　例1—对人工数据集的k均值
聚类 29
2.6.1　数据集描述和加载 29
2.6.2　模型架构 30
2.6.3　损失函数描述和优化循环 31
2.6.4　停止条件 31
2.6.5　结果描述 31
2.6.6　每次迭代中的质心变化 32
2.6.7　完整源代码 32
2.6.8　k均值用于环状数据集 34
2.7　例2—对人工数据集使用最近
邻算法 36
2.7.1　数据集生成 36
2.7.2　模型结构 36
2.7.3　损失函数描述 37
2.7.4　停止条件 37
2.7.5　结果描述 37
2.7.6　完整源代码 37
2.8　小结 39
第3章　线性回归 40
3.1　单变量线性模型方程 40
3.2　选择损失函数 41
3.3　最小化损失函数 42
3.3.1　最小方差的全局最小值 42
3.3.2　迭代方法：梯度下降 42
3.4　示例部分 43
3.4.1　TensorFlow中的优化方法—
训练模块 43
3.4.2　tf.train.Optimizer类 43
3.4.3　其他Optimizer实例类型 44
3.5　例1—单变量线性回归 44
3.5.1　数据集描述 45
3.5.2　模型结构 45
3.5.3　损失函数描述和Optimizer 46
3.5.4　停止条件 48
3.5.5　结果描述 48
3.5.6　完整源代码 49
3.6　例2—多变量线性回归 51
3.6.1　有用的库和方法 51
3.6.2　Pandas库 51
3.6.3　数据集描述 51
3.6.4　模型结构 53
3.6.5　损失函数和Optimizer 54
3.6.6　停止条件 55
3.6.7　结果描述 55
3.6.8　完整源代码 56
3.7　小结 57
第4章　逻辑回归 58
4.1　问题描述 58
4.2　Logistic函数的逆函数—Logit
函数 59
4.2.1　伯努利分布 59
4.2.2　联系函数 60
4.2.3　Logit函数 60
4.2.4　对数几率函数的逆函数—
Logistic函数 60
4.2.5　多类分类应用—Softmax
回归 62
4.3　例1—单变量逻辑回归 64
4.3.1　有用的库和方法 64
4.3.2　数据集描述和加载 65
4.3.3　模型结构 67
4.3.4　损失函数描述和优化器
循环 67
4.3.5　停止条件 68
4.3.6　结果描述 68
4.3.7　完整源代码 69
4.3.8　图像化表示 71
4.4　例2—基于skflow单变量逻辑
回归 72
4.4.1　有用的库和方法 72
4.4.2　数据集描述 72
4.4.3　模型结构 72
4.4.4　结果描述 73
4.4.5　完整源代码 74
4.5　小结 74
第5章　简单的前向神经网络 75
5.1　基本概念 75
5.1.1　人工神经元 75
5.1.2　神经网络层 76
5.1.3　有用的库和方法 78
5.2　例1—非线性模拟数据
回归 79
5.2.1　数据集描述和加载 79
5.2.2　数据集预处理 80
5.2.3　模型结构—损失函数
描述 80
5.2.4　损失函数优化器 80
5.2.5　准确度和收敛测试 80
5.2.6　完整源代码 80
5.2.7　结果描述 81
5.3　例2—通过非线性回归，对
汽车燃料效率建模 82
5.3.1　数据集描述和加载 82
5.3.2　数据预处理 83
5.3.3　模型架构 83
5.3.4　准确度测试 84
5.3.5　结果描述 84
5.3.6　完整源代码 84
5.4　例3—多类分类：葡萄酒
分类 86
5.4.1　数据集描述和
加载 86
5.4.2　数据集预处理 86
5.4.3　模型架构 87
5.4.4　损失函数描述 87
5.4.5　损失函数优化器 87
5.4.6　收敛性测试 88
5.4.7　结果描述 88
5.4.8　完整源代码 88
5.5　小结 89
第6章　卷积神经网络 90
6.1　卷积神经网络的起源 90
6.1.1　卷积初探 90
6.1.2　降采样操作—池化 95
6.1.3　提高效率—dropout
操作 98
6.1.4　卷积类型层构建办法 99
6.2　例1—MNIST数字分类 100
6.2.1　数据集描述和加载 100
6.2.2　数据预处理 102
6.2.3　模型结构 102
6.2.4　损失函数描述 103
6.2.5　损失函数优化器 103
6.2.6　准确性测试 103
6.2.7　结果描述 103
6.2.8　完整源代码 104
6.3　例2—CIFAR10数据集的图像
分类 106
6.3.1　数据集描述和加载 107
6.3.2　数据集预处理 107
6.3.3　模型结构 108
6.3.4　损失函数描述和
优化器 108
6.3.5　训练和准确性测试 108
6.3.6　结果描述 108
6.3.7　完整源代码 109
6.4　小结 110
第7章　循环神经网络和LSTM 111
7.1　循环神经网络 111
7.1.1　梯度爆炸和梯度消失 112
7.1.2　LSTM神经网络 112
7.1.3　其他RNN结构 116
7.1.4　TensorFlow LSTM有用的类和
方法 116
7.2　例1—能量消耗、单变量时间序
列数据预测 117
7.2.1　数据集描述和加载 117
7.2.2　数据预处理 118
7.2.3　模型结构 119
7.2.4　损失函数描述 121
7.2.5　收敛检测 121
7.2.6　结果描述 122
7.2.7　完整源代码 122
7.3　例2—创作巴赫风格的
曲目 125
7.3.1　字符级模型 125
7.3.2　字符串序列和概率表示 126
7.3.3　使用字符对音乐编码—
ABC音乐格式 126
7.3.4　有用的库和方法 128
7.3.5　数据集描述和加载 129
7.3.6　网络训练 129
7.3.7　数据集预处理 130
7.3.8　损失函数描述 131
7.3.9　停止条件 131
7.3.10　结果描述 131
7.3.11　完整源代码 132
7.4　小结 137
第8章　深度神经网络 138
8.1　深度神经网络的定义 138
8.2　深度网络结构的历史变迁 138
8.2.1　LeNet 5 138
8.2.2　Alexnet 139
8.2.3　VGG模型 139
8.2.4　第一代Inception模型 140
8.2.5　第二代Inception模型 141
8.2.6　第三代Inception模型 141
8.2.7　残差网络（ResNet） 142
8.2.8　其他的深度神经网络
结构 143
8.3　例子—VGG艺术风格转移 143
8.3.1　有用的库和方法 143
8.3.2　数据集描述和加载 143
8.3.3　数据集预处理 144
8.3.4　模型结构 144
8.3.5　损失函数 144
8.3.6　收敛性测试 145
8.3.7　程序执行 145
8.3.8　完整源代码 146
8.4　小结 153
第9章　规模化运行模型—GPU和
服务 154
9.1　TensorFlow中的GPU支持 154
9.2　打印可用资源和设备参数 155
9.2.1　计算能力查询 155
9.2.2　选择CPU用于计算 156
9.2.3　设备名称 156
9.3　例1—将一个操作指派给
GPU 156
9.4　例2—并行计算Pi的数值 157
9.4.1　实现方法 158
9.4.2　源代码 158
9.5　分布式TensorFlow 159
9.5.1　分布式计算组件 159
9.5.2　创建TensorFlow集群 160
9.5.3　集群操作—发送计算方法
到任务 161
9.5.4　分布式编码结构示例 162
9.6　例3—分布式Pi计算 163
9.6.1　服务器端脚本 163
9.6.2　客户端脚本 164
9.7　例4—在集群上运行分布式
模型 165
9.8　小结 168
第10章　库的安装和其他技巧 169
10.1　Linux安装 169
10.1.1　安装要求 170
10.1.2　Ubuntu安装准备（安装操作的
前期操作） 170
10.1.3　Linux下通过pip安装
TensorFlow 170
10.1.4　Linux下从源码安装
TensorFlow 175
10.2　Windows安装 179
10.2.1　经典的Docker工具箱
方法 180
10.2.2　安装步骤 180
10.3　MacOS X安装 183
10.4　小结 185

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow机器学习项目实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow机器学习实战指南
目　　录
译者序
作者简介
审校者简介
前言
第1章 TensorFlow基础 1
1.1 TensorFlow介绍 1
1.2 TensorFlow如何工作 1
1.3 声明张量 3
1.4 使用占位符和变量 6
1.5 操作（计算）矩阵 7
1.6 声明操作 10
1.7 实现激励函数 12
1.8 读取数据源 14
1.9 学习资料 19
第2章 TensorFlow进阶 20
2.1 本章概要 20
2.2 计算图中的操作 20
2.3 TensorFlow的嵌入Layer 21
2.4 TensorFlow的多层Layer 23
2.5 TensorFlow实现损失函数 26
2.6 TensorFlow实现反向传播 30
2.7 TensorFlow实现随机训练和批量训练 34
2.8 TensorFlow实现创建分类器 37
2.9 TensorFlow实现模型评估 40
第3章 基于TensorFlow的线性回归 45
3.1 线性回归介绍 45
3.2 用TensorFlow求逆矩阵 45
3.3 用TensorFlow实现矩阵分解 47
3.4 用TensorFlow实现线性回归算法 49
3.5 理解线性回归中的损失函数 52
3.6 用TensorFlow实现戴明回归算法 55
3.7 用TensorFlow实现lasso回归和岭回归算法 58
3.8 用TensorFlow实现弹性网络回归算法 60
3.9 用TensorFlow实现逻辑回归算法 62
第4章 基于TensorFlow的支持向量机 66
4.1 支持向量机简介 66
4.2 线性支持向量机的使用 67
4.3 弱化为线性回归 72
4.4 TensorFlow上核函数的使用 77
4.5 用TensorFlow实现非线性支持向量机 82
4.6 用TensorFlow实现多类支持向量机 85
第5章 最近邻域法 90
5.1 最近邻域法介绍 90
5.2 最近邻域法的使用 91
5.3 如何度量文本距离 95
5.4 用TensorFlow实现混合距离计算 98
5.5 用TensorFlow实现地址匹配 101
5.6 用TensorFlow实现图像识别 105
第6章 神经网络算法 109
6.1 神经网络算法基础 109
6.2 用TensorFlow实现门函数 110
6.3 使用门函数和激励函数 113
6.4 用TensorFlow实现单层神经网络 117
6.5 用TensorFlow实现神经网络常见层 120
6.6 用TensorFlow实现多层神经网络 126
6.7 线性预测模型的优化 131
6.8 用TensorFlow基于神经网络实现井字棋 136
第7章 自然语言处理 143
7.1 文本处理介绍 143
7.2 词袋的使用 144
7.3 用TensorFlow实现TF-IDF算法 149
7.4 用TensorFlow实现skip-gram模型 155
7.5 用TensorFlow实现CBOW词嵌入模型 162
7.6 使用TensorFlow的Word2Vec预测 167
7.7 用TensorFlow实现基于Doc2Vec的情感分析 172
第8章 卷积神经网络 181
8.1 卷积神经网络介绍 181
8.2 用TensorFlow实现简单的CNN 182
8.3 用TensorFlow实现进阶的CNN 188
8.4 再训练已有的CNN模型 196
8.5 用TensorFlow实现模仿大师绘画 199
8.6 用TensorFlow实现DeepDream 205
第9章 递归神经网络 211
9.1 递归神经网络介绍 211
9.2 用TensorFlow实现RNN模型进行垃圾短信预测 212
9.3 用TensorFlow实现LSTM模型 218
9.4 Stacking多个LSTM Layer 226
9.5 用TensorFlow实现Seq2Seq翻译模型 229
9.6 TensorFlow实现孪生RNN预测相似度 235
第10章 TensorFlow产品化 243
10.1 简介 243
10.2 TensorFlow的单元测试 243
10.3 TensorFlow的并发执行 247
10.4 分布式TensorFlow实践 250
10.5 TensorFlow产品化开发提示 252
10.6 TensorFlow产品化的实例 254
第11章 TensorFlow的进阶应用 257
11.1 简介 257
11.2 TensorFlow可视化：Tensorboard 257
11.3 Tensorboard的进阶 260
11.4 用TensorFlow实现遗传算法 262
11.5 TensorFlow实现k-means算法 266
11.6 用TensorFlow求解常微分方程问题 270
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow机器学习实战指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow機械学習クックブック
第1章 TensorFlowの仕組みと基本事項を押さえる
第2章 TensorFlowスタイル
―演算/層の追加、損失関数やモデル評価などの実装
第3章 線形回帰
―逆行列/分解法からロジスティック回帰まで
第4章 サポートベクトルマシン
―線形SVMの操作/次元縮約、非線形SVM/多クラスSVMの実装など
第5章 最近傍法
―編集距離、距離関数の組み合わせ、最近傍法の画像認識など
第6章 ニューラルネットワーク
―論理ゲート、単層/多層ニューラルネットワークの実装など
第7章 自然言語処理
―BoW/TF-IDF/スキップグラム/CBOWなど
第8章 畳み込みニューラルネットワーク
―単純なCNN/高度なCNN/モデルの再トレーニングなど
第9章 リカレントニューラルネットワーク
―LSTM/Sequence-to-Sequence/Siamese Similarity法
第10章 TensorFlowを運用環境で使用する
第11章 TensorFlowをさらに活用する
―遺伝的アルゴリズム/連立常微分方程式など
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow機械学習クックブック
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>基于TensorFlow的深度学习
目录
前言 1
第1章 深度学习概述 5
1.1 机器学习吞噬计算机科学 .5
1.2 深度学习原型 6
1.3 深度学习架构 10
1.4 深度学习框架 19
1.5 小结 20
第2章 TensorFlow原型概述 21
2.1 张量介绍 21
2.2 TensorFlow中的基本计算 32
2.3 命令式和声明式编程 40
2.4 小结 44
第3章 使用TensorFlow进行线性和Logistic回归 45
3.1 数学回顾 45
3.2 学习TensorFlow 56
3.3 在TensorFlow中训练线性和Logistics模型 66
3.4 小结 78
第4章 全连接深层网络 81
4.1 什么是全连接深层网络？ 81
4.2 全连接网络中的“神经元”.83
4.3 训练全连接神经网络 89
4.4 在TensorFlow中实现 95
4.5 小结 .100
第5章 超参数优化 103
5.1 模型评估与超参数优化 .104
5.2 指标，指标，指标  105
5.3 超参数调优算法 111
5.4 小结 .117
第6章 卷积神经网络 118
6.1卷积结构概述 119
6.2 卷积网络的应用 125
6.3 用TensorFlow训练卷积网络 132
6.4 小结 .144
第7章 递归神经网络 145
7.1 递归结构概述 .146
7.2 循环神经元 148
7.3 递归模型的应用 150
7.4 神经网络图灵机 153
7.5 递归神经网络的实际应用 155
7.6 处理Penn Treebank语料库 155
7.7 小结  163
第8章 强化学习 164
8.1 马尔科夫决策过程 .168
8.2 强化学习算法 .170
8.3 强化学习的局限性 .174
8.4 玩转tic-tac-toe 175
8.5 A3C算法 187
8.6 小结 .196
第9章 训练大型深度网络 .198
9.1 为深度网络自定义硬件 .198
9.2 使用CPU训练 199
9.3 分布式深度网络训练 204
9.4 在Cifar10上与多GPS进行数据并行训练 206
9.5 小结 .215
第10章 深度学习的未来 216
10.1 技术行业以外的深度学习 .216
10.2 道德地使用深度学习 219
10.3 通用人工智能是否迫在眉睫？ .221
10.4 接下来，何去何从？ 222
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>基于TensorFlow的深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习-基于Python语言和TensorFlow平台
第1章　人工智能极简历史　1
1.1　重要的奠基时期　2
1.1.1　神经元的研究和人工神经元模型的提出　2
1.1.2　计算机和程序的出现　3
1.1.3　图灵测试的提出　4
1.2　人工智能的诞生　4
1.3　第一个快速发展期　5
1.4　人工智能的第一个寒冬　5
1.5　人工智能研究的沉默探索与复苏　6
1.6　人工智能的第二个冬天　9
1.7　再一次腾飞　9
1.7.1　计算机综合计算能力的大幅提升　9
1.7.2　大数据的出现　11
1.7.3　神经网络研究的成熟化　11
1.8　未来展望　13
1.9　本章小结：历史指引未来　18
第2章　开发环境准备　19
2.1　安装Python　20
2.1.1　Windows操作系统下安装Python　20
2.1.2　Mac OS X操作系统下安装Python　29
2.1.3　Linux操作系统下安装Python　30
2.2　安装TensorFlow　30
2.3　打造更舒适的开发环境　32
2.3.1　修改Windows资源管理器的一些显示设置　32
2.3.2　命令提示符CMD的替代方案　34
2.3.3　文本文件编辑器　36
2.3.4　Python语言专用的开发工具　40
2.4　知识背景准备　45
2.4.1　怎样输入Python程序　45
2.4.2　怎样执行Python程序　45
2.4.3　变量　46
2.4.4　函数（方法）　50
2.4.5　对象　51
2.4.6　条件判断与分支　53
2.4.7　循环　54
2.4.8　注释　55
2.4.9　程序运行时出现错误怎么办　55
2.4.10　本章小结：一段示例代码　56
第3章　初识TensorFlow　57
3.1　三好学生成绩问题的引入　58
3.2　搭建解决三好学生成绩问题的神经网络　58
3.3　训练神经网络　62
3.4　本章小结：解决的第一个问题　68
3.5　练习　68
第4章　简化神经网络模型　69
4.1　在程序运行中查看变量取值　70
4.2　张量概念的引入　70
4.3　用向量重新组织输入数据　72
4.4　简化的神经网络模型　75
4.5　概念补充——标量、多维数组等　76
4.5.1　标量　76
4.5.2　多维数组　76
4.5.3　张量的阶和形态　77
4.6　在TensorFlow中查看和设定张量的形态　78
4.7　用softmax函数来规范可变参数　81
4.8　本章小结：线性问题　83
4.9　练习　84
第5章　用神经网络解决非线性问题　85
5.1　非线性问题的引入　86
5.1.1　三好学生评选结果问题　86
5.1.2　二分类问题：是否为三好学生　86
5.1.3　非线性问题　87
5.2　设计神经网络模型　88
5.2.1　激活函数sigmoid　88
5.2.2　使用sigmoid函数后的神经网络模型　89
5.2.3　实现本模型的代码　89
5.3　准备训练数据　90
5.3.1　随机数　90
5.3.2　产生随机训练数据　90
5.4　完整的训练代码　92
5.4.1　使用随机数据进行训练　92
5.4.2　加入偏移量b加快训练过程　94
5.5　进阶：批量生成随机训练数据　97
5.6　本章小结：非线性问题　100
5.7　练习　100
第6章　从文件中载入训练数据　101
6.1　用纯文本文件准备训练数据　102
6.1.1　数据的数字化　102
6.1.2　训练数据的格式　102
6.1.3　数据整理　103
6.1.4　使用CSV格式文件辅助处理数据　104
6.2　加载文件中的训练数据　106
6.2.1　加载函数　106
6.2.2　非数字列的舍弃　106
6.2.3　非数字列与数字列的转换　107
6.2.4　行数据的分拆及如何“喂”给训练过程　108
6.3　本章小结：读取训练数据最常用的方式　110
6.4　练习　110
第7章　多层全连接神经网络　111
7.1　身份证问题的引入　112
7.2　问题分析　112
7.3　单层网络的模型　112
7.4　多层全连接神经网络　115
7.4.1　矩阵乘法　115
7.4.2　如何用矩阵乘法实现全连接层　116
7.4.3　使用均方误差作为计算误差的方法　119
7.4.4　激活函数tanh　120
7.4.5　新的模型　121
7.5　身份证问题新模型的代码实现　121
7.6　进一步优化模型和代码　124
7.7　本章小结：多层、全连接、线性与非线性　125
7.8　练习　126
第8章　保存和载入训练过程　127
8.1　保存训练过程　128
8.2　载入保存的训练过程并继续训练　130
8.3　通过命令行参数控制是否强制重新开始训练　132
8.4　训练过程中手动保存　135
8.5　保存训练过程前征得同意　137
8.6　本章小结：善于利用保存和载入训练过程　139
8.7　练习　139
第9章　查看图形化的模型　140
9.1　数据流图的概念　141
9.2　用TensorBoard查看数据流图　141
9.3　控制TensorBoard图中对象的名称　143
9.4　本章小结：图形化的模型　145
9.5　练习　145
第10章　用训练好的模型进行预测　146
10.1　从命令行参数读取需要预测的数据　147
10.2　从文件中读取数据进行预测　149
10.3　从任意字符串中读取数据进行预测　152
10.4　本章小结：预测与训练的区别　154
10.5　练习　154
第11章　用高级工具简化建模和训练过程　155
11.1　Keras框架介绍　156
11.2　用Keras实现神经网络模型　156
11.3　用Keras进行预测　158
11.4　保存和载入Keras模型　160
11.5　本章小结：方便与灵活度的取舍　161
11.6　练习　161
第12章　在其他语言中调用TensorFlow模型　162
12.1　如何保存模型　163
12.2　在Java语言中载入TensorFlow模型并进行预测计算　165
12.3　在Go语言中载入TensorFlow模型并进行预测计算　167
12.4　本章小结：仅能预测　167
第13章　用卷积神经网络进行图像识别　169
13.1　情凭谁来定错对——一首歌引出的对错问题　170
13.2　卷积神经网络介绍　170
13.2.1　卷积神经网络的基本概念　170
13.2.2　数字图片在计算机中的表达形式　170
13.2.3　卷积层的具体计算过程　172
13.2.4　卷积层的原理和优点　174
13.2.5　卷积神经网络的典型结构　177
13.3　用卷积网络实现图像识别　177
13.3.1　钩叉问题的图像数据格式　177
13.3.2　准备钩叉问题的训练数据　178
13.3.3　设计钩叉问题的神经网络模型并实现　179
13.4　本章小结：进一步优化的方向　183
13.5　练习　183
第14章　循环神经网络初探　184
14.1　循环神经网络简介　185
14.2　长短期记忆模型LSTM的作用　186
14.3　汇率预测问题的引入　186
14.4　用于汇率预测的LSTM神经网络模型　187
14.5　实现汇率预测LSTM网络的代码　188
14.6　用循环神经网络来进行自然语言处理　193
14.7　本章小结：时序有关问题　195
14.8　练习　195
第15章　优化器的选择与设置　196
15.1　优化器的作用　197
15.2　梯度下降算法　197
15.3　学习率的影响　198
15.4　主流优化方法介绍　199
15.5　优化器效率对比　200
15.6　本章小结：渡河之筏　203
第16章　下一步学习方向指南　204
16.1　更多的激活函数　205
16.2　更多的隐藏层类型　205
16.3　确定最适合的神经网络类型　206
16.4　GPU版本　206
16.5　有监督学习与无监督学习　207
16.6　深度学习进阶　207
16.7　升级到最新的TensorFlow版本　207
16.8　本章小结：最后的实例　208
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习-基于Python语言和TensorFlow平台
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>零起点TensorFlow快速入门
第1章  不朽的梵高	1
1.1  星夜传奇	1
1.2  TensorFlow简介	3
案例1-1：星夜传奇实战编程	5
案例1-2：星夜传奇批量编程	11
1.3  十万次迭代	14
1.4  黑箱大法	16
1.5  超级KISS法则与大智若愚	17
第2章  TensorFlow安装	22
2.1  目录结构	22
2.2  化繁为简	23
2.3  CPU版本安装	25
2.4  GPU版本安装	25
案例2-1：GPU开发环境测试	28
第3章  可视化AI图表	33
3.1  TensorBoard可视化工具	33
案例3-1：hello，爱丽丝	34
3.1.1  TensorBoard主界面	36
3.1.2  快速启动脚本	38
3.2  加法器	41
案例3-2：加法器的算法结构图	41
案例3-3：加法器的变化版本	43
案例3-4：乘法器的算法结构图	44
第4章  快速入门	47
4.1  你好，神经网络	47
案例4-1：字符串表达式	47
案例4-2：hello，TensorFlow	48
4.2  图运算与Session	49
案例4-3：缺省图运算	49
案例4-4：可视化Session	52
4.3  常量、变量与占位符	56
案例4-5：常量加法	57
案例4-6：加法与占位符	58
案例4-7：加法与变量	59
4.4  TensorFlow常用数值运算	62
第5章  TensorFlow总览	63
5.1  TensorFlow产业链关系图	65
5.2  TensorFlow模块列表	66
5.2.1  源码目录结构	66
案例5-1：内置模块列表	68
5.2.2  模块结构图	70
5.2.3  API抽象接口示意图	71
5.2.4  神经网络三大模块	71
5.3  数据类型	72
5.4  TensorFlow常用术语	73
5.5  TensorFlow简化接口	77
第6章  基础知识	79
6.1  数据流图	79
6.2  设备切换Device	81
6.3  三大数据类型	82
案例6-1：变量操作	86
6.4  Feed数据提交	89
案例6-2：Feed提交数据	90
案例6-3：批量Feed提交数据	92
案例6-4：批量Feed提交多维数据	92
6.5  Fetch获取数据	93
案例6-5：Fetch获取数据	93
案例6-6：Fetch获取多维数组	95
案例6-7：会话Session	97
6.6  批尺寸Batch_Size	99
第7章  孤独的神经元	101
7.1  神经元模型	101
案例7-1：单细胞算法	102
7.2  可视化分析	107
案例7-2：单细胞算法优化版	108
第8章  归来吧，数据	112
8.1  分类——机器学习的核心	112
8.2  万物皆回归	112
案例8-1：传统机器学习	114
案例8-2：TensorFlow线性回归模型	117
8.3  模型管理	124
案例8-3：TensorFlow模型保存	125
案例8-4：TensorFlow模型读取	126
第9章  Pkmital入门案例套餐（上）	128
9.1  Halcon简介	128
9.2  帕拉格•库马尔案例合集简介	129
9.3  Pkmital案例集合详解	131
9.4  TensorFlow基础	132
案例9-1：TensorFlow基础权重设置和图形计算	132
案例9-2：图像的卷积计算	140
9.5  回归算法	145
案例9-3：线性回归	145
案例9-4：线性回归修正版	150
9.6  多项式回归	151
案例9-5：多项式回归	151
案例9-6：多项式回归修正版	153
9.7  逻辑回归模型	154
案例9-7：逻辑回归模型	154
9.8  CNN卷积神经网络算法	159
案例9-8：CNN卷积神经网络	159
第10章  Pkmital入门案例套餐（下）	165
10.1  自编码算法	165
案例10-1：自编码算法	165
10.2  dAE降噪自编码算法	170
案例10-2：dAE降噪自编码算法	170
10.3  CAE卷积编码算法	177
案例10-3：CAE卷积编码算法	177
10.4  DRN深度残差网络	183
案例10-4：DRN深度残差网络	183
10.5  VAE变分自编码算法	189
案例10-5：VAE变分自编码算法	189
10.6  TDV联合矩阵模型	199
第11章  TensorFlow内置案例分析	201
11.1  预备知识	202
11.2  Mnist手写数字识别	211
案例11-1：Mnist初级案例	211
案例11-2：Mnist专业版本	215
11.3  FFNNs前馈神经网络模型	224
案例11-3：FFNNs前馈神经网络模型	224
第12章  TensorLayer案例分析	234
12.1  手写识别算法	234
案例12-1：Mnist手写识别	234
12.2  Mnist神经网络模型合集	242
案例12-2：Dropout网络模型	243
案例12-3：DropConnect网络模型	253
案例12-4：dAE降噪自编码算法1	257
案例12-5：dAE降噪自编码算法2	260
案例12-6：CNN卷积神经网络算法	263
第13章  TFLearn案例分析	267
13.1  生存游戏	268
13.1.1  泰坦尼克数据集	268
13.1.2  Kaggle机器学习公开赛	269
案例13-1：泰坦尼克号生存与死亡	270
13.2  线性回归	274
案例13-2：线性回归模型	275
13.3  模型管理	278
案例13-3：保存读取模型数据	278
13.4  超智能体	283
案例13-4：超智能体：NOT取反运算	284
案例13-5：超智能体：OR或运算	286
案例13-6：超智能体：AND（与）运算	288
案例13-7：超智能体：XOR（异或）运算	289
13.5  CNN卷积神经网络算法	292
案例13-8：CNN卷积神经网络算法	292
第14章  Keras案例分析	297
14.1  Keras模型	298
14.2  Keras使用流程	299
14.3  Keras常用模块	300
14.4  Keras模型可视化	301
案例14-1：MLP多层神经网络	303
案例14-2：CNN卷积神经网络	309
案例14-3：IRNN修正循环神经网络	316
案例14-4：HRNN分层循环神经网络	322
第15章  TensorFlow常用运行模式	327
15.1  深度学习三大要素	327
15.2  神经网络基本结构	328
15.3  基本神经元层	329
15.4  神经网络通用流程	335
15.5  Loss损失函数	338
15.6  TensorFlow常用优化算法	340
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>零起点TensorFlow快速入门
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow+Keras深度学习人工智能实践应用
第1章 人工智能、机器学习与深度学习简介 1
1.1 人工智能、机器学习、深度学习的关系 2
1.2 机器学习介绍 4
1.3 机器学习分类 4
1.4 深度学习简介 7
1.5 结论 8
第2章 深度学习的原理 9
2.1 神经传导的原理 10
2.2 以矩阵运算仿真神经网络 13
2.3 多层感知器模型 14
2.4 使用反向传播算法进行训练 16
2.5 结论 21
第3章 TensorFlow与Keras介绍 22
3.1 TensorFlow架构图 23
3.2 TensorFlow简介 24
3.3 TensorFlow程序设计模式 26
3.4 Keras介绍 27
3.5 Keras程序设计模式 28
3.6 Keras与TensorFlow比较 29
3.7 结论 30
第4章 在Windows中安装TensorFlow与Keras 31
4.1 安装Anaconda 32
4.2 启动命令提示符 35
4.3 建立TensorFlow的Anaconda虚拟环境 37
4.4 在Anaconda虚拟环境安装TensorFlow与Keras 40
4.5 启动Jupyter Notebook 42
4.6 结论 48
第5章 在Linux Ubuntu中安装TensorFlow与Keras 49
5.1 安装Anaconda 50
5.2 安装TensorFlow与Keras 52
5.3 启动Jupyter Notebook 53
5.4 结论 54
第6章 Keras MNIST手写数字识别数据集 55
6.1 下载MNIST数据 56
6.2 查看训练数据 58
6.3 查看多项训练数据images与label 60
6.4 多层感知器模型数据预处理 62
6.5 features数据预处理 62
6.6 label数据预处理 64
6.7 结论 65
第7章 Keras多层感知器识别手写数字 66
7.1 Keras多元感知器识别MNIST手写数字图像的介绍 67
7.2 进行数据预处理 69
7.3 建立模型 69
7.4 进行训练 73
7.5 以测试数据评估模型准确率 77
7.6 进行预测 78
7.7 显示混淆矩阵 79
7.8 隐藏层增加为1000个神经元 81
7.9 多层感知器加入DropOut功能以避免过度拟合 84
7.10 建立多层感知器模型包含两个隐藏层 86
7.11 结论 89
第8章 Keras卷积神经网络识别手写数字 90
8.1 卷积神经网络简介 91
8.2 进行数据预处理 97
8.3 建立模型 98
8.4 进行训练 101
8.5 评估模型准确率 104
8.6 进行预测 104
8.7 显示混淆矩阵 105
8.8 结论 107
第9章 Keras CIFAR-10图像识别数据集 108
9.1 下载CIFAR-10数据 109
9.2 查看训练数据 111
9.3 查看多项images与label 112
9.4 将images进行预处理 113
9.5 对label进行数据预处理 114
9.6 结论 115
第10章 Keras卷积神经网络识别CIFAR-10图像 116
10.1 卷积神经网络简介 117
10.2 数据预处理 118
10.3 建立模型 119
10.4 进行训练 123
10.5 评估模型准确率 126
10.6 进行预测 126
10.7 查看预测概率 127
10.8 显示混淆矩阵 129
10.9 建立3次的卷积运算神经网络 132
10.10 模型的保存与加载 135
10.11 结论 136
第11章 Keras泰坦尼克号上的旅客数据集 137
11.1 下载泰坦尼克号旅客数据集 138
11.2 使用Pandas DataFrame读取数据并进行预处理 140
11.3 使用Pandas DataFrame进行数据预处理 142
11.4 将DataFrame转换为Array 143
11.5 将ndarray特征字段进行标准化 145
11.6 将数据分为训练数据与测试数据 145
11.7 结论 147
第12章 Keras多层感知器预测泰坦尼克号上旅客的生存概率 148
12.1 数据预处理 149
12.2 建立模型 150
12.3 开始训练 152
12.4 评估模型准确率 155
12.5 加入《泰坦尼克号》电影中Jack与Rose的数据 156
12.6 进行预测 157
12.7 找出泰坦尼克号背后的感人故事 158
12.8 结论 160
第13章 IMDb网络电影数据集与自然语言处理 161
13.1 Keras自然语言处理介绍 163
13.2 下载IMDb数据集 167
13.3 读取IMDb数据 169
13.4 查看IMDb数据 172
13.5 建立token 173
13.6 使用token将“影评文字”转换成“数字列表” 174
13.7 让转换后的数字长度相同 174
13.8 结论 176
第14章 Keras建立MLP、RNN、LSTM模型进行IMDb情感分析 177
14.1 建立多层感知器模型进行IMDb情感分析 178
14.2 数据预处理 179
14.3 加入嵌入层 180
14.4 建立多层感知器模型 181
14.5 训练模型 182
14.6 评估模型准确率 184
14.7 进行预测 185
14.8 查看测试数据预测结果 185
14.9 查看《美女与野兽》的影评 187
14.10 预测《美女与野兽》的影评是正面或负面的 190
14.11 文字处理时使用较大的字典提取更多文字 192
14.12 RNN模型介绍 193
14.13 使用Keras RNN模型进行IMDb情感分析 195
14.14 LSTM模型介绍 197
14.15 使用Keras LSTM模型进行IMDb情感分析 199
14.16 结论 200
第15章 TensorFlow程序设计模式 201
15.1 建立“计算图” 202
15.2 执行“计算图” 204
15.3 TensorFlow placeholder 206
15.4 TensorFlow数值运算方法介绍 207
15.5 TensorBoard 208
15.6 建立一维与二维张量 211
15.7 矩阵基本运算 212
15.8 结论 214
第16章 以TensorFlow张量运算仿真神经网络的运行 215
16.1 以矩阵运算仿真神经网络 216
16.2 以placeholder传入X值 220
16.3 创建layer函数以矩阵运算仿真神经网络 222
16.4 建立layer_debug函数显示权重与偏差 225
16.5 结论 226
第17章 TensorFlow MNIST手写数字识别数据集 227
17.1 下载MNIST数据 228
17.2 查看训练数据 229
17.3 查看多项训练数据images与labels 232
17.4 批次读取MNIST数据 234
17.5 结论 235
第18章 TensorFlow多层感知器识别手写数字 236
18.1 TensorFlow建立多层感知器辨识手写数字的介绍 237
18.2 数据准备 239
18.3 建立模型 239
18.4 定义训练方式 242
18.5 定义评估模型准确率的方式 243
18.6 进行训练 244
18.7 评估模型准确率 249
18.8 进行预测 249
18.9 隐藏层加入更多神经元 250
18.10 建立包含两个隐藏层的多层感知器模型 251
18.11 结论 252
第19章 TensorFlow卷积神经网络识别手写数字 253
19.1 卷积神经网络简介 254
19.2 进行数据预处理 255
19.3 建立共享函数 256
19.4 建立模型 258
19.5 定义训练方式 264
19.6 定义评估模型准确率的方式 264
19.7 进行训练 265
19.8 评估模型准确率 266
19.9 进行预测 267
19.10 TensorBoard 268
19.11 结论 270
第20章 TensorFlow GPU版本的安装 271
20.1 确认显卡是否支持CUDA 273
20.2 安装CUDA 274
20.3 安装cuDNN 278
20.4 将cudnn64_5.dll存放的位置加入Path环境变量 281
20.5 在Anaconda建立TensorFlow GPU虚拟环境 283
20.6 安装TensorFlow GPU版本 285
20.7 安装Keras 286
20.8 结论 286
第21章 使用GPU加快TensorFlow与Keras训练 287
21.1 启动TensorFlow GPU环境 288
21.2 测试GPU与CPU执行性能 293
21.3 超出显卡内存的限制 296
21.4 以多层感知器的实际范例比较CPU与GPU的执行速度 297
21.5 以CNN的实际范例比较CPU与GPU的执行速度 299
21.6 以Keras Cifar CNN的实际范例比较CPU与GPU的执行速度 302
21.7 结论 304
附录A 本书范例程序的下载与安装说明 305
A.1 在Windows系统中下载与安装范例程序 306
A.2 在Ubuntu Linux系统中下载与安装范例程序 310
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow+Keras深度学习人工智能实践应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow深度学习
第一部分探索深度学习之方式的开始
第1章开篇
1.1人工智能的发展
1.1.1萌芽
1.1.2复苏
1.1.3现代实践：大数据+深度神经网络模型
1.2大数据
1.3机器学习与深度学习
1.3.1机器学习
1.3.2深度学习
1.3.3同人工智能的关系
1.4人工神经网络与TensorFlow
1.4.1人工神经网络
1.4.2TensorFlow
1.5其他主流深度学习框架介绍
1.5.1Caffe
1.5.2Torch
1.5.3Theano
1.5.4MXNet
1.5.5Keras
1.6机器学习的常见任务
1.6.1分类
1.6.2回归
1.6.3去噪
1.6.4转录
1.6.5机器翻译
1.6.6异常检测
1.6.7结构化输出
1.7深度学习的现代应用
1.7.1计算机视觉
1.7.2自然语言处理
1.7.3语音识别
第2章安装TensorFlow
2.1安装前的须知
2.1.1检查硬件是否达标
2.1.2推荐选用GPU进行训练
2.1.3为什么选择Linux系统
2.1.4为什么选择Python语言
2.2安装Anaconda
2.3TensorFlow的两个主要依赖包
2.3.1Protocol Buffer
2.3.2Bazel
2.4安装CUDA和cuDNN
2.4.1CUDA
2.4.2cuDNN
2.5正式安装TensorFlow
2.5.1使用pip安装
2.5.2从源代码编译并安装
2.6测试你的TensorFlow
2.6.1运行向量相加的例子
2.6.2加载过程存在的一些问题
2.7推荐使用IDE
第3章TensorFlow编程策略
3.1初识计算图与张量
3.2计算图——TensorFlow的计算模型
3.3张量——TensorFlow的数据模型
3.3.1概念
3.3.2使用张量
3.4会话——TensorFlow的运行模型
3.4.1TensorFlow系统结构概述
3.4.2简单使用会话
3.4.3使用with/as环境上下文管理器
3.4.4Session的参数配置
3.4.5placeholder机制
3.5TensorFlow变量
3.5.1创建变量
3.5.2变量与张量
3.6管理变量的变量空间
3.6.1get_variable()函数
3.6.2variable_scope()与name_scope()
第二部分TensorFlow实现深度网络
第4章深度前馈神经网络
4.1网络的前馈方式
4.2全连接
4.2.1神经元与全连接结构
4.2.2前向传播算法
4.3线性模型的局限性
4.4激活函数
4.4.1常用激活函数
4.4.2激活函数实现去线性化
4.5多层网络解决异或运算
4.6损失函数
4.6.1经典损失函数
4.6.2自定义损失函数
第5章优化网络的方法
5.1基于梯度的优化
5.1.1梯度下降算法
5.1.2随机梯度下降
5.2反向传播
5.2.1简要解释反向传播算法
5.2.2自适应学习率算法
5.2.3TensorFlow提供的优化器
5.3学习率的独立设置
5.3.1指数衰减的学习率
5.3.2其他优化学习率的方法
5.4拟合
5.4.1过拟合和欠拟合
5.4.2正则化的方法
5.4.3Bagging方法
5.4.4Dropout方法
第6章全连神经网络的经典实践
6.1MNIST数据集
6.2网络的设计
6.3超参数和验证集
6.4与简单模型的对比
第7章卷积神经网络
7.1准备性的认识
7.1.1图像识别与经典数据集
7.1.2卷积网络的神经科学基础
7.1.3卷积神经网络的历史
7.2卷积
7.2.1卷积运算
7.2.2卷积运算的稀疏连接
7.2.3卷积运算的参数共享
7.2.4卷积运算的平移等变
7.2.5多卷积核
7.2.6卷积层的代码实现
7.3池化
7.3.1池化过程
7.3.2常用池化函数
7.3.3池化层的代码实现
7.4实现卷积神经网络的简例
7.4.1卷积神经网络的一般框架
7.4.2用简单卷积神经网络实现Cifar-10数据集分类
7.5图像数据处理
7.5.1图像编解码处理
7.5.2翻转图像
7.5.3图像色彩调整
7.5.4图像标准化处理
7.5.5调整图像大小
7.5.6图像的标注框
第8章经典卷积神经网络
8.1LeNet-5卷积网络模型
8.1.1模型结构
8.1.2TensorFlow实现
8.2AlexNet卷积网络模型
8.2.1模型结构
8.2.2TensorFlow实现
8.3VGGNet卷积网络模型
8.3.1模型结构
8.3.2TensorFlow实现
8.4InceptionNet-V3卷积网络模型
8.4.1模型结构
8.4.2Inception V3 Module的实现
8.4.3使用Inception V3完成模型迁移
8.5ResNet卷积网络模型
8.5.1模型结构
8.5.2TensorFlow实现
第9章循环神经网络
9.1循环神经网络简介
9.1.1循环神经网络的前向传播程序设计
9.1.2计算循环神经网络的梯度
9.1.3循环神经网络的不同设计模式
9.2自然语言建模与词向量
9.2.1统计学语言模型
9.2.2Word2Vec
9.2.3用TensorFlow实现Word2Vec
9.3LSTM实现自然语言建模
9.3.1长短时记忆网络（LSTM）
9.3.2LSTM在自然语言建模中的应用
9.3.3循环神经网络的Dropout
9.4循环神经网络的变种
9.4.1双向循环神经网络
9.4.2深层循环神经网络
第10章深度强化学习
10.1理解基本概念
10.2深度强化学习的思路
10.3典型应用场景举例
10.3.1场景1：机械臂自控
10.3.2场景2：自动游戏系统
10.3.3场景3：自动驾驶
10.3.4场景4：智能围棋系统
10.4Q学习与深度Q网络
10.4.1Q学习与深度Q学习
10.4.2深度Q网络
第三部分TensorFlow的使用进阶
第11章数据读取
11.1文件格式
11.1.1TFRecord格式
11.1.2CSV格式
11.2队列
11.2.1数据队列
11.2.2文件队列
11.3使用多线程处理输入的数据
11.3.1使用Coordinator类管理线程
11.3.2使用QueueRunner创建线程
11.4组织数据batch
第12章模型持久化
12.1通过代码实现
12.2模型持久化的原理
12.2.1model.ckpt.mate文件
12.2.2从.index与.data文件读取变量的值
12.3持久化的MNIST手写字识别
12.4PB文件
第13章TensorBoard可视化
13.1TensorBoard简要介绍
13.2MNIST手写字识别的可视化
13.2.1实现的过程
13.2.2标量数据可视化结果
13.2.3图像数据可视化结果
13.2.4计算图可视化结果
13.3其他监控指标可视化
第14章加速计算
14.1TensorFlow支持的设备
14.2TensorFlow单机实现
14.2.1查看执行运算的设备
14.2.2device()函数的使用
14.3并行训练的原理
14.3.1数据并行
14.3.2模型并行
14.4单机多GPU加速TensorFlow程序
14.4.1实现的过程
14.4.2多GPU并行的可视化
14.5分布式TensorFlow概述
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow深度学习
第1章　深度学习入门　　1
1.1　机器学习简介　　1
1.1.1　监督学习　　2
1.1.2　无监督学习　　2
1.1.3　强化学习　　3
1.2　深度学习定义　　3
1.2.1　人脑的工作机制　　3
1.2.2　深度学习历史　　4
1.2.3　应用领域　　5
1.3　神经网络　　5
1.3.1　生物神经元　　5
1.3.2　人工神经元　　6
1.4　人工神经网络的学习方式　　8
1.4.1　反向传播算法　　8
1.4.2　权重优化　　8
1.4.3　随机梯度下降法　　9
1.5　神经网络架构　　10
1.5.1　多层感知器　　10
1.5.2　DNN架构　　11
1.5.3　卷积神经网络　　12
1.5.4　受限玻尔兹曼机　　12
1.6　自编码器　　13
1.7　循环神经网络　　14
1.8　几种深度学习框架对比　　14
1.9　小结　　16
第2章　TensorFlow初探　　17
2.1　总览　　17
2.1.1　TensorFlow 1.x版本特性　　18
2.1.2　使用上的改进　　18
2.1.3　TensorFlow安装与入门　　19
2.2　在Linux上安装TensorFlow　　19
2.3　为TensorFlow启用NVIDIA GPU　　20
2.3.1　第1步：安装NVIDIA CUDA　　20
2.3.2　　第2步：安装NVIDIA cuDNN v5.1+　　21
2.3.3　　第3步：确定GPU卡的CUDA计算能力为3.0+　　22
2.3.4　第4步：安装libcupti-dev库　　22
2.3.5　　第5步：安装Python
（或Python 3）　　22
2.3.6　第6步：安装并升级PIP
（或PIP3）　　22
2.3.7　第7步：安装TensorFlow　　23
2.4　如何安装TensorFlow　　23
2.4.1　直接使用pip安装　　23
2.4.2　使用virtualenv安装　　24
2.4.3　从源代码安装　　26
2.5　在Windows上安装TensorFlow　　27
2.5.1　在虚拟机上安装TensorFlow　　27
2.5.2　直接安装到Windows　　27
2.6　测试安装是否成功　　28
2.7　计算图　　28
2.8　为何采用计算图　　29
2.9　编程模型　　30
2.10　数据模型　　33
2.10.1　阶　　33
2.10.2　形状　　33
2.10.3　数据类型　　34
2.10.4　变量　　36
2.10.5　取回　　37
2.10.6　注入　　38
2.11　TensorBoard　　38
2.12　实现一个单输入神经元　　39
2.13　单输入神经元源代码　　43
2.14　迁移到TensorFlow 1.x版本　　43
2.14.1　如何用脚本升级　　44
2.14.2　局限　　47
2.14.3　手动升级代码　　47
2.14.4　变量　　47
2.14.5　汇总函数　　47
2.14.6　简化的数学操作　　48
2.14.7　其他事项　　49
2.15　小结　　49
第3章　用TensorFlow构建前馈
神经网络　　51
3.1　前馈神经网络介绍　　51
3.1.1　前馈和反向传播　　52
3.1.2　权重和偏差　　53
3.1.3　传递函数　　53
3.2　手写数字分类　　54
3.3　探究MNIST数据集　　55
3.4　softmax分类器　　57
3.5　TensorFlow模型的保存和还原　　63
3.5.1　保存模型　　63
3.5.2　还原模型　　63
3.5.3　softmax源代码　　65
3.5.4　softmax启动器源代码　　66
3.6　实现一个五层神经网络　　67
3.6.1　可视化　　69
3.6.2　五层神经网络源代码　　70
3.7　ReLU分类器　　72
3.8　可视化　　73
3.9　dropout优化　　76
3.10　可视化　　78
3.11　小结　　80
第4章　TensorFlow与卷积神经网络　　82
4.1　CNN简介　　82
4.2　CNN架构　　84
4.3　构建你的第一个CNN　　86
4.4　CNN表情识别　　95
4.4.1　表情分类器源代码　　104
4.4.2　使用自己的图像测试模型　　107
4.4.3　源代码　　109
4.5　小结　　111
第5章　优化TensorFlow自编码器　　112
5.1　自编码器简介　　112
5.2　实现一个自编码器　　113
5.3　增强自编码器的鲁棒性　　119
5.4　构建去噪自编码器　　120
5.5　卷积自编码器　　127
5.5.1　编码器　　127
5.5.2　解码器　　128
5.5.3　卷积自编码器源代码　　134
5.6　小结　　138
第6章　循环神经网络　　139
6.1　RNN的基本概念　　139
6.2　RNN的工作机制　　140
6.3　RNN的展开　　140
6.4　梯度消失问题　　141
6.5　LSTM网络　　142
6.6　RNN图像分类器　　143
6.7　双向RNN　　149
6.8　文本预测　　155
6.8.1　数据集　　156
6.8.2　困惑度　　156
6.8.3　PTB模型　　156
6.8.4　运行例程　　157
6.9　小结　　158
第7章　GPU计算　　160
7.1　GPGPU计算　　160
7.2　GPGPU的历史　　161
7.3　CUDA架构　　161
7.4　GPU编程模型　　162
7.5　TensorFlow中GPU的设置　　163
7.6　TensorFlow的GPU管理　　165
7.7　GPU内存管理　　168
7.8　在多GPU系统上分配单个GPU　　168
7.9　使用多个GPU　　170
7.10　小结　　171
第8章　TensorFlow高级编程　　172
8.1　Keras简介　　172
8.2　构建深度学习模型　　174
8.3　影评的情感分类　　175
8.4　添加一个卷积层　　179
8.5　Pretty Tensor　　181
8.6　数字分类器　　182
8.7　TFLearn　　187
8.8　泰坦尼克号幸存者预测器　　188
8.9　小结　　191
第9章　TensorFlow高级多媒体编程　　193
9.1　多媒体分析简介　　193
9.2　基于深度学习的大型对象检测　　193
9.2.1　瓶颈层　　195
9.2.2　使用重训练的模型　　195
9.3　加速线性代数　　197
9.3.1　TensorFlow的核心优势　　197
9.3.2　加速线性代数的准时编译　　197
9.4　TensorFlow和Keras　　202
9.4.1　Keras简介　　202
9.4.2　拥有Keras的好处　　203
9.4.3　视频问答系统　　203
9.5　Android上的深度学习　　209
9.5.1　TensorFlow演示程序　　209
9.5.2　Android入门　　211
9.6　小结　　214
第10章　强化学习　　215
10.1　强化学习基本概念　　216
10.2　Q-learning算法　　217
10.3　OpenAI Gym框架简介　　218
10.4　FrozenLake-v0实现问题　　220
10.5　使用TensorFlow实现Q-learning　　223
10.6　小结　　227
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>零起点TensorFlow与量化交易
第1章  TensorFlow概述  1
1.1  TensorFlow要点概括  2
1.2  TensorFlow简化接口  2
1.3  Keras简介  3
1.4  运行环境模块的安装  4
1.4.1  CUDA运行环境的安装  4
案例1-1：重点模块版本测试  5
案例1-2：GPU开发环境测试  8
1.4.2  GPU平台运行结果  9
第2章  无数据不量化（上）  12
2.1  金融数据源  13
2.1.1  TopDat金融数据集  14
2.1.2  量化分析与试错成本  15
2.2  OHLC金融数据格式  16
案例2-1：金融数据格式  17
2.3  K线图  18
案例2-2：绘制金融数据K线图  19
2.4  Tick数据格式  22
案例2-3：Tick数据格式  23
2.4.1  Tick数据与分时数据转换  25
案例2-4：分时数据  25
2.4.2  resample函数  26
2.4.3  分时数据  26
2.5  离线金融数据集  29
案例2-5：TopDat金融数据集的日线数据  29
案例2-6：TopDat金融数据集的Tick数据  31
2.6  TopDown金融数据下载  33
案例2-7：更新单一A股日线数据  34
案例2-8：批量更新A股日线数据  37
2.6.1  Tick数据与分时数据  40
案例2-9：更新单一A股分时数据  40
案例2-10：批量更新分时数据  43
2.6.2  Tick数据与实时数据  45
案例2-11：更新单一实时数据  45
案例2-12：更新全部实时数据  48
第3章  无数据不量化（下）  51
3.1  均值优先  51
案例3-1：均值计算与价格曲线图  52
3.2  多因子策略和泛因子策略  54
3.2.1  多因子策略  54
3.2.2  泛因子策略  55
案例3-2：均线因子  55
3.3 “25日神定律”  59
案例3-3：时间因子  61
案例3-4：分时时间因子  63
3.4  TA-Lib金融指标  66
3.5  TQ智能量化回溯系统  70
3.6  全内存计算  70
案例3-5：增强版指数索引  71
案例3-6：AI版索引数据库  73
3.7  股票池  77
案例3-7：股票池的使用  77
3.8  TQ_bar全局变量类  81
案例3-8：TQ_bar初始化  82
案例3-9：TQ版本日线数据  85
3.9  大盘指数  87
案例3-10：指数日线数据  88
案例3-11：TQ版本指数K线图  89
案例3-12：个股和指数曲线对照图  92
3.10  TDS金融数据集  96
案例3-13：TDS衍生数据  98
案例3-14：TDS金融数据集的制作  102
案例3-15：TDS金融数据集2.0  105
案例3-16：读取TDS金融数据集  108
第4章  人工智能与趋势预测  112
4.1  TFLearn简化接口  112
4.2  人工智能与统计关联度分析  113
4.3  关联分析函数corr  113
4.3.1  Pearson相关系数  114
4.3.2  Spearman相关系数  114
4.3.3  Kendall相关系数  115
4.4  open（开盘价）关联性分析  115
案例4-1：open关联性分析  115
4.5  数值预测与趋势预测  118
4.5.1  数值预测  119
4.5.2  趋势预测  120
案例4-2：ROC计算  120
案例4-3：ROC与交易数据分类  123
4.6  n+1大盘指数预测  128
4.6.1  线性回归模型  128
案例4-4：上证指数n+1的开盘价预测  129
案例4-5：预测数据评估  133
4.6.2  效果评估函数  136
4.6.3  常用的评测指标  138
4.7  n+1大盘指数趋势预测  139
案例4-6：涨跌趋势归一化分类  140
案例4-7：经典版涨跌趋势归一化分类  143
4.8  One-Hot  145
案例4-8：One-Hot格式  146
4.9  DNN模型  149
案例4-9：DNN趋势预测  150
第5章  单层神经网络预测股价  156
5.1  Keras简化接口  156
5.2  单层神经网络  158
案例5-1：单层神经网络模型  158
5.3  神经网络常用模块  168
案例5-2：可视化神经网络模型  170
案例5-3：模型读写  174
案例5-4：参数调优入门  177
第6章  MLP与股价预测  182
6.1  MLP  182
案例6-1：MLP价格预测模型  183
6.2  神经网络模型应用四大环节  189
案例6-2：MLP模型评估  190
案例6-3：优化MLP价格预测模型  194
案例6-4：优化版MLP模型评估  197
第7章  RNN与趋势预测  200
7.1  RNN  200
7.2  IRNN与趋势预测  201
案例7-1：RNN趋势预测模型  201
案例7-2：RNN模型评估  209
案例7-3：RNN趋势预测模型2  211
案例7-4：RNN模型2评估  214
第8章  LSTM与量化分析  217
8.1  LSTM模型  217
8.1.1  数值预测  218
案例8-1：LSTM价格预测模型  219
案例8-2：LSTM价格预测模型评估  226
8.1.2  趋势预测  230
案例8-3：LSTM股价趋势预测模型  231
案例8-4：LSTM趋势模型评估  239
8.2  LSTM量化回溯分析  242
8.2.1  构建模型  243
案例8-5：构建模型  243
8.2.2  数据整理  251
案例8-6：数据整理  251
8.2.3  回溯分析  262
案例8-7：回溯分析  262
8.2.4  专业回报分析  268
案例8-8：量化交易回报分析  268
8.3  完整的LSTM量化分析程序  279
案例8-9：LSTM量化分析程序  280
8.3.1  数据整理  280
8.3.2  量化回溯  284
8.3.3  回报分析  285
8.3.4  专业回报分析  288
第9章  日线数据回溯分析  293
9.1  数据整理  293
案例9-1：数据更新  294
案例9-2：数据整理  296
9.2  回溯分析  307
9.2.1  回溯主函数  307
9.2.2  交易信号  308
9.3  交易接口函数  309
案例9-3：回溯分析  309
案例9-4：多模式回溯分析  316
第10章  Tick数据回溯分析  318
10.1  ffn金融模块库  318
案例10-1：ffn功能演示  318
案例10-2：量化交易回报分析  330
案例10-3：完整的量化分析程序  343
10.2  Tick分时数据量化分析  357
案例10-4：Tick分时量化分析程序  357
总结  371
附录A  TensorFlow 1.1函数接口变化  372
附录B  神经网络常用算法模型  377
附录C  机器学习常用算法模型  414
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>零起点TensorFlow与量化交易
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习算法实践（基于Theano和TensorFlow）
第一部分  深度学习算法概述
第1章  深度学习算法简介	2
1.1  神经网络发展简史	2
1.1.1  神经网络第一次兴起	3
1.1.2  神经网络沉寂期（20世纪80年代—21世纪）	4
1.1.3  神经网络技术积累期（20世纪90年代—2006年）	5
1.1.4  深度学习算法崛起（2006年至今）	8
1.2  深度学习现状	10
1.2.1  传统神经网络困境	10
1.2.2  深度多层感知器	12
1.2.3  深度卷积神经网络	14
1.2.4  深度递归神经网络	15
1.3  深度学习研究前瞻	16
1.3.1  自动编码机	17
1.3.2  深度信念网络	18
1.3.3  生成式网络最新进展	19
1.4  深度学习框架比较	20
1.4.1  TensorFlow	20
1.4.2  Theano	21
1.4.3  Torch	22
1.4.4  DeepLearning4J	23
1.4.5  Caffe	23
1.4.6  MXNet	24
1.4.7  CNTK	27
1.4.8  深度学习框架造型指导原则	27
1.5  深度学习入门路径	28
1.5.1  运行MNIST	28
1.5.2  深度学习框架的选择	29
1.5.3  小型试验网络	33
1.5.4  训练生产网络	33
1.5.5  搭建生产环境	34
1.5.6  持续改进	35
第二部分  深度学习算法基础
第2章  搭建深度学习开发环境	38
2.1  安装Python开发环境	38
2.1.1  安装最新版本Python	38
2.1.2  Python虚拟环境配置	39
2.1.3  安装科学计算库	40
2.1.4  安装最新版本Theano	40
2.1.5  图形绘制	40
2.2  NumPy简易教程	43
2.2.1  Python基础	43
2.2.2  多维数组的使用	51
2.2.3  向量运算	58
2.2.4  矩阵运算	60
2.2.5  线性代数	62
2.3  TensorFlow简易教程	68
2.3.1  张量定义	69
2.3.2  变量和placeholder	69
2.3.3  神经元激活函数	71
2.3.4  线性代数运算	72
2.3.5  操作数据集	74
2.4  Theano简易教程	77
2.4.1  安装Theano	77
2.4.2  Theano入门	78
2.4.3  Theano矩阵相加	79
2.4.4  变量和共享变量	80
2.4.5  随机数的使用	84
2.4.6  Theano求导	84
2.5  线性回归	86
2.5.1  问题描述	86
2.5.2  线性模型	88
2.5.3  线性回归学习算法	89
2.5.4  解析法	90
2.5.5  Theano实现	93
第3章  逻辑回归	100
3.1  逻辑回归数学基础	100
3.1.1  逻辑回归算法的直观解释	100
3.1.2  逻辑回归算法数学推导	101
3.1.3  牛顿法解逻辑回归问题	103
3.1.4  通用学习模型	106
3.2  逻辑回归算法简单应用	113
3.3  MNIST手写数字识别库简介	124
3.4  逻辑回归MNIST手写数字识别	126
第4章  感知器模型和MLP	139
4.1  感知器模型	139
4.1.1  神经元模型	139
4.1.2  神经网络架构	143
4.2  数值计算形式	144
4.2.1  前向传播	144
4.2.2  误差反向传播	145
4.2.3  算法推导	147
4.3  向量化表示形式	152
4.4  应用要点	153
4.4.1  输入信号模型	154
4.4.2  权值初始化	155
4.4.3  早期停止	155
4.4.4  输入信号调整	156
4.5  TensorFlow实现MLP	156
第5章  卷积神经网络	174
5.1  卷积神经网络原理	174
5.1.1  卷积神经网络的直观理解	174
5.1.2  卷积神经网络构成	177
5.1.3  卷积神经网络设计	191
5.1.4  迁移学习和网络微调	193
5.2  卷积神经网络的TensorFlow实现	195
5.2.1  模型搭建	197
5.2.2  训练方法	203
5.2.3  运行方法	208
第6章  递归神经网络	212
6.1  递归神经网络原理	212
6.1.1  递归神经网络表示方法	213
6.1.2  数学原理	214
6.1.3  简单递归神经网络应用示例	219
6.2  图像标记	226
6.2.1  建立开发环境	226
6.2.2  图像标记数据集处理	227
6.2.3  单步前向传播	229
6.2.4  单步反向传播	231
6.2.5  完整前向传播	234
6.2.6  完整反向传播	236
6.2.7  单词嵌入前向传播	239
6.2.8  单词嵌入反向传播	241
6.2.9  输出层前向/反向传播	243
6.2.10  输出层代价函数计算	245
6.2.11  图像标注网络整体架构	248
6.2.12  代价函数计算	249
6.2.13  生成图像标记	255
6.2.14  网络训练过程	258
6.2.15  网络持久化	265
第7章  长短时记忆网络	269
7.1  长短时记忆网络原理	269
7.1.1  网络架构	269
7.1.2  数学公式	272
7.2  MNIST手写数字识别	274
第三部分  深度学习算法进阶
第8章  自动编码机	286
8.1  自动编码机概述	286
8.1.1  自动编码机原理	287
8.1.2  去噪自动编码机	287
8.1.3  稀疏自动编码机	288
8.2  去噪自动编码机TensorFlow实现	291
8.3  去噪自动编码机的Theano实现	298
第9章  堆叠自动编码机	307
9.1  堆叠去噪自动编码机	308
9.2  TensorFlow实现	322
9.3  Theano实现	341
第10章  受限玻尔兹曼机	344
10.1  受限玻尔兹曼机原理	344
10.1.1  网络架构	344
10.1.2  能量模型	346
10.1.3  CD-K算法	351
10.2  受限玻尔兹曼机TensorFlow实现	353
10.3  受限玻尔兹曼机Theano实现	362
第11章  深度信念网络	381
11.1  深度信念网络原理	381
11.2  深度信念网络TensorFlow实现	382
11.3  深度信念网络Theano实现	403
第四部分  机器学习基础
第12章  生成式学习	420
12.1  高斯判别分析	422
12.1.1  多变量高斯分布	422
12.1.2  高斯判决分析公式	423
12.2  朴素贝叶斯	436
12.2.1  朴素贝叶斯分类器	436
12.2.2  拉普拉斯平滑	439
12.2.3  多项式事件模型	441
第13章  支撑向量机	444
13.1  支撑向量机概述	444
13.1.1  函数间隔和几何间隔	445
13.1.2  最优距离分类器	448
13.2  拉格朗日对偶	448
13.3  最优分类器算法	450
13.4  核方法	453
13.5  非线性可分问题	455
13.6  SMO算法	457
13.6.1  坐标上升算法	458
13.6.2  SMO算法详解	458
第五部分  深度学习平台API
第14章  Python Web编程	462
14.1  Python Web开发环境搭建	462
14.1.1  CherryPy框架	463
14.1.2  CherryPy安装	463
14.1.3  测试CherryPy安装是否成功	464
14.2  最简Web服务器	465
14.2.1  程序启动	465
14.2.2  显示HTML文件	466
14.2.3  静态内容处理	468
14.3  用户认证系统	471
14.4  AJAX请求详解	473
14.4.1  添加数据	474
14.4.2  修改数据	476
14.4.3  删除数据	478
14.4.4  REST服务实现	479
14.5  数据持久化技术	487
14.5.1  环境搭建	487
14.5.2  数据库添加操作	488
14.5.3  数据库修改操作	489
14.5.4  数据库删除操作	490
14.5.5  数据库查询操作	491
14.5.6  数据库事务操作	492
14.5.7  数据库连接池	494
14.6  任务队列	499
14.7  媒体文件上传	502
14.8  Redis操作	504
14.8.1  Redis安装配置	504
14.8.2  Redis使用例程	505
第15章  深度学习云平台	506
15.1  神经网络持久化	506
15.1.1  数据库表设计	506
15.1.2  整体目录结构	511
15.1.3  训练过程及模型文件保存	512
15.2  神经网络运行模式	528
15.3  AJAX请求调用神经网络	531
15.3.1  显示静态网页	531
15.3.2  上传图片文件	540
15.3.3  AJAX接口	543
15.4  请求合法性验证	545
15.4.1  用户注册和登录	546
15.4.2  客户端生成请求	553
15.4.3  服务器端验证请求	555
15.5  异步结果处理	557
15.5.1  网页异步提交	557
15.5.2  应用队列管理模块	559
15.5.3  任务队列	560
15.5.4  结果队列	561
15.5.5  异步请求处理流程	562
15.6  神经网络持续改进	563
15.6.1  应用遗传算法	563
15.6.2  重新训练	564
15.6.3  生成式对抗网络	565
后  记	567
参考文献	568
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习算法实践（基于Theano和TensorFlow）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>图解深度学习与神经网络：从张量到TensorFlow实现
1 深度学习及TensorFlow 简介1
1.1 深度学习  1
1.2 TensorFlow 简介及安装  2
2 基本的数据结构及运算6
2.1 张量   6
2.1.1 张量的定义   6
2.1.2 Tensor 与Numpy 的ndarray 转换  9
2.1.3 张量的尺寸   10
2.1.4 图像转换为张量  13
2.2 随机数   14
2.2.1 均匀（平均）分布随机数   14
2.2.2 正态（高斯）分布随机数   15
2.3 单个张量的运算  17
2.3.1 改变张量的数据类型   17
2.3.2 访问张量中某一个区域的值   19
2.3.3 转置   22
2.3.4 改变形状  26
2.3.5 归约运算：求和、平均值、最大（小）值   29
2.3.6 最大（小）值的位置索引   34
2.4 多个张量之间的运算   35
2.4.1 基本运算：加、减、乘、除   35
2.4.2 乘法   41
2.4.3 张量的连接   42
2.4.4 张量的堆叠   44
2.4.5 张量的对比   48
2.5 占位符   49
2.6 Variable 对象  50
3 梯度及梯度下降法52
3.1 梯度   52
3.2 导数计算的链式法则   53
3.2.1 多个函数和的导数   54
3.2.2 复合函数的导数  54
3.2.3 单变量函数的驻点、极值点、鞍点   55
3.2.4 多变量函数的驻点、极值点、鞍点   57
3.2.5 函数的泰勒级数展开   60
3.2.6 梯度下降法   63
3.3 梯度下降法   73
3.3.1 Adagrad 法   73
3.3.2 Momentum 法   75
3.3.3 NAG 法   77
3.3.4 RMSprop 法  78
3.3.5 具备动量的RMSprop 法   80
3.3.6 Adadelta 法   81
3.3.7 Adam 法  82
3.3.8 Batch 梯度下降  84
3.3.9 随机梯度下降   85
3.3.10 mini-Batch 梯度下降  86
3.4 参考文献  86
4 回归分析88
4.1 线性回归分析   88
4.1.1 一元线性回归   88
4.1.2 保存和加载回归模型   91
4.1.3 多元线性回归   95
4.2 非线性回归分析  99
5 全连接神经网络102
5.1 基本概念  102
5.2 计算步骤  104
5.3 神经网络的矩阵表达   107
5.4 激活函数  112
5.4.1 sigmoid 激活函数   112
5.4.2 tanh 激活函数   113
5.4.3 ReLU 激活函数  114
5.4.4 leaky relu 激活函数  115
5.4.5 elu 激活函数  118
5.4.6 crelu 激活函数   119
5.4.7 selu 激活函数   120
5.4.8 relu6 激活函数   121
5.4.9 softplus 激活函数   121
5.4.10 softsign 激活函数   122
5.5 参考文献  123
6 神经网络处理分类问题125
6.1 TFRecord 文件   125
6.1.1 将ndarray 写入TFRecord 文件  125
6.1.2 从TFRecord 解析数据  128
6.2 建立分类问题的数学模型   134
6.2.1 数据类别（标签）  134
6.2.2 图像与TFRecrder  135
6.2.3 建立模型  140
6.3 损失函数与训练模型   143
6.3.1 sigmoid 损失函数   143
6.3.2 softmax 损失函数   144
6.3.3 训练和评估模型  148
6.4 全连接神经网络的梯度反向传播   151
6.4.1 数学原理及示例  151
6.4.2 梯度消失  166
7 一维离散卷积168
7.1 一维离散卷积的计算原理   168
7.1.1 full 卷积  169
7.1.2 valid 卷积  170
7.1.3 same 卷积   170
7.1.4 full、same、valid 卷积的关系  171
7.2 一维卷积定理   174
7.2.1 一维离散傅里叶变换   174
7.2.2 卷积定理  177
7.3 具备深度的一维离散卷积   182
7.3.1 具备深度的张量与卷积核的卷积   182
7.3.2 具备深度的张量分别与多个卷积核的卷积   183
7.3.3 多个具备深度的张量分别与多个卷积核的卷积   185
8 二维离散卷积187
8.1 二维离散卷积的计算原理   187
8.1.1 full 卷积  187
8.1.2 same 卷积   189
8.1.3 valid 卷积  191
8.1.4 full、same、valid 卷积的关系  192
8.1.5 卷积结果的输出尺寸   193
8.2 离散卷积的性质  194
8.2.1 可分离的卷积核  194
8.2.2 full 和same 卷积的性质  195
8.2.3 快速计算卷积   197
8.3 二维卷积定理   198
8.3.1 二维离散傅里叶变换   198
8.3.2 二维与一维傅里叶变换的关系  201
8.3.3 卷积定理  203
8.3.4 利用卷积定理快速计算卷积   203
8.4 多深度的离散卷积   205
8.4.1 基本的多深度卷积   205
8.4.2 一个张量与多个卷积核的卷积  207
8.4.3 多个张量分别与多个卷积核的卷积   208
8.4.4 在每一深度上分别卷积  211
8.4.5 单个张量与多个卷积核在深度上分别卷积   212
8.4.6 分离卷积  214
9 池化操作218
9.1 same 池化  218
9.1.1 same 最大值池化   218
9.1.2 多深度张量的same 池化   221
9.1.3 多个三维张量的same 最大值池化  223
9.1.4 same 平均值池化   224
9.2 valid 池化  226
9.2.1 多深度张量的vaild 池化   228
9.2.2 多个三维张量的valid 池化  229
10 卷积神经网络231
10.1 浅层卷积神经网络   231
10.2 LeNet   238
10.3 AlexNet  244
10.3.1 AlexNet 网络结构详解  244
10.3.2 dropout 及其梯度下降   247
10.4 VGGNet  256
10.5 GoogleNet   264
10.5.1 网中网结构   264
10.5.2 Batch Normalization   269
10.5.3 BN 与卷积运算的关系  273
10.5.4 指数移动平均   275
10.5.5 带有BN 操作的卷积神经网络  276
10.6 ResNet   281
10.7 参考文献  284
11 卷积的梯度反向传播286
11.1 valid 卷积的梯度  286
11.1.1 已知卷积核，对未知张量求导  286
11.1.2 已知输入张量，对未知卷积核求导   290
11.2 same 卷积的梯度  294
11.2.1 已知卷积核，对输入张量求导  294
11.2.2 已知输入张量，对未知卷积核求导   298
12 池化操作的梯度303
12.1 平均值池化的梯度   303
12.2 最大值池化的梯度   306
13 BN 的梯度反向传播311
13.1 BN 操作与卷积的关系   311
13.2 示例详解  314
14 TensorFlow 搭建神经网络的主要函数324
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>图解深度学习与神经网络：从张量到TensorFlow实现
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习之TensorFlow工程化项目实战
第1篇  准备
第1章  学习准备	2
1.1  TensorFlow能做什么	2
1.2  学习TensorFlow的必备知识	3
1.3  学习技巧：跟读代码	4
1.4  如何学习本书	4
第2章  搭建开发环境	5
2.1  准备硬件环境	5
2.2  下载及安装Anaconda	6
2.3  安装TensorFlow	9
2.4  GPU版本的安装方法	10
2.4.1  在Windows中安装CUDA	10
2.4.2  在Linux中安装CUDA	13
2.4.3  在Windows中安装cuDNN	13
2.4.4  在Linux中安装cuDNN	14
2.4.5  常见错误及解决方案	16
2.5  测试显卡的常用命令	16
2.6  TensorFlow 1.x版本与2.x版本共存的解决方案	18
第3章  实例1：用AI模型识别图像是桌子、猫、狗，还是其他	21
3.1  准备代码环境并预训练模型	21
3.2  代码实现：初始化环境变量，并载入ImgNet标签	24
3.3  代码实现：定义网络结构	25
3.4  代码实现：载入模型进行识别	26
3.5  扩展：用更多预训练模型完成图片分类任务	28
第2篇  基础
第4章  用TensorFlow制作自己的数据集	30
4.1  快速导读	30
4.1.1  什么是数据集	30
4.1.2  TensorFlow的框架	31
4.1.3  什么是TFDS	31
4.2  实例2：将模拟数据制作成内存对象数据集	32
4.2.1  代码实现：生成模拟数据	32
4.2.2  代码实现：定义占位符	33
4.2.3  代码实现：建立会话，并获取数据	34
4.2.4  代码实现：将模拟数据可视化	34
4.2.5  运行程序	34
4.2.6  代码实现：创建带有迭代值并支持乱序功能的模拟数据集	35
4.3  实例3：将图片制作成内存对象数据集	37
4.3.1  样本介绍	38
4.3.2  代码实现：载入文件名称与标签	39
4.3.3  代码实现：生成队列中的批次样本数据	40
4.3.4  代码实现：在会话中使用数据集	41
4.3.5  运行程序	42
4.4  实例4：将Excel文件制作成内存对象数据集	42
4.4.1  样本介绍	43
4.4.2  代码实现：逐行读取数据并分离标签	43
4.4.3  代码实现：生成队列中的批次样本数据	44
4.4.4  代码实现：在会话中使用数据集	45
4.4.5  运行程序	46
4.5  实例5：将图片文件制作成TFRecord数据集	46
4.5.1  样本介绍	47
4.5.2  代码实现：读取样本文件的目录及标签	47
4.5.3  代码实现：定义函数生成TFRecord数据集	48
4.5.4  代码实现：读取TFRecord数据集，并将其转化为队列	49
4.5.5  代码实现：建立会话，将数据保存到文件	50
4.5.6  运行程序	51
4.6  实例6：将内存对象制作成Dataset数据集	52
4.6.1  如何生成Dataset数据集	52
4.6.2  如何使用Dataset接口	53
4.6.3  tf.data.Dataset接口所支持的数据集变换操作	54
4.6.4  代码实现：以元组和字典的方式生成Dataset对象	58
4.6.5  代码实现：对Dataset对象中的样本进行变换操作	59
4.6.6  代码实现：创建Dataset迭代器	60
4.6.7  代码实现：在会话中取出数据	60
4.6.8  运行程序	61
4.6.9  使用tf.data.Dataset.from_tensor_slices接口的注意事项	62
4.7  实例7：将图片文件制作成Dataset数据集	63
4.7.1  代码实现：读取样本文件的目录及标签	64
4.7.2  代码实现：定义函数，实现图片转换操作	64
4.7.3  代码实现：用自定义函数实现图片归一化	65
4.7.4  代码实现：用第三方函数将图片旋转30°	65
4.7.5  代码实现：定义函数，生成Dataset对象	66
4.7.6  代码实现：建立会话，输出数据	67
4.7.7  运行程序	68
4.8  实例8：将TFRecord文件制作成Dataset数据集	69
4.8.1  样本介绍	69
4.8.2  代码实现：定义函数，生成Dataset对象	70
4.8.3  代码实现：建立会话输出数据	71
4.8.4  运行程序	72
4.9  实例9：在动态图中读取Dataset数据集	72
4.9.1  代码实现：添加动态图调用	72
4.9.2  制作数据集	73
4.9.3  代码实现：在动态图中显示数据	73
4.9.4  实例10：在TensorFlow 2.x中操作数据集	74
4.10  实例11：在不同场景中使用数据集	77
4.10.1  代码实现：在训练场景中使用数据集	78
4.10.2  代码实现：在应用模型场景中使用数据集	79
4.10.3  代码实现：在训练与测试混合场景中使用数据集	80
4.11  tf.data.Dataset接口的更多应用	81
第5章  10分钟快速训练自己的图片分类模型	82
5.1  快速导读	82
5.1.1  认识模型和模型检查点文件	82
5.1.2  了解“预训练模型”与微调（Fine-Tune）	82
5.1.3  学习TensorFlow中的预训练模型库——TF-Hub库	83
5.2  实例12：通过微调模型分辨男女	83
5.2.1  准备工作	84
5.2.2  代码实现：处理样本数据并生成Dataset对象	85
5.2.3  代码实现：定义微调模型的类MyNASNetModel	88
5.2.4  代码实现：构建MyNASNetModel类中的基本模型	88
5.2.5  代码实现：实现MyNASNetModel类中的微调操作	89
5.2.6  代码实现：实现与训练相关的其他方法	90
5.2.7  代码实现：构建模型，用于训练、测试、使用	92
5.2.8  代码实现：通过二次迭代来训练微调模型	94
5.2.9  代码实现：测试模型	96
5.3  扩展：通过摄像头实时分辨男女	100
5.4  TF-slim接口中的更多成熟模型	100
5.5  实例13：用TF-Hub库微调模型以评估人物的年龄	100
5.5.1  准备样本	101
5.5.2  下载TF-Hub库中的模型	102
5.5.3  代码实现：测试TF-Hub库中的MobileNet_V2模型	104
5.5.4  用TF-Hub库微调MobileNet_V2模型	107
5.5.5  代码实现：用模型评估人物的年龄	109
5.5.6  扩展：用TF-Hub库中的其他模型处理不同领域的分类任务	113
5.6  总结	113
5.7  练习题	114
5.7.1  基于TF-slim接口的练习	115
5.7.2  基于TF-Hub库的练习	115
第6章  用TensorFlow编写训练模型的程序	117
6.1  快速导读	117
6.1.1  训练模型是怎么一回事	117
6.1.2  用“静态图”方式训练模型	117
6.1.3  用“动态图”方式训练模型	118
6.1.4  什么是估算器框架接口（Estimators API）	119
6.1.5  什么是tf.layers接口	120
6.1.6  什么是tf.keras接口	121
6.1.7  什么是tf.js接口	122
6.1.8  什么是TFLearn框架	123
6.1.9  该选择哪种框架	123
6.1.10  分配运算资源与使用分布策略	124
6.1.11  用tfdbg调试TensorFlow模型	127
6.1.12  用钩子函数（Training_Hooks）跟踪训练状态	127
6.1.13  用分布式运行方式训练模型	128
6.1.14  用T2T框架系统更方便地训练模型	128
6.1.15  将TensorFlow 1.x中的代码移植到2.x版本	129
6.1.16  TensorFlow 2.x中的新特性——自动图	130
6.2  实例14：用静态图训练一个具有保存检查点功能的回归模型	131
6.2.1  准备开发步骤	131
6.2.2  生成检查点文件	131
6.2.3  载入检查点文件	132
6.2.4  代码实现：在线性回归模型中加入保存检查点功能	132
6.2.5  修改迭代次数，二次训练	135
6.3  实例15：用动态图（eager）训练一个具有保存检查点功能的回归模型	136
6.3.1  代码实现：启动动态图，生成模拟数据	136
6.3.2  代码实现：定义动态图的网络结构	137
6.3.3  代码实现：在动态图中加入保存检查点功能	138
6.3.4  代码实现：按指定迭代次数进行训练，并可视化结果	139
6.3.5  运行程序，显示结果	140
6.3.6  代码实现：用另一种方法计算动态图梯度	141
6.3.7  实例16：在动态图中获取参数变量	142
6.3.8  小心动态图中的参数陷阱	144
6.3.9  实例17：在静态图中使用动态图	145
6.4  实例18：用估算器框架训练一个回归模型	147
6.4.1  代码实现：生成样本数据集	147
6.4.2  代码实现：设置日志级别	148
6.4.3  代码实现：实现估算器的输入函数	148
6.4.4  代码实现：定义估算器的模型函数	149
6.4.5  代码实现：通过创建config文件指定硬件的运算资源	151
6.4.6  代码实现：定义估算器	152
6.4.7  用tf.estimator.RunConfig控制更多的训练细节	153
6.4.8  代码实现：用估算器训练模型	153
6.4.9  代码实现：通过热启动实现模型微调	155
6.4.10  代码实现：测试估算器模型	158
6.4.11  代码实现：使用估算器模型	158
6.4.12  实例19：为估算器添加日志钩子函数	159
6.5  实例20：将估算器代码改写成静态图代码	161
6.5.1  代码实现：复制网络结构	161
6.5.2  代码实现：重用输入函数	163
6.5.3  代码实现：创建会话恢复模型	163
6.5.4  代码实现：继续训练	163
6.6  实例21：用tf.layers API在动态图上识别手写数字	165
6.6.1  代码实现：启动动态图并加载手写图片数据集	165
6.6.2  代码实现：定义模型的类	166
6.6.3  代码实现：定义网络的反向传播	167
6.6.4  代码实现：训练模型	167
6.7  实例22：用tf.keras API训练一个回归模型	168
6.7.1  代码实现：用model类搭建模型	168
6.7.2  代码实现：用sequential类搭建模型	169
6.7.3  代码实现：搭建反向传播的模型	171
6.7.4  代码实现：用两种方法训练模型	172
6.7.5  代码实现：获取模型参数	172
6.7.6  代码实现：测试模型与用模型进行预测	173
6.7.7  代码实现：保存模型与加载模型	173
6.7.8  代码实现：将模型导出成JSON文件，再将JSON文件导入模型	175
6.7.9  实例23：在tf.keras接口中使用预训练模型ResNet	176
6.7.10  扩展：在动态图中使用tf.keras接口	178
6.7.11  实例24：在静态图中使用tf.keras接口	178
6.8  实例25：用tf.js接口后方训练一个回归模型	180
6.8.1  代码实现：在HTTP的头标签中添加tfjs模块	180
6.8.2  代码实现：用JavaScript脚本实现回归模型	181
6.8.3  运行程序：在浏览器中查看效果	181
6.8.4  扩展：tf.js 接口的应用场景	182
6.9  实例26：用估算器框架实现分布式部署训练	182
6.9.1  运行程序：修改估算器模型，使其支持分布式	182
6.9.2  通过TF_CONFIG进行分布式配置	183
6.9.3  运行程序	185
6.9.4  扩展：用分布策略或KubeFlow框架进行分布式部署	186
6.10  实例27：在分布式估算器框架中用tf.keras接口训练ResNet模型，识别图片中是橘子还是苹果	186
6.10.1  样本准备	186
6.10.2  代码实现：准备训练与测试数据集	187
6.10.3  代码实现：制作模型输入函数	187
6.10.4  代码实现：搭建ResNet模型	188
6.10.5  代码实现：训练分类器模型	189
6.10.6  运行程序：评估模型	190
6.10.7  扩展：全连接网络的优化	190
6.11  实例28：在T2T框架中用tf.layers接口实现MNIST数据集分类	191
6.11.1  代码实现：查看T2T框架中的数据集（problems）	191
6.11.2  代码实现：构建T2T框架的工作路径及下载数据集	192
6.11.3  代码实现：在T2T框架中搭建自定义卷积网络模型	193
6.11.4  代码实现：用动态图方式训练自定义模型	194
6.11.5  代码实现：在动态图中用metrics模块评估模型	195
6.12  实例29：在T2T框架中，用自定义数据集训练中英文翻译模型	196
6.12.1  代码实现：声明自己的problems数据集	196
6.12.2  代码实现：定义自己的problems数据集	197
6.12.3  在命令行下生成TFrecoder格式的数据	198
6.12.4  查找T2T框架中的模型及超参，并用指定的模型及超参进行训练	199
6.12.5  用训练好的T2T框架模型进行预测	201
6.12.6  扩展：在T2T框架中，如何选取合适的模型及超参	202
6.13  实例30：将TensorFlow 1.x中的代码升级为可用于2.x版本的代码	203
6.13.1  准备工作：创建Python虚环境	203
6.13.2  使用工具转换源码	204
6.13.3  修改转换后的代码文件	204
6.13.4  将代码升级到TensorFlow 2.x版本的经验总结	205
第3篇  进阶
第7章  特征工程——会说话的数据	208
7.1  快速导读	208
7.1.1  特征工程的基础知识	208
7.1.2  离散数据特征与连续数据特征	209
7.1.3  了解特征列接口	210
7.1.4  了解序列特征列接口	210
7.1.5  了解弱学习器接口——梯度提升树（TFBT接口）	210
7.1.6  了解特征预处理模块（tf.Transform）	211
7.1.7  了解因子分解模块	212
7.1.8  了解加权矩阵分解算法	212
7.1.9  了解Lattice模块——点阵模型	213
7.1.10  联合训练与集成学习	214
7.2  实例31：用wide_deep模型预测人口收入	214
7.2.1  了解人口收入数据集	214
7.2.2  代码实现：探索性数据分析	217
7.2.3  认识wide_deep模型	218
7.2.4  部署代码文件	219
7.2.5  代码实现：初始化样本常量	220
7.2.6  代码实现：生成特征列	220
7.2.7  代码实现：生成估算器模型	222
7.2.8  代码实现：定义输入函数	223
7.2.9  代码实现：定义用于导出冻结图文件的函数	224
7.2.10  代码实现：定义类，解析启动参数	225
7.2.11  代码实现：训练和测试模型	226
7.2.12  代码实现：使用模型	227
7.2.13  运行程序	228
7.3  实例32：用弱学习器中的梯度提升树算法预测人口收入	229
7.3.1  代码实现：为梯度提升树模型准备特征列	230
7.3.2  代码实现：构建梯度提升树模型	230
7.3.3  代码实现：训练并导出梯度提升树模型	231
7.3.4  代码实现：设置启动参数，运行程序	232
7.3.5  扩展：更灵活的TFBT接口	233
7.4  实例33：用feature_column模块转换特征列	233
7.4.1  代码实现：用feature_column模块处理连续值特征列	234
7.4.2  代码实现：将连续值特征列转化成离散值特征列	237
7.4.3  代码实现：将离散文本特征列转化为one-hot与词向量	239
7.4.4  代码实现：根据特征列生成交叉列	246
7.5  实例34：用sequence_feature_column接口完成自然语言处理任务的数据预处理工作	248
7.5.1  代码实现：构建模拟数据	248
7.5.2  代码实现：构建词嵌入初始值	249
7.5.3  代码实现：构建词嵌入特征列与共享特征列	249
7.5.4  代码实现：构建序列特征列的输入层	250
7.5.5  代码实现：建立会话输出结果	251
7.6  实例35：用factorization模块的kmeans接口聚类COCO数据集中的标注框	253
7.6.1  代码实现：设置要使用的数据集	253
7.6.2  代码实现：准备带聚类的数据样本	253
7.6.3  代码实现：定义聚类模型	255
7.6.4  代码实现：训练模型	256
7.6.5  代码实现：输出图示化结果	256
7.6.6  代码实现：提取并排序聚类结果	258
7.6.7  扩展：聚类与神经网络混合训练	258
7.7  实例36：用加权矩阵分解模型实现基于电影评分的推荐系统	259
7.7.1  下载并加载数据集	259
7.7.2  代码实现：根据用户和电影特征列生成稀疏矩阵	260
7.7.3  代码实现：建立WALS模型，并对其进行训练	261
7.7.4  代码实现：评估WALS模型	263
7.7.5  代码实现：用WALS模型为用户推荐电影	264
7.7.6  扩展：使用WALS的估算器接口	265
7.8  实例37：用Lattice模块预测人口收入	265
7.8.1  代码实现：读取样本，并创建输入函数	266
7.8.2  代码实现：创建特征列，并保存校准关键点	267
7.8.3  代码实现：创建校准线性模型	270
7.8.4  代码实现：创建校准点阵模型	270
7.8.5  代码实现：创建随机微点阵模型	271
7.8.6  代码实现：创建集合的微点阵模型	271
7.8.7  代码实现：定义评估与训练函数	272
7.8.8  代码实现：训练并评估模型	273
7.8.9  扩展：将点阵模型嵌入神经网络中	274
7.9  实例38：结合知识图谱实现基于电影的推荐系统	278
7.9.1  准备数据集	278
7.9.2  预处理数据	279
7.9.3  搭建MKR模型	279
7.9.4  训练模型并输出结果	286
7.10  可解释性算法的意义	286
第8章  卷积神经网络（CNN）——在图像处理中应用最广泛的模型	287
8.1  快速导读	287
8.1.1  认识卷积神经网络	287
8.1.2  什么是空洞卷积	288
8.1.3  什么是深度卷积	290
8.1.4  什么是深度可分离卷积	290
8.1.5  了解卷积网络的缺陷及补救方法	291
8.1.6  了解胶囊神经网络与动态路由	292
8.1.7  了解矩阵胶囊网络与EM路由算法	297
8.1.8  什么是NLP任务	298
8.1.9  了解多头注意力机制与内部注意力机制	298
8.1.10  什么是带有位置向量的词嵌入	300
8.1.11  什么是目标检测任务	300
8.1.12  什么是目标检测中的上采样与下采样	301
8.1.13  什么是图片分割任务	301
8.2  实例39：用胶囊网络识别黑白图中服装的图案	302
8.2.1  熟悉样本：了解Fashion-MNIST数据集	302
8.2.2  下载Fashion-MNIST数据集	303
8.2.3  代码实现：读取及显示Fashion-MNIST数据集中的数据	304
8.2.4  代码实现：定义胶囊网络模型类CapsuleNetModel	305
8.2.5  代码实现：实现胶囊网络的基本结构	306
8.2.6  代码实现：构建胶囊网络模型	309
8.2.7  代码实现：载入数据集，并训练胶囊网络模型	310
8.2.8  代码实现：建立会话训练模型	311
8.2.9  运行程序	313
8.2.10  实例40：实现带有EM路由的胶囊网络	314
8.3  实例41：用TextCNN模型分析评论者是否满意	322
8.3.1  熟悉样本：了解电影评论数据集	322
8.3.2  熟悉模型：了解TextCNN模型	322
8.3.3  数据预处理：用preprocessing接口制作字典	323
8.3.4  代码实现：生成NLP文本数据集	326
8.3.5  代码实现：定义TextCNN模型	327
8.3.6  代码实现：训练TextCNN模型	330
8.3.7  运行程序	332
8.3.8  扩展：提升模型精度的其他方法	333
8.4  实例42：用带注意力机制的模型分析评论者是否满意	333
8.4.1  熟悉样本：了解tf.keras接口中的电影评论数据集	333
8.4.2  代码实现：将tf.keras接口中的IMDB数据集还原成句子	334
8.4.3  代码实现：用tf.keras接口开发带有位置向量的词嵌入层	336
8.4.4  代码实现：用tf.keras接口开发注意力层	338
8.4.5  代码实现：用tf.keras接口训练模型	340
8.4.6  运行程序	341
8.4.7  扩展：用Targeted Dropout技术进一步提升模型的性能	342
8.5  实例43：搭建YOLO V3模型，识别图片中的酒杯、水果等物体	343
8.5.1  YOLO V3模型的样本与结构	343
8.5.2  代码实现：Darknet-53 模型的darknet块	344
8.5.3  代码实现：Darknet-53 模型的下采样卷积	345
8.5.4  代码实现：搭建Darknet-53模型，并返回3种尺度特征值	345
8.5.5  代码实现：定义YOLO检测模块的参数及候选框	346
8.5.6  代码实现：定义YOLO检测块，进行多尺度特征融合	347
8.5.7  代码实现：将YOLO检测块的特征转化为bbox attrs单元	347
8.5.8  代码实现：实现YOLO V3的检测部分	349
8.5.9  代码实现：用非极大值抑制算法对检测结果去重	352
8.5.10  代码实现：载入预训练权重	355
8.5.11  代码实现：载入图片，进行目标实物的识别	356
8.5.12  运行程序	358
8.6  实例44：用YOLO V3模型识别门牌号	359
8.6.1  工程部署：准备样本	359
8.6.2  代码实现：读取样本数据，并制作标签	359
8.6.3  代码实现：用tf.keras接口构建YOLO V3模型，并计算损失	364
8.6.4  代码实现：在动态图中训练模型	368
8.6.5  代码实现：用模型识别门牌号	372
8.6.6  扩展：标注自己的样本	374
8.7  实例45：用Mask R-CNN模型定位物体的像素点	375
8.7.1  下载COCO数据集及安装pycocotools	376
8.7.2  代码实现：验证pycocotools及读取COCO数据集	377
8.7.3  拆分Mask R-CNN模型的处理步骤	383
8.7.4  工程部署：准备代码文件及模型	385
8.7.5  代码实现：加载数据构建模型，并输出模型权重	385
8.7.6  代码实现：搭建残差网络ResNet	387
8.7.7  代码实现：搭建Mask R-CNN模型的骨干网络ResNet	393
8.7.8  代码实现：可视化Mask R-CNN模型骨干网络的特征输出	396
8.7.9  代码实现：用特征金字塔网络处理骨干网络特征	400
8.7.10  计算RPN中的锚点	402
8.7.11  代码实现：构建RPN	403
8.7.12  代码实现：用非极大值抑制算法处理RPN的结果	405
8.7.13  代码实现：提取RPN的检测结果	410
8.7.14  代码实现：可视化RPN的检测结果	412
8.7.15  代码实现：在MaskRCNN类中对ROI区域进行分类	415
8.7.16  代码实现：金字塔网络的区域对齐层（ROIAlign）中的区域框与特征的匹配算法	416
8.7.17  代码实现：在金字塔网络的ROIAlign层中按区域边框提取内容	418
8.7.18  代码实现：调试并输出ROIAlign层的内部运算值	421
8.7.19  代码实现：对ROI内容进行分类	422
8.7.20  代码实现：用检测器DetectionLayer检测ROI内容，得到最终的实物矩形	426
8.7.21  代码实现：根据ROI内容进行实物像素分割	432
8.7.22  代码实现：用Mask R-CNN模型分析图片	436
8.8  实例46：训练Mask R-CNN模型，进行形状的识别	439
8.8.1  工程部署：准备代码文件及模型	440
8.8.2  样本准备：生成随机形状图片	440
8.8.3  代码实现：为Mask R-CNN模型添加损失函数	442
8.8.4  代码实现：为Mask R-CNN模型添加训练函数，使其支持微调与全网训练	444
8.8.5  代码实现：训练并使用模型	446
8.8.6  扩展：替换特征提取网络	449
第9章  循环神经网络（RNN）——处理序列样本的神经网络	450
9.1  快速导读	450
9.1.1  什么是循环神经网络	450
9.1.2  了解RNN模型的基础单元LSTM与GRU	451
9.1.3  认识QRNN单元	451
9.1.4  认识SRU单元	451
9.1.5  认识IndRNN单元	452
9.1.6  认识JANET单元	453
9.1.7  优化RNN模型的技巧	453
9.1.8  了解RNN模型中多项式分布的应用	453
9.1.9  了解注意力机制的Seq2Seq框架	454
9.1.10  了解BahdanauAttention与LuongAttention	456
9.1.11  了解单调注意力机制	457
9.1.12  了解混合注意力机制	458
9.1.13  了解Seq2Seq接口中的采样接口（Helper）	460
9.1.14  了解RNN模型的Wrapper接口	460
9.1.15  什么是时间序列（TFTS）框架	461
9.1.16  什么是梅尔标度	461
9.1.17  什么是短时傅立叶变换	462
9.2  实例47：搭建RNN模型，为女孩生成英文名字	463
9.2.1  代码实现：读取及处理样本	463
9.2.2  代码实现：构建Dataset数据集	466
9.2.3  代码实现：用tf.keras接口构建生成式RNN模型	467
9.2.4  代码实现：在动态图中训练模型	468
9.2.5  代码实现：载入检查点文件并用模型生成名字	469
9.2.6  扩展：用RNN模型编写文章	471
9.3  实例48：用带注意力机制的Seq2Seq模型为图片添加内容描述	471
9.3.1  设计基于图片的Seq2Seq	471
9.3.2  代码实现：图片预处理——用ResNet提取图片特征并保存	472
9.3.3  代码实现：文本预处理——过滤处理、字典建立、对齐与向量化处理	475
9.3.4  代码实现：创建数据集	477
9.3.5  代码实现：用tf.keras接口构建Seq2Seq模型中的编码器	477
9.3.6  代码实现：用tf.keras接口构建Bahdanau类型的注意力机制	478
9.3.7  代码实现：搭建Seq2Seq模型中的解码器Decoder	478
9.3.8  代码实现：在动态图中计算Seq2Seq模型的梯度	480
9.3.9  代码实现：在动态图中为Seq2Seq模型添加保存检查点功能	480
9.3.10  代码实现：在动态图中训练Seq2Seq模型	481
9.3.11  代码实现：用多项式分布采样获取图片的内容描述	482
9.4  实例49：用IndRNN与IndyLSTM单元制作聊天机器人	485
9.4.1  下载及处理样本	486
9.4.2  代码实现：读取样本，分词并创建字典	487
9.4.3  代码实现：对样本进行向量化、对齐、填充预处理	489
9.4.4  代码实现：在Seq2Seq模型中加工样本	489
9.4.5  代码实现：在Seq2Seq模型中，实现基于IndRNN与IndyLSTM的
动态多层RNN编码器	491
9.4.6  代码实现：为Seq2Seq模型中的解码器创建Helper	491
9.4.7  代码实现：实现带有Bahdanau注意力、dropout、OutputProjectionWrapper的解码器	492
9.4.8  代码实现：在Seq2Seq模型中实现反向优化	493
9.4.9  代码实现：创建带有钩子函数的估算器，并进行训练	494
9.4.10  代码实现：用估算器框架评估模型	496
9.4.11  扩展：用注意力机制的Seq2Seq模型实现中英翻译	498
9.5  实例50：预测飞机发动机的剩余使用寿命	498
9.5.1  准备样本	499
9.5.2  代码实现：预处理数据——制作数据集的输入样本与标签	500
9.5.3  代码实现：构建带有JANET单元的多层动态RNN模型	504
9.5.4  代码实现：训练并测试模型	505
9.5.5  运行程序	507
9.5.6  扩展：为含有JANET单元的RNN模型添加注意力机制	508
9.6  实例51：将动态路由用于RNN模型，对路透社新闻进行分类	509
9.6.1  准备样本	509
9.6.2  代码实现：预处理数据——对齐序列数据并计算长度	510
9.6.3  代码实现：定义数据集	510
9.6.4  代码实现：用动态路由算法聚合信息	511
9.6.5  代码实现：用IndyLSTM单元搭建RNN模型	513
9.6.6  代码实现：建立会话，训练网络	514
9.6.7  扩展：用分级网络将文章（长文本数据）分类	515
9.7  实例52：用TFTS框架预测某地区每天的出生人数	515
9.7.1  准备样本	515
9.7.2  代码实现：数据预处理——制作TFTS框架中的读取器	515
9.7.3  代码实现：用TFTS框架定义模型，并进行训练	516
9.7.4  代码实现：用TFTS框架评估模型	517
9.7.5  代码实现：用模型进行预测，并将结果可视化	517
9.7.6  运行程序	518
9.7.7  扩展：用TFTS框架进行异常值检测	519
9.8  实例53：用Tacotron模型合成中文语音（TTS）	520
9.8.1  准备安装包及样本数据	520
9.8.2  代码实现：将音频数据分帧并转为梅尔频谱	521
9.8.3  代码实现：用多进程预处理样本并保存结果	523
9.8.4  拆分Tacotron网络模型的结构	525
9.8.5  代码实现：搭建CBHG网络	527
9.8.6  代码实现：构建带有混合注意力机制的模块	529
9.8.7  代码实现：构建自定义wrapper	531
9.8.8  代码实现：构建自定义采样器	534
9.8.9  代码实现：构建自定义解码器	537
9.8.10  代码实现：构建输入数据集	539
9.8.11  代码实现：构建Tacotron网络	542
9.8.12  代码实现：构建Tacotron网络模型的训练部分	545
9.8.13  代码实现：训练模型并合成音频文件	546
9.8.14  扩展：用pypinyin模块实现文字到声音的转换	551
第4篇  高级
第10章  生成式模型——能够输出内容的模型	554
10.1  快速导读	554
10.1.1  什么是自编码网络模型	554
10.1.2  什么是对抗神经网络模型	554
10.1.3  自编码网络模型与对抗神经网络模型的关系	555
10.1.4  什么是批量归一化中的自适应模式	555
10.1.5  什么是实例归一化	556
10.1.6  了解SwitchableNorm及更多的归一化方法	556
10.1.7  什么是图像风格转换任务	557
10.1.8  什么是人脸属性编辑任务	558
10.1.9  什么是TFgan框架	558
10.2  实例54：构建DeblurGAN模型，将模糊相片变清晰	559
10.2.1  获取样本	559
10.2.2  准备SwitchableNorm算法模块	560
10.2.3  代码实现：构建DeblurGAN中的生成器模型	560
10.2.4  代码实现：构建DeblurGAN中的判别器模型	562
10.2.5  代码实现：搭建DeblurGAN的完整结构	563
10.2.6  代码实现：引入库文件，定义模型参数	563
10.2.7  代码实现：定义数据集，构建正反向模型	564
10.2.8  代码实现：计算特征空间损失，并将其编译到生成器模型的训练模型中	566
10.2.9  代码实现：按指定次数训练模型	568
10.2.10  代码实现：用模型将模糊相片变清晰	569
10.2.11  练习题	572
10.2.12  扩展：DeblurGAN模型的更多妙用	572
10.3  实例55：构建AttGAN模型，对照片进行加胡子、加头帘、加眼镜、变年轻等修改	573
10.3.1  获取样本	573
10.3.2  了解AttGAN模型的结构	574
10.3.3  代码实现：实现支持动态图和静态图的数据集工具类	575
10.3.4  代码实现：将CelebA做成数据集	577
10.3.5  代码实现：构建AttGAN模型的编码器	581
10.3.6  代码实现：构建含有转置卷积的解码器模型	582
10.3.7  代码实现：构建AttGAN模型的判别器模型部分	584
10.3.8  代码实现：定义模型参数，并构建AttGAN模型	585
10.3.9  代码实现：定义训练参数，搭建正反向模型	587
10.3.10  代码实现：训练模型	592
10.3.11  实例56：为人脸添加不同的眼镜	595
10.3.12  扩展：AttGAN模型的局限性	597
10.4  实例57：用RNN.WGAN模型模拟生成恶意请求	597
10.4.1  获取样本：通过Panabit设备获取恶意请求样本	597
10.4.2  了解RNN.WGAN模型	600
10.4.3  代码实现：构建RNN.WGAN模型	601
10.4.4  代码实现：训练指定长度的RNN.WGAN模型	607
10.4.5  代码实现：用长度依次递增的方式训练模型	612
10.4.6  运行代码	613
10.4.7  扩展：模型的使用及优化	614
第11章  模型的攻与防——看似智能的AI也有脆弱的一面	616
11.1  快速导读	616
11.1.1  什么是FGSM方法	616
11.1.2  什么是cleverhans模块	616
11.1.3  什么是黑箱攻击	617
11.1.4  什么是基于雅可比矩阵的数据增强方法	618
11.1.5  什么是数据中毒攻击	620
11.2  实例58：用FGSM方法生成样本，并攻击PNASNet模型，让其将“狗”识别成“盘子”	621
11.2.1  代码实现：创建PNASNet模型	621
11.2.2  代码实现：搭建输入层并载入图片，复现PNASNet模型的预测效果	623
11.2.3  代码实现：调整参数，定义图片的变化范围	624
11.2.4  代码实现：用梯度下降方式生成对抗样本	625
11.2.5  代码实现：用生成的样本攻击模型	626
11.2.6  扩展：如何防范攻击模型的行为	627
11.2.7  代码实现：将数据增强方式用在使用场景，以加固PNASNet模型，防范攻击	627
11.3  实例59：击破数据增强防护，制作抗旋转对抗样本	629
11.3.1  代码实现：对输入的数据进行多次旋转	629
11.3.2  代码实现：生成并保存鲁棒性更好的对抗样本	630
11.3.3  代码实现：在PNASNet模型中比较对抗样本的效果	631
11.4  实例60：以黑箱方式攻击未知模型	633
11.4.1  准备工程代码	633
11.4.2  代码实现：搭建通用模型框架	634
11.4.3  代码实现：搭建被攻击模型	637
11.4.4  代码实现：训练被攻击模型	638
11.4.5  代码实现：搭建替代模型	639
11.4.6  代码实现：训练替代模型	639
11.4.7  代码实现：黑箱攻击目标模型	641
11.4.8  扩展：利用黑箱攻击中的对抗样本加固模型	645
第5篇  实战——深度学习实际应用
第12章  TensorFlow模型制作——一种功能，多种身份	648
12.1  快速导读	648
12.1.1  详细分析检查点文件	648
12.1.2  什么是模型中的冻结图	649
12.1.3  什么是TF Serving模块与saved_model模块	649
12.1.4  用编译子图（defun）提升动态图的执行效率	649
12.1.5  什么是TF_Lite模块	652
12.1.6  什么是TFjs-converter模块	653
12.2  实例61：在源码与检查点文件分离的情况下，对模型进行二次训练	653
12.2.1  代码实现：在线性回归模型中，向检查点文件中添加指定节点	654
12.2.2  代码实现：在脱离源码的情况下，用检查点文件进行二次训练	657
12.2.3  扩展：更通用的二次训练方法	659
12.3  实例62：导出/导入冻结图文件	661
12.3.1  熟悉TensorFlow中的freeze_graph工具脚本	661
12.3.2  代码实现：从线性回归模型中导出冻结图文件	662
12.3.3  代码实现：导入冻结图文件，并用模型进行预测	664
12.4  实例63：逆向分析冻结图文件	665
12.4.1  使用import_to_tensorboard工具	666
12.4.2  用TensorBoard工具查看模型结构	666
12.5  实例64：用saved_model模块导出与导入模型文件	668
12.5.1  代码实现：用saved_model模块导出模型文件	668
12.5.2  代码实现：用saved_model模块导入模型文件	669
12.5.3  扩展：用saved_model模块导出带有签名的模型文件	670
12.6  实例65：用saved_model_cli工具查看及使用saved_model模型	672
12.6.1  用show参数查看模型	672
12.6.2  用run参数运行模型	673
12.6.3  扩展：了解scan参数的黑名单机制	674
12.7  实例66：用TF-Hub库导入、导出词嵌入模型文件	674
12.7.1  代码实现：模拟生成通用词嵌入模型	674
12.7.2  代码实现：用TF-Hub库导出词嵌入模型	675
12.7.3  代码实现：导出TF-Hub模型	678
12.7.4  代码实现：用TF-Hub库导入并使用词嵌入模型	680
第13章  部署TensorFlow模型——模型与项目的深度结合	681
13.1  快速导读	681
13.1.1  什么是gRPC服务与HTTP/REST API	681
13.1.2  了解TensorFlow对移动终端的支持	682
13.1.3  了解树莓派上的人工智能	683
13.2  实例67：用TF_Serving部署模型并进行远程使用	684
13.2.1  在Linux系统中安装TF_Serving	684
13.2.2  在多平台中用Docker安装TF_Serving	685
13.2.3  编写代码：固定模型的签名信息	686
13.2.4  在Linux中开启TF_Serving服务	688
13.2.5  编写代码：用gRPC访问远程TF_Serving服务	689
13.2.6  用HTTP/REST API访问远程TF_Serving服务	691
13.2.7  扩展：关于TF_Serving的更多例子	694
13.3  实例68：在安卓手机上识别男女	694
13.3.1  准备工程代码	694
13.3.2  微调预训练模型	695
13.3.3  搭建安卓开发环境	698
13.3.4  制作lite模型文件	701
13.3.5  修改分类器代码，并运行App	702
13.4  实例69：在iPhone手机上识别男女并进行活体检测	703
13.4.1  搭建iOS开发环境	703
13.4.2  部署工程代码并编译	704
13.4.3  载入Lite模型，实现识别男女功能	706
13.4.4  代码实现：调用摄像头并采集视频流	707
13.4.5  代码实现：提取人脸特征	710
13.4.6  活体检测算法介绍	712
13.4.7  代码实现：实现活体检测算法	713
13.4.8  代码实现：完成整体功能并运行程序	714
13.5  实例70：在树莓派上搭建一个目标检测器	717
13.5.1  安装树莓派系统	718
13.5.2  在树莓派上安装TensorFlow	721
13.5.3  编译并安装Protobuf	725
13.5.4  安装OpenCV	726
13.5.5  下载目标检测模型SSDLite	726
13.5.6  代码实现：用SSDLite 模型进行目标检测	727
第14章  商业实例——科技源于生活，用于生活	730
14.1  实例71：将特征匹配技术应用在商标识别领域	730
14.1.1  项目背景	730
14.1.2  技术方案	730
14.1.3  预处理图片——统一尺寸	731
14.1.4  用自编码网络加夹角余弦实现商标识别	731
14.1.5  用卷积网络加triplet-loss提升特征提取效果	731
14.1.6  进一步的优化空间	732
14.2  实例72：用RNN抓取蠕虫病毒	732
14.2.1  项目背景	733
14.2.2  判断是否恶意域名不能只靠域名	733
14.2.3  如何识别恶意域名	733
14.3  实例73：迎宾机器人的技术关注点——体验优先	734
14.3.1  迎宾机器人的产品背景	734
14.3.2  迎宾机器人的实现方案	734
14.3.3  迎宾机器人的同类产品	736
14.4  实例74：基于摄像头的路边停车场项目	737
14.4.1  项目背景	737
14.4.2  技术方案	738
14.4.3  方案缺陷	738
14.4.4  工程化补救方案	738
14.5  实例75：智能冰箱产品——硬件成本之痛	739
14.5.1  智能冰箱系列的产品背景	739
14.5.2  智能冰箱的技术基础	740
14.5.3  真实的非功能性需求——低成本	740
14.5.4  未来的技术趋势及应对策略	741
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习之TensorFlow工程化项目实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python机器学习（原书第2版)
译者序
关于作者
关于审校人员
前言
第1章　赋予计算机从数据中学习的能力 1
1.1　构建把数据转换为知识的智能机器 1
1.2　三种不同类型的机器学习 1
1.2.1　用有监督学习预测未来 2
1.2.2　用强化学习解决交互问题 3
1.2.3　用无监督学习发现隐藏结构 4
1.3　基本术语与符号 4
1.4　构建机器学习系统的路线图 6
1.4.1　预处理—整理数据 6
1.4.2　训练和选择预测模型 7
1.4.3　评估模型和预测新样本数据 7
1.5　用Python进行机器学习 7
1.5.1　从Python包索引安装Python和其他包 8
1.5.2　采用Anaconda Python和软件包管理器 8
1.5.3　科学计算、数据科学和机器学习软件包 8
1.6　小结 9
第2章　训练简单的机器学习分类算法 10
2.1　人工神经元—机器学习早期历史一瞥 10
2.1.1　人工神经元的正式定义 11
2.1.2　感知器学习规则 12
2.2　在Python中实现感知器学习算法 14
2.2.1　面向对象的感知器API 14
2.2.2　在鸢尾花数据集上训练感知器模型 16
2.3　自适应神经元和学习收敛 20
2.3.1　梯度下降为最小代价函数 21
2.3.2　用Python实现Adaline 22
2.3.3　通过调整特征大小改善梯度下降 25
2.3.4　大规模机器学习与随机梯度下降 27
2.4　小结 30
第3章　scikit-learn机器学习分类器一览 32
3.1　选择分类算法 32
3.2　了解scikit-learn软件库的第一步—训练感知器 32
3.3　基于逻辑回归的分类概率建模 37
3.3.1　逻辑回归的直觉与条件概率 37
3.3.2　学习逻辑代价函数的权重 39
3.3.3　把转换的Adaline用于逻辑回归算法 41
3.3.4　用scikit-learn训练逻辑回归模型 44
3.3.5　通过正则化解决过拟合问题 45
3.4　支持向量机的最大余量分类 47
3.4.1　最大边际的直觉 48
3.4.2　用松弛变量处理非线性可分 48
3.4.3　其他的scikit-learn 实现 50
3.5　用核支持向量机求解非线性问题 50
3.5.1　处理线性不可分数据的核方法 50
3.5.2　利用核技巧，发现高维空间的分离超平面 52
3.6　决策树学习 55
3.6.1　最大限度地获取信息—获得最大收益 55
3.6.2　构建决策树 58
3.6.3　通过随机森林组合多个决策树 61
3.7　K-近邻—一种懒惰的学习算法 63
3.8　小结 65
第4章　构建良好的训练集——预处理 66
4.1　处理缺失数据 66
4.1.1　识别数据中的缺失数值 66
4.1.2　删除缺失的数据 67
4.1.3　填补缺失的数据 68
4.1.4　了解scikit-learn评估器API 68
4.2　处理分类数据 69
4.2.1　名词特征和序数特征 69
4.2.2　映射序数特征 70
4.2.3　分类标签编码 70
4.2.4　为名词特征做热编码 71
4.3　分裂数据集为独立的训练集和测试集 73
4.4　把特征保持在同一尺度上 75
4.5　选择有意义的特征 76
4.5.1　L1和L2正则化对模型复杂度的惩罚 76
4.5.2　L2正则化的几何解释 77
4.5.3　L1正则化的稀疏解决方案 78
4.5.4　为序数特征选择算法 80
4.6　用随机森林评估特征的重要性 84
4.7　小结 87
第5章　通过降维压缩数据 88
5.1　用主成分分析实现无监督降维 88
5.1.1　主成分分析的主要步骤 88
5.1.2　逐步提取主成分 89
5.1.3　总方差和解释方差 91
5.1.4　特征变换 92
5.1.5　scikit-learn的主成分分析 93
5.2　基于线性判别分析的有监督数据压缩 96
5.2.1　主成分分析与线性判别分析 96
5.2.2　线性判别分析的内部逻辑 97
5.2.3　计算散布矩阵 97
5.2.4　在新的特征子空间选择线性判别式 99
5.2.5　将样本投影到新的特征空间 101
5.2.6　用scikit-learn实现的LDA 101
5.3　非线性映射的核主成分分析 102
5.3.1　核函数与核技巧 103
5.3.2　用Python实现核主成分分析 106
5.3.3　投影新的数据点 111
5.3.4　scikit-learn的核主成分分析 113
5.4　小结 114
第6章　模型评估和超参数调优的最佳实践 115
6.1　用管道方法简化工作流 115
6.1.1　加载威斯康星乳腺癌数据集 115
6.1.2　集成管道中的转换器和评估器 116
6.2　使用k折交叉验证评估模型的性能 118
6.2.1　抵抗方法 118
6.2.2　k折交叉验证 119
6.3　用学习和验证曲线调试算法 122
6.3.1　用学习曲线诊断偏差和方差问题 122
6.3.2　用验证曲线解决过拟合和欠拟合问题 124
6.4　通过网格搜索为机器学习模型调优 126
６.4.1　通过网格搜索为超参数调优 126
6.4.2　以嵌套式交叉验证来选择算法 127
6.5　比较不同的性能评估指标 128
6.5.1　含混矩阵分析 128
6.5.2　优化分类模型的准确度和召回率 129
6.5.3　绘制受试者操作特性图 130
6.5.4　多元分类评分指标 133
6.6　处理类的不平衡问题 133
6.7　小结 135
第7章　综合不同模型的组合学习 136
7.1　集成学习 136
7.2　采用多数票机制的集成分类器 139
7.2.1　实现基于多数票的简单分类器 139
7.2.2　用多数票原则进行预测 143
7.2.3　评估和优化集成分类器 145
7.3　套袋—基于导引样本构建分类器集成 149
7.3.1　套袋简介 150
7.3.2　应用套袋技术对葡萄酒数据集中的样本分类 151
7.4　通过自适应增强来利用弱学习者 153
7.4.1　增强是如何实现的 154
7.4.2　用scikit-learn实现AdaBoost 156
7.5　小结 158
第8章　应用机器学习于情感分析 159
8.1　为文本处理预备好IMDb电影评论数据 159
8.1.1　获取电影评论数据集 159
8.1.2　把电影评论数据预处理成更方便格式的数据 160
8.2　词袋模型介绍 161
8.2.1　把词转换成特征向量 161
8.2.2　通过词频逆反文档频率评估单词相关性 162
8.2.3　清洗文本数据 164
8.2.4　把文档处理为令牌 165
8.3　训练文档分类的逻辑回归模型 166
8.4　处理更大的数据集—在线算法和核心学习 168
8.5　具有潜在狄氏分配的主题建模 171
8.5.1　使用LDA分解文本文档 171
8.5.2　LDA与scikit-learn 172
8.6　小结 174
第9章　将机器学习模型嵌入网络应用 175
9.1　序列化拟合scikit-learn评估器 175
9.2　搭建SQLite数据库存储数据 177
9.3　用Flask开发网络应用 179
9.3.1　第一个Flask网络应用 179
9.3.2　表单验证与渲染 181
9.4　将电影评论分类器转换为网络应用 184
9.4.1　文件与文件夹—研究目录树 185
9.4.2　实现主应用app.py 186
9.4.3　建立评论表单 188
9.4.4　创建一个结果页面的模板 189
9.5　在面向公众的服务器上部署网络应用 190
9.5.1　创建PythonAnywhere账户 190
9.5.2　上传电影分类应用 191
9.5.3　更新电影分类器 191
9.6　小结 193
第10章　用回归分析预测连续目标变量 194
10.1　线性回归简介 194
10.1.1　简单线性回归 194
10.1.2　多元线性回归 195
10.2　探索住房数据集 196
10.2.1　加载住房数据 196
10.2.2　可视化数据集的重要特点 197
10.2.3　用关联矩阵查看关系 198
10.3　普通最小二乘线性回归模型的实现 200
10.3.1　用梯度下降方法求解回归参数 200
10.3.2　通过scikit-learn估计回归模型的系数 203
10.4　利用RANSAC拟合稳健的回归模型 205
10.5　评估线性回归模型的性能 206
10.6　用正则化方法进行回归 209
10.7　将线性回归模型转换为曲线—多项式回归 210
10.7.1　用scikit-learn增加多项式的项 210
10.7.2　为住房数据集中的非线性关系建模 211
10.8　用随机森林处理非线性关系 214
10.8.1　决策树回归 214
10.8.2　随机森林回归 215
10.9　小结 217
第11章　用聚类分析处理无标签数据 218
11.1　用k-均值进行相似性分组 218
11.1.1　scikit-learn的k-均值聚类 218
11.1.2　k-均值++—更聪明地设置初始聚类中心的方法 221
11.1.3　硬聚类与软聚类 222
11.1.4　用肘法求解最佳聚类数 223
11.1.5　通过轮廓图量化聚类质量 224
11.2　把集群组织成有层次的树 228
11.2.1　以自下而上的方式聚类 228
11.2.2　在距离矩阵上进行层次聚类 229
11.2.3　热度图附加树状图 232
11.2.4　scikit-learn凝聚聚类方法 233
11.3　通过DBSCAN定位高密度区域 233
11.4　小结 237
第12章　从零开始实现多层人工神经网络 238
12.1　用人工神经网络为复杂函数建模 238
12.1.1　单层神经网络扼要重述 239
12.1.2　介绍多层神经网络体系 240
12.1.3　利用正向传播激活神经网络 242
12.2　识别手写数字 243
12.2.1　获取MNIST数据集 243
12.2.2　实现一个多层感知器 247
12.3　训练人工神经网络 256
12.3.1　逻辑成本函数的计算 256
12.3.2　开发反向传播的直觉 257
12.3.3　通过反向传播训练神经网络 258
12.4　关于神经网络的收敛性 260
12.5　关于神经网络实现的最后几句话 261
12.6　小结 261
第13章　用TensorFlow并行训练神经网络 262
13.1　TensorFlow与模型训练的性能 262
13.1.1　什么是TensorFlow 263
13.1.2　如何学习TensorFlow 264
13.1.3　学习TensorFlow的第一步 264
13.1.4　使用阵列结构 266
13.1.5　用TensorFlow的底层API开发简单的模型 267
13.2　用TensorFlow的高级 API高效率地训练神经网络 270
13.2.1　用TensorFlow的Layers API构建多层神经网络 270
13.2.2　用Keras研发多层神经网络 274
13.3　多层网络激活函数的选择 277
13.3.1　逻辑函数回顾 278
13.3.2　在多元分类中调用softmax函数评估类别概率 279
13.3.3　利用双曲正切拓宽输出范围 280
13.3.4　修正线性单元激活函数 281
13.4　小结 282
第14章　深入探讨TensorFlow的工作原理 283
14.1　TensorFlow的主要功能 283
14.2　TensorFlow 的排序与张量 284
14.3　了解TensorFlow的计算图 285
14.4　TensorFlow中的占位符 287
14.4.1　定义占位符 287
14.4.2　为占位符提供数据 287
14.4.3　用batchsizes 为数据阵列定义占位符 288
14.5　TensorFlow中的变量 289
14.5.1　定义变量 289
14.5.2　初始化变量 290
14.5.3　变量范围 291
14.5.4　变量复用 292
14.6　建立回归模型 295
14.7　在TensorFlow计算图中用张量名执行对象 297
14.8　在TensorFlow中存储和恢复模型 298
14.9　把张量转换成多维数据阵列 300
14.10　利用控制流构图 303
14.11　用TensorBoard可视化图 305
14.12　小结 308
第15章　深度卷积神经网络图像识别 309
15.1　构建卷积神经网络的模块 309
15.1.1　理解CNN与学习特征的层次 309
15.1.2　执行离散卷积 310
15.1.3　子采样 316
15.2　拼装构建CNN 317
15.2.1　处理多个输入或者彩色频道 317
15.2.2　通过淘汰正则化神经网络 319
15.3　用TensorFlow实现深度卷积神经网络 321
15.3.1　多层CNN体系结构 321
15.3.2　加载和预处理数据 322
15.3.3　用TensorFlow的低级API实现CNN模型 323
15.3.4　用TensorFlow 的Layers API实现CNN 332
15.4　小结 336
第16章　用递归神经网络为序列数据建模 338
16.1　序列数据 338
16.1.1　序列数据建模—顺序很重要 338
16.1.2　表示序列 339
16.1.3　不同类别的序列建模 339
16.2　用于序列建模的RNN 340
16.2.1　理解RNN的结构和数据流 340
16.2.2　在RNN中计算激活值 341
16.2.3　长期交互学习的挑战 343
16.2.4　LSTM单元 343
16.3　用TensorFlow实现多层RNN序列建模 345
16.4　项目一：利用多层RNN对IMDb电影评论进行情感分析 345
16.4.1　准备数据 345
16.4.2　嵌入式 348
16.4.3　构建一个RNN模型 350
16.4.4　情感RNN类构造器 350
16.4.5　build方法 351
16.4.6　train方法 353
16.4.7　predict方法 354
16.4.8　创建SentimentRNN类的实例 355
16.4.9　训练与优化情感分析RNN模型 355
16.5　项目二：用TensorFlow实现字符级 RNN语言建模 356
16.5.1　准备数据 356
16.5.2　构建字符级RNN语言模型 359
16.5.3　构造器 359
16.5.4　build方法 360
16.5.5　train方法 362
16.5.6　sample方法 362
16.5.7　创建和训练CharRNN模型 364
16.5.8　处于取样状态的CharRNN模型 364
16.6　总结 365
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python机器学习（原书第2版)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Tensorflow与卷积神经网络从算法入门到项目实战
第一篇  TensorFlow基础篇
第1章  绪论	2
1.1  人工智能简介	2
1.2  卷积神经网络	3
1.3  搭建TensorFlow框架环境	5
1.3.1  安装Anaconda	5
1.3.2  安装TensorFlow	7
第2章  TensorFlow基础入门	9
2.1  第一个TensorFlow程序	9
2.1.1  TensorFlow中的hello world	9
2.1.2  TensorFlow中的图	11
2.1.3  静态图与动态图	14
2.2  初识Session	15
2.2.1  将Session对象关联Graph对象	15
2.2.2  Session参数配置	17
2.3  常量与变量	18
2.3.1  TensorFlow中的常量	18
2.3.2  TensorFlow中的变量	20
2.3.3  TensorFlow中的tf.placeholder	28
2.4  Tensor对象	29
2.4.1  什么是Tensor对象	29
2.4.2  Python对象转Tensor对象	31
2.4.3  Tensor对象转Python对象	32
2.4.4  SparseTensor对象	34
2.4.5  强制转换Tensor对象数据类型	35
2.5  Operation对象	37
2.5.1  什么是Operation对象	37
2.5.2  获取并执行Operation对象	37
2.6  TensorFlow流程控制	40
2.6.1  条件判断tf.cond与tf.where	40
2.6.2  TensorFlow比较判断	43
2.6.3  TensorFlow逻辑运算	44
2.6.4  循环tf.while_loop	45
2.7  TensorFlow位运算	48
2.7.1  且位运算	48
2.7.2  或位运算	49
2.7.3  异或位运算	50
2.7.4  取反位运算	51
2.8  TensorFlow字符串	52
2.8.1  字符串的定义与转换	53
2.8.2  字符串拆分	55
2.8.3  字符串拼接	56
第3章  高维Tensor对象的工具函数	58
3.1  重定义Shape	58
3.1.1  Reshape原理	58
3.1.2  函数tf.reshape	59
3.1.3  使用Python实现Reshape	60
3.2  维度交换函数	62
3.2.1  Transpose原理	62
3.2.2  函数tf.transpose	63
3.2.3  使用Python实现Transpose	64
3.3  维度扩充与消除	65
3.3.1  函数tf.expand_dims	65
3.3.2  函数tf.squeeze	66
3.4  Tensor对象裁剪	68
3.4.1  Tensor对象裁剪原理	68
3.4.2  函数tf.slice	69
3.5  Tensor对象拼接	70
3.5.1  Tensor对象拼接原理	70
3.5.2  函数tf.concat使用	71
3.6  tf.stack与tf.unstack	72
3.6.1  函数tf.stack的原理	72
3.6.2  函数tf.stack的使用	73
3.6.3  函数tf.unstack的使用	76
3.7  tf.argmax与tf.argmin	79
3.7.1  函数tf.argmax与tf.argmin的原理	79
3.7.2  函数tf.argmax与tf.argmin的使用	79
第二篇  卷积神经网络篇
第4章  前馈网络	83
4.1  卷积	83
4.1.1  卷积的原理	83
4.1.2  输出宽高与输入、Stride、卷积核及Padding之间的关系	90
4.1.3  空洞卷积	92
4.1.4  在TensorFlow中使用卷积	93
4.1.5  用Python语言实现卷积算法	95
4.2  反卷积	97
4.2.1  反卷积的原理	97
4.2.2  输出宽高与输入、Stride、反卷积核及Padding之间的关系	103
4.2.3  在TensorFlow中使用反卷积	105
4.2.4  用Python语言实现反卷积算法	110
4.3  Batch Normalization	113
4.3.1  Batch Normalization的原理	113
4.3.2  在TensorFlow中使用Batch Normalization	114
4.3.3  用Python语言实现Batch Normalization	122
4.3.4  在TensorFlow中使用Batch Normalization时的注意事项	123
4.4  Instance Normalization	125
4.4.1  Instance Normalization的原理	125
4.4.2  在TensorFlow中使用Instance Normalization	126
4.4.3  用Python语言实现Instance Normalization	130
4.5  全连接层	132
4.5.1  全连接层的原理	132
4.5.2  在TensorFlow中使用全连接层	133
4.5.3  用Python语言实现全连接层	134
4.6  激活函数	135
4.6.1  激活函数的作用	135
4.6.2  Sigmoid函数	136
4.6.3  Tanh函数	138
4.6.4  ReLU函数	140
4.7  池化层	142
4.7.1  池化层的原理	142
4.7.2  在TensorFlow中使用池化层	146
4.7.3  用Python语言实现池化层	150
4.8  Dropout	153
4.8.1  Dropout的作用	153
4.8.2  在TensorFlow中使用Dropout	154
第5章  常见网络	156
5.1  移动端定制卷积神经网络——MobileNet	156
5.1.1  MobileNet的原理与优势	156
5.1.2  在TensorFlow中实现MobileNet卷积	158
5.1.3  用Python语言实现Depthwise卷积	164
5.1.4  MobileNet完整的网络结构	167
5.1.5  MobileNet V2进一步裁剪加速	168
5.2  深度残差网络——ResNet	171
5.2.1  ResNet的结构与优势	171
5.2.2  在TensorFlow中实现ResNet	172
5.2.3  完整的ResNet网络结构	175
5.3  DenseNet	176
5.3.1  DenseNet的结构与优势	176
5.3.2  在TensorFlow中实现DenseNet	177
5.3.3  完整的DenseNet网络结构	180
第三篇  TensorFlow进阶篇
第6章  TensorFlow数据存取	183
6.1  队列	183
6.1.1  构建队列	183
6.1.2  Queue、QueueRunner及Coordinator	190
6.1.3  在队列中批量读取数据	194
6.2  文件存取	200
6.2.1  读取文本文件	200
6.2.2  读取定长字节文件	202
6.2.3  读取图片	205
6.3  从CSV文件中读取训练集	207
6.3.1  解析CSV格式文件	207
6.3.2  封装CSV文件读取类	209
6.4  从自定义文本格式文件中读取训练集	210
6.4.1  解析自定义文本格式文件	211
6.4.2  封装自定义文本格式文件读取类	212
6.5  TFRecord方式存取数据	213
6.5.1  将数据写入TFRecord文件	214
6.5.2  从TFRecord文件中读取数据	215
6.6  模型存取	217
6.6.1  存储模型	217
6.6.2  从checkpoint文件中加载模型	220
6.6.3  从meta文件中加载模型	222
6.6.4  将模型导出为单个pb文件	223
第7章  TensorFlow数据预处理	226
7.1  随机光照变化	226
7.1.1  随机饱和度变化	226
7.1.2  随机色相变化	228
7.1.3  随机对比度变化	230
7.1.4  随机亮度变化	232
7.1.5  随机伽玛变化	234
7.2  翻转、转置与旋转	237
7.2.1  随机上下、左右翻转	237
7.2.2  随机图像转置	239
7.2.3  随机旋转	241
7.3  裁剪与Resize	245
7.3.1  图像裁剪	245
7.3.2  图像Resize	249
7.3.3  其他Resize函数	254
7.4  用OpenCV对图像进行动态预处理	256
7.4.1  静态预处理与动态预处理	256
7.4.2  在TensorFlow中调用OpenCV	257
第8章  TensorFlow模型训练	260
8.1  反向传播中的优化器与学习率	260
8.1.1  Global Step与Epoch	260
8.1.2  梯度理论	260
8.1.3  使用学习率与梯度下降法求最优值	262
8.1.4  TensorFlow中的优化器	265
8.1.5  优化器中常用的函数	265
8.1.6  在TensorFlow中动态调整学习率	269
8.2  模型数据与参数名称映射	273
8.2.1  通过名称映射加载	273
8.2.2  以pickle文件为中介加载模型	275
8.3  冻结指定参数	277
8.3.1  从模型中加载部分参数	277
8.3.2  指定网络层参数不参与更新	278
8.3.3  两个学习率同时训练	280
8.4  TensorFlow中的命名空间	282
8.4.1  使用tf.variable_scope添加名称前缀	282
8.4.2  使用tf.name_scope添加名称前缀	284
8.4.3  tf.variable_scope与tf.name_scope的混合使用	285
8.5  TensorFlow多GPU训练	286
8.5.1  多GPU训练读取数据	286
8.5.2  平均梯度与参数更新	289
第9章  TensorBoard可视化工具	293
9.1  可视化静态图	293
9.1.1  图结构系列化并写入文件	293
9.1.2  启动TensorBoard	294
9.2  图像显示	296
9.2.1  系列化图像Tensor并写入文件	296
9.2.2  用TensorBoard查看图像	299
9.3  标量曲线	301
9.3.1  系列化标量Tensor并写入文件	301
9.3.2  用TensorBoard查看标量曲线	302
9.4  参数直方图	303
9.4.1  系列化参数Tensor并写入文件	303
9.4.2  用TensorBoard查看参数直方图	304
9.5  文本显示	306
9.5.1  系列化文本Tensor并写入文件	306
9.5.2  用TensorBoard查看文本	307
第四篇  卷积神经网络实战篇
第10章  中文手写字识别	310
10.1  网络结构及数据集	310
10.1.1  网络结构	310
10.1.2  数据集	311
10.2  代码实现	312
10.2.1  封装通用网络层	312
10.2.2  定义网络结构	314
10.2.3  数据读取	316
10.2.4  训练代码实现	318
10.3  模型训练	321
10.4  模型精度测试	321
10.4.1  精度测试	322
10.4.2  代码实现	322
第11章  移植模型到TensorFlow Serving端	324
11.1  模型转换	324
11.1.1  转换模型为TensorFlow Serving模型	324
11.1.2  代码实现	327
11.2  模型部署	329
11.2.1  搭建TensorFlow Serving环境	329
11.2.2  启动TensorFlow Serving服务	331
11.3  HTTP服务实现	333
11.3.1  使用gRPC调用TensorFlow Serving服务	333
11.3.2  实现HTTP服务	334
11.4  前端交互实现	336
11.4.1  界面布局	336
11.4.2  手写板实现	337
11.4.3  数据交互	339
11.4.4  流程测试	340
第12章  移植TensorFlow模型到Android端	341
12.1  交互界面	341
12.1.1  页面布局	341
12.1.2  实现手写板	342
12.2  使用TensorFlow Mobile库	346
12.2.1  模型转换	347
12.2.2  模型调用	347
12.2.3  模型测试	351
12.3  使用TensorFlow Lite库	354
12.3.1  模型转换	354
12.3.2  模型调用	355
12.3.3  模型测试	360
第13章  移植TensorFlow模型到iOS端	361
13.1  界面布局	361
13.1.1  页面布局	361
13.1.2  实现手写板	362
13.1.3  界面布局代码实现	366
13.2  TensorFlow 模型转CoreML模型	369
13.2.1  模型转换	369
13.2.2  分析模型对象的调用接口	370
13.3  模型调用	373
13.3.1  实现模型调用	373
13.3.2  模型测试	376
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Tensorflow与卷积神经网络从算法入门到项目实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习私房菜：跟着案例学TensorFlow
1 卷积神经网络与环境搭建 1
1.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1 卷积层 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2.2 修正线性单元 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2.3 池化层 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.4 全连接层 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.5 softmax 层 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2.6 LeNet-5 网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3 准备开发环境 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.1 Anaconda 环境搭建 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.2 安装 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.3.3 FloydHub 使用介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3.4 AWS 使用介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.4 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2 卷积神经网络实践：图像分类 27
2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.2 卷积神经网络项目实践：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . 27
2.2.1 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.2.2 网络模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.2.3 训练网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.3 卷积神经网络项目实践：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . 41
2.3.1 TensorFlow 2.0 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.3.2 CIFAR-100 分类网络的 TensorFlow 2.0 实现 . . . . . . . . . . . . . . . . . . 44
2.4 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3 彩票预测和生成古诗 61
3.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2 RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.3 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.4 嵌入矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.5 实现彩票预测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.5.1 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.5.2 构建神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.5.3 训练神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.5.4 分析网络训练情况 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
3.5.5 生成预测号码 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.6 文本生成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
3.7 生成古诗：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.7.1 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.7.2 构建网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
3.7.3 开始训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.7.4 生成古诗 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.8 自然语言处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.8.1 序列到序列 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.8.2 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.8.3 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.9 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4 个性化推荐系统 119
4.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
4.2 MovieLens 1M 数据集分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.2.1 下载数据集 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.2.2 用户数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.2.3 电影数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.2.4 评分数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.3 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.3.1 代码实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
4.3.2 加载数据并保存到本地 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.3.3 从本地读取数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
4.4 神经网络模型设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
4.5 文本卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.6 实现电影推荐：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.6.1 构建计算图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.6.2 训练网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
4.6.3 实现个性化推荐 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.7 实现电影推荐：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.7.1 构建模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.7.2 训练网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.7.3 实现个性化推荐 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.8 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
5 广告点击率预估：Kaggle 实战 170
5.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.2 下载数据集 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.3 数据字段的含义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
5.4 点击率预估的实现思路 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
5.4.1 梯度提升决策树 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
5.4.2 因子分解机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
5.4.3 场感知分解机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.4.4 网络模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
5.5 数据预处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
5.5.1 GBDT 的输入数据处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
5.5.2 FFM 的输入数据处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
5.5.3 DNN 的输入数据处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.5.4 数据预处理的实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.6 训练 FFM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
5.7 训练 GBDT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
5.8 用 LightGBM 的输出生成 FM 数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
5.9 训练 FM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
5.10 实现点击率预估：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.10.1 构建神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
5.10.2 训练网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
5.10.3 点击率预估 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
5.11 实现点击率预估：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . 237
5.12 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
6 人脸识别 246
6.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
6.2 人脸检测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
6.2.1 OpenCV 人脸检测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
6.2.2 dlib 人脸检测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
6.2.3 MTCNN 人脸检测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
6.3 提取人脸特征 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
6.3.1 使用 FaceNet 提取人脸特征 . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
6.3.2 使用 VGG 网络提取人脸特征 . . . . . . . . . . . . . . . . . . . . . . . . . . 265
6.3.3 使用 dlib 提取人脸特征 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
6.4 人脸特征的比较 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
6.5 从视频中找人的实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
6.6 视频找人的案例实践 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
6.7 人脸识别：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.8 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
7 AlphaZero / AlphaGo 实践：中国象棋 304
7.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
7.2 论文解析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
7.2.1 蒙特卡罗树搜索算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
7.2.2 神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
7.2.3 AlphaZero 论文解析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
7.3 实现中国象棋：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
7.3.1 中国象棋着法表示和 FEN 格式 . . . . . . . . . . . . . . . . . . . . . . . . . 317
7.3.2 输入特征的设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
7.3.3 实现神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
7.3.4 神经网络训练和预测 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
7.3.5 通过自我对弈训练神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
7.3.6 自我对弈 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
7.3.7 实现蒙特卡罗树搜索：异步方式 . . . . . . . . . . . . . . . . . . . . . . . . 340
7.3.8 训练和运行 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
7.4 实现中国象棋：基于 TensorFlow 2.0，多 GPU 版 . . . . . . . . . . . . . . . . . . . 354
7.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
8 汉字 OCR 365
8.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
8.2 分类网络实现汉字 OCR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
8.2.1 图片矫正 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
8.2.2 文本切割 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
8.2.3 汉字分类网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
8.3 端到端的汉字 OCR：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . . . . . 371
8.3.1 CNN 设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
8.3.2 双向 LSTM 设计 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
8.3.3 CTC 损失 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
8.3.4 端到端汉字 OCR 的网络训练 . . . . . . . . . . . . . . . . . . . . . . . . . . 388
8.4 汉字 OCR：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
8.4.1 CNN 的实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
8.4.2 双向 LSTM 的实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
8.4.3 OCR 网络的训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
8.5 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
9 强化学习：玩转 Flappy Bird 和超级马里奥 407
9.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
9.2 DQN 算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
9.3 实现 DQN 玩 Flappy Bird：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . . . . . 412
9.4 实现 DQN 玩 Flappy Bird：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . . . . . 417
9.5 使用 OpenAI Baselines 玩超级马里奥 . . . . . . . . . . . . . . . . . . . . . . . . . . 424
9.5.1 Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
9.5.2 自定义 Gym 环境 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
9.5.3 使用 Baselines 训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
9.5.4 使用训练好的智能体玩游戏 . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
9.5.5 开始训练马里奥游戏智能体 . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
9.6 具有好奇心的强化学习算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
9.7 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
10 生成对抗网络实践：人脸生成 445
10.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
10.2 GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
10.3 DCGAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
10.3.1 生成器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
10.3.2 判别器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
10.4 WGAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
10.5 WGAN-GP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
10.5.1 WGAN-GP 算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
10.5.2 训练 WGAN-GP 生成人脸：基于 TensorFlow 1.x . . . . . . . . . . . . . . . . 452
10.5.3 训练 WGAN-GP 生成人脸：基于 TensorFlow 2.0 . . . . . . . . . . . . . . . . 462
10.6 PG-GAN 和 TL-GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
10.7 本章小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习私房菜：跟着案例学TensorFlow
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从AI模型到智能机器人：基于Python与TensorFlow
第 1 章 AI 与面向对象 Python  1
1.1 AI 思维简史  2
1.2 Python 语言与 AI  2
1.3 布置 Python 开发环境  3
1.4 开始编写 Python 程序  6
1.5  面向对象（Object-Oriented）入门  10
1.5.1  对象（Object）  10
1.5.2  消息（Message）  10
1.5.3  事件（Event）  10
1.6  软件中的对象（Object）  11
1.6.1  抽象的目的  11
1.6.2  抽象表示  12
1.6.3  数据和函数  12
1.6.4  历史的足迹  12
1.7  对象与变量（Variable）  13
1.7.1  数据类型  13
1.7.2  变量即对象  14
1.8  对象与函数（Function）  17
1.8.1  函数的角色  17
1.8.2  事件驱动观念  18
1.9  自然界的分类  19
1.9.1  分类与抽象  19
1.9.2  对象与类  19
1.9.3  类的体系  20
1.10  软件的分类  21
1.10.1  类是数据类型  21
1.10.2  类的用途：描述对象的属性与行为  22
第 2 章 Python 的对象与类  24
2.1 OOP 入门  25
2.2  对象的概念  25
2.3  对象分类与组合  27
2.3.1  类的永恒性  27
2.3.2  将对象分门别类  27
2.3.3  对象的组合关系  28
2.4 AKO 抽象关系  30
2.5  对象行为与接口  36
2.5.1  接口入门  36
2.5.2  消息传递与对象行为 37
2.5.3  对象的运算行为  38
第 3 章 善用类  46
3.1  如何描述对象：善用类  47
3.2  如何创建软件对象  48
3.3  对象参考  49
3.4  构造函数  52
3.5  子类如何创建对象  54
第 4 章 对象的组合  58
4.1  认识 self 参考  59
4.2  建立对象的包含关系  60
4.3  self 参考值的妙用  64
4.4  包容多样化物件  71
4.5  集合对象  73
第 5 章 类的封装性  76
5.1  对象的封装性  77
5.2  类：创造对象的封装性  77
5.3  类的私有属性与函数  81
5.4  类级别的属性  89
5.5  类级别的函数  93
第 6 章 类的继承体系  96
6.1  继承的意义  97
6.2  建立类继承体系  98
6.3  函数覆写的意义  108
第 7 章 活用抽象类  111
7.1  抽象类与继承体系  112
7.2 Python 抽象类的表示法  112
7.2.1  一般具象类  112
7.2.2  抽象类  114
7.3  从“抽象类”衍生“具象类”  115
7.4  抽象类的妙用：默认行为  118
7.4.1 Python 默认行为的表示法  118
7.4.2  默认行为的意义  120
7.5  默认函数的妙用：反向调用  120
第 8 章 发挥“多态性”  127
8.1 “多态性”的意义  128
8.1.1  自然界的多态性  128
8.1.2  多态性物体  129
8.2  多态函数  130
8.3  可覆写函数  132
第 9 章 如何设计抽象类  138
9.1  抽象：抽出共同的现象  139
9.2  抽象的步骤  141
9.2.1  Step 1: 抽出名称、引数及内容都一致的函数  147
9.2.2  Step 2: 抽出名称相同、参数及内容有差异的函数  149
9.3  洞悉“变”与“不变”  152
9.4  着手设计抽象类  154
第 10 章 接口与抽象类  160
10.1  接口的意义  161
10.2  以 Python 抽象类来实现接口  162
10.3  接口设计实例一：并联电池对象  167
10.3.1  不理解原理但也能用  167
10.3.2  实现步骤  169
10.4  接口设计实例二：串联电池对象  172
10.4.1  基本设计  172
10.4.2  实现步骤  173
10.4.3  总结  176
10.5 接口设计实例三：Chain Of Responsibility 设计模式  177
第 11 章 不插电学 AI  183
11.1 “不插电学 AI”的意义  184
11.2 AlphaGo 的惊人学习能力  184
11.3  范例：一只老鼠的探索及学习  184
11.4  记录老鼠的探索选择及结果  186
11.5 老鼠当教练：训练 AI 机器人  188
11.5.1  以简单算数，让机器人表达智能  188
11.5.2  机器人智能的提升过程  189
11.5.3  一回生、两回熟  191
11.5.4  三回变高手  192
11.5.5  第四回合训练：迈向完美  194
11.5.6  重新检测一次  195
第 12 章 撰写单层 Perceptron 程序  198
12.1 开始“插电学 AI”：使用 Python  199
12.2 展开第#0 组数据的训练 200
12.3  进行更多组数据的训练  202
12.4  加入学习率  206
12.5 增添一个 Training 类  209
12.6 一个更详细的 Perceptron 代码  213
第 13 章 使用 TensorFlow 编程  225
13.1 TensorFlow 入门  226
13.2 安装 TensorFlow 环境  226
13.3 开始使用 TensorFlow  230
13.4 展开第 1 回合的训练：以老鼠教练为例  237
13.5 展开100 回合更周全的训练  240
13.6 设计 Perceptron 类  243
13.7 采用 TensorFlow 的损失函数  245
13.8 撰写多层 Perceptron 程序  248
第 14 章 TensorFlow 应用范例 251
14.1 mnist 手写数字识别范例  252
14.2  开始训练 NN 模型  256
14.3 改进 NN 模型：建立两层 Perceptron  260
14.4 改进 NN 模型：建立三层 Perceptron  263
14.5 撰写一个 MLP 类  265
第 15 章 如何导出 AI 模型  268
15.1 导出模型入门  269
15.2  机器人：像老鼠一样学习  270
15.3 基于 TensorFlow 建立 AI 模型  270
15.4 存入 Checkpoint 文件  272
15.5 读取 Checkpoint 文件  275
15.6  读取流图定义文件  277
15.7 导出模型：写入.pb 文件  280
15.8 导入模型，读取.pb 文件  284
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从AI模型到智能机器人：基于Python与TensorFlow
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow移动端机器学习实战
第1章  机器学习和TensorFlow简述	1
1.1  机器学习和TensorFlow的历史及发展现状	1
1.1.1  人工智能和机器学习	1
1.1.2  TensorFlow	3
1.1.3  TensorFlow Mobile	5
1.1.4  TensorFlow Lite	5
1.2  在移动设备上运行机器学习的应用	6
1.2.1  生态和现状	7
1.2.2  从移动优先到人工智能优先	8
1.2.3  人工智能的发展	9
1.2.4  在移动设备上进行机器学习的难点和挑战	9
1.2.5  TPU	10
1.3  机器学习框架	11
1.3.1  CAFFE2	11
1.3.2  Android NNAPI	12
1.3.3  CoreML	12
1.3.4  树莓派（Raspberry Pi）	13
第2章  构建开发环境	14
2.1  开发主机和设备的选择	14
2.2  在网络代理环境下开发	15
2.3  集成开发环境IDE	16
2.3.1  Android Studio	16
2.3.2  Visual Studio Code	16
2.3.3  其他IDE	18
2.4  构建工具Bazel	18
2.4.1  Bazel生成调试	19
2.4.2  Bazel Query命令	20
2.5  装载TensorFlow	20
2.6  文档	25
第3章  基于移动端的机器学习的开发方式和流程	26
3.1  开发方式和流程简介	26
3.2  使用TPU进行训练	28
3.3  设备端进行机器学习训练	35
3.4  使用TensorFlow Serving优化TensorFlow模型	41
3.4.1  训练和导出TensorFlow模型	42
3.4.2  使用标准TensorFlow ModelServer加载导出的模型	50
3.4.3  测试服务器	50
3.5  TensorFlow扩展（Extended）	54
第4章  构建TensorFlow Mobile	55
4.1  TensorFlow Mobile的历史	55
4.2  TensorFlow代码结构	55
4.3  构建及运行	61
4.3.1  代码的流程	67
4.3.2  代码的依赖性	68
4.3.3  性能和代码跟踪	69
第5章  用TensorFlow Mobile构建机器学习应用	71
5.1  准备工作	71
5.2  图像分类（Image Classification）	74
5.2.1  应用	74
5.2.2  模型	85
5.3  物体检测（Object Detection）	87
5.3.1  应用	87
5.3.2  模型	92
5.4  时尚渲染（Stylization）	95
5.4.1  应用	95
5.4.2  模型	96
5.5  声音识别（Speech Recognization）	96
5.5.1  应用	96
5.5.2  模型	99
第6章  TensorFlow Lite的架构	101
6.1  模型格式	102
6.1.1  Protocol Buffer	102
6.1.2  FlatBuffers	105
6.1.3  模型结构	112
6.1.4  转换器（Toco）	113
6.1.5  解析器（Interpreter）	119
6.2  底层结构和设计	123
6.2.1  设计目标	123
6.2.2  错误反馈	124
6.2.3  装载模型	125
6.2.4  运行模型	126
6.2.5  定制演算子（CUSTOM Ops）	128
6.2.6  定制内核	132
6.3  工具	133
6.3.1  图像标注（label_image）	133
6.3.2  最小集成（Minimal）	143
6.3.3  Graphviz	143
6.3.4  模型评效	148
第7章  用TensorFlow Lite构建机器学习应用	151
7.1  模型设计	151
7.1.1  使用预先训练的模型	151
7.1.2  重新训练	152
7.1.3  使用瓶颈（Bottleneck）	154
7.2  开发应用	158
7.2.1  程序接口	158
7.2.2  线程和性能	162
7.2.3  模型优化	163
7.3  TensorFlow Lite的应用	170
7.3.1  声音识别	173
7.3.2  图像识别	177
7.4  TensorFlow Lite使用GPU	178
7.4.1  GPU与CPU性能比较	178
7.4.2  开发GPU代理（Delegate）	178
7.5  训练模型	182
7.5.1  仿真器	183
7.5.2  构建执行文件	183
第8章  移动端的机器学习开发	186
8.1  其他设备的支持	186
8.1.1  在iOS上运行TensorFlow的应用	186
8.1.2  在树莓派上运行TensorFlow	189
8.2  设计和优化模型	190
8.2.1  模型大小	191
8.2.2  运行速度	192
8.2.3  可视化模型	196
8.2.4  线程	196
8.2.5  二进制文件大小	197
8.2.6  重新训练移动数据	197
8.2.7  优化模型加载	198
8.2.8  保护模型文件	198
8.2.9  量化计算	199
8.2.10  使用量化计算	202
8.3  设计机器学习应用程序要点	207
第9章  TensorFlow的硬件加速	209
9.1  神经网络接口	209
9.1.1  了解Neural Networks API运行时	210
9.1.2  Neural Networks API编程模型	211
9.1.3  NNAPI 实现的实例	213
9.2  硬件加速	222
9.2.1  高通网络处理器	223
9.2.2  华为HiAI Engine	229
9.2.3  简要比较	235
9.2.4  开放式神经网络交换格式	236
第10章  机器学习应用框架	237
10.1  ML Kit	237
10.1.1  面部识别（Face Detection）	242
10.1.2  文本识别	247
10.1.3  条形码识别	248
10.2  联合学习（Federated Learning）	248
第11章  基于移动设备的机器学习的未来	252
11.1  TensorFlow 2.0和路线图	252
11.1.1  更简单的开发模型	253
11.1.2  更可靠的跨平台的模型发布	254
11.1.3  TensorFlow Lite	254
11.1.4  TensorFlow 1.0 和TensorFlow 2.0的不同	255
11.2  人工智能的发展方向	255
11.2.1  提高人工智能的可解释性	255
11.2.2  贡献社会	256
11.2.3  改善生活	258
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>TensorFlow移动端机器学习实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实战GAN：TensorFlow与Keras生成对抗网络构建
序言	1
第1章  什么是生成对抗网络	7
简介	7
生成模型和判别模型	8
工作流程	8
工作原理	9
神经网络的“爱情故事”	10
工作流程	10
工作原理	11
深度神经网络	11
工作流程	11
工作原理	12
架构基础	13
工作流程	13
工作原理	14
基本构建块——生成器	15
工作流程	15
工作原理	15
基本构建块——判别器	16
工作流程	16
工作原理	17
基本构建块——损失函数	18
工作流程	18
工作原理	18
训练	20
工作流程	20
工作原理	20
以不同方式组织GAN	20
工作流程	21
工作原理	21
GAN的输出是什么	22
工作流程	22
工作原理	22
理解GAN架构的优点	24
工作流程	24
工作原理	25
练习	25
第2章  数据优先、环境和数据准备	27
简介	27
数据是否如此重要	27
准备工作	28
工作流程	28
工作原理	29
更多内容	29
搭建开发环境	29
准备工作	30
工作流程	30
更多内容	35
数据类型	35
准备工作	36
工作流程	36
工作原理	38
更多内容	40
数据预处理	41
准备工作	41
工作流程	41
工作原理	42
更多内容	45
异常数据	46
准备工作	46
工作流程	46
更多内容	49
平衡数据	49
准备工作	49
工作流程	49
更多内容	53
数据强化	54
准备工作	54
工作流程	55
工作原理	56
更多内容	57
练习	58
第3章  用100行代码实现第一个GAN	59
简介	59
从理论到实践——一个简单例子	59
准备工作	60
工作流程	60
参考内容	62
使用Keras和TensorFlow构建神经网络	62
准备工作	63
工作流程	63
参考内容	66
解释你的第一个GAN组件——判别器	66
准备工作	67
工作流程	67
解释你的第二个GAN组件——生成器	71
准备工作	71
工作流程	71
组合GAN组件	75
准备工作	76
工作流程	76
训练你的第一个GAN	78
准备工作	78
工作流程	78
训练模型并理解GAN的输出	84
准备工作	84
工作流程	84
工作原理	86
练习	87
第4章  使用DCGAN创造新的室外结构	89
简介	89
什么是DCGAN？一个简单的伪代码样例	89
准备工作	90
工作流程	90
参考内容	93
工具——是否需要特殊的工具	93
准备工作	93
工作流程	94
更多内容	97
参考内容	97
解析数据——数据是否独特	97
准备工作	97
工作流程	98
代码实现——生成器	100
准备工作	100
工作流程	100
参考内容	103
代码实现——判别器	103
准备工作	104
工作流程	104
参考内容	107
训练	107
准备工作	107
工作流程	107
评估——如何判断它是否有效	114
准备工作	115
工作原理	115
调整参数优化性能	116
工作流程	116
练习	118
第5章  Pix2Pix图像转换	119
简介	119
使用伪代码介绍Pix2Pix	119
准备工作	120
工作流程	120
数据集解析	122
准备工作	122
工作流程	123
代码实现——生成器	124
准备工作	124
工作流程	125
代码实现——GAN	127
准备工作	127
工作流程	128
代码实现——判别器	129
准备工作	129
工作流程	129
训练	131
准备工作	131
工作流程	132
练习	139
第6章  使用CycleGAN进行图像风格转换	141
简介	141
伪代码——工作原理	141
准备工作	142
工作流程	142
解析CycleGAN数据集	144
准备工作	144
工作流程	145
代码实现——生成器	147
准备工作	147
工作流程	148
代码实现——判别器	150
准备工作	150
工作流程	151
代码实现——GAN	153
准备工作	153
工作流程	154
训练	155
准备工作	155
工作流程	156
练习	162
第7章  利用SimGAN使用模拟图像制作具有真实感的眼球图片	163
简介	163
SimGAN架构的工作原理	163
准备工作	164
工作流程	164
伪代码——工作原理	165
准备工作	165
工作流程	165
如何使用训练数据	166
准备工作	166
工作流程	166
代码实现——损失函数	169
准备工作	169
工作流程	169
代码实现——生成器	170
准备工作	170
工作流程	171
代码实现——判别器	173
准备工作	173
工作流程	174
代码实现——GAN	176
准备工作	176
工作流程	177
训练SimGAN	178
准备工作	178
工作流程	179
练习	183
第8章  使用GAN从图像生成3D模型	185
简介	185
使用GAN生成3D模型	185
准备工作	186
工作流程	186
环境准备	188
准备工作	189
工作流程	189
对2D数据进行编码并匹配3D对象	190
准备工作	191
工作流程	191
代码实现——生成器	193
准备工作	193
工作流程	194
代码实现——判别器	196
准备工作	196
工作流程	197
代码实现——GAN	199
准备工作	199
工作流程	199
训练模型	200
准备工作	201
工作流程	201
练习	208
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实战GAN：TensorFlow与Keras生成对抗网络构建
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>详解深度学习：基于TensorFlow和Keras学习RNN
第1章　数学准备　　001
1.1 偏微分　　001
1.1.1　导函数和偏导函数　　001
1.1.2　微分系数与偏微分系数　　003
1.1.3　偏微分的基本公式　　006
1.1.4　复合函数的偏微分　　007
1.1.5　拓展全微分　　009
1.2 线性代数　　011
1.2.1　向量　　011
1.2.1.1　向量的基础知识　　011
1.2.1.2　向量的和与标量倍数　　011
1.2.1.3　向量的内积　　012
1.2.2　矩阵　　013
1.2.2.1　矩阵的基础知识　　013
1.2.2.2　矩阵的和与标量倍数　　014
1.2.2.3　矩阵的乘积　　014
1.2.2.4　正则矩阵与逆矩阵　　016
1.2.2.5　转置矩阵　　017
1.3 小结　　018
第2章　Python准备　　019
2.1 Python　2和Python　3　　020
2.2 Anaconda发行版　　021
2.3 Python的基础知识　　025
2.3.1　Python程序的执行　　025
2.3.2　数据类型　　026
2.3.2.1　类型是什么　　026
2.3.2.2　字符串类型　　027
2.3.2.3　数值类型　　028
2.3.2.4　布尔类型　　030
2.3.3　变量　　031
2.3.3.1　变量是什么　　031
2.3.3.2　变量与类型　　032
2.3.4　数据结构　　033
2.3.4.1　列表　　033
2.3.4.2　字典　　034
2.3.5　运算　　035
2.3.5.1　运算符与操作数　　035
2.3.5.2　算术运算的运算符　　036
2.3.5.3　赋值运算符　　036
2.3.6　基本结构　　038
2.3.6.1　if语句　　038
2.3.6.2　while语句　　039
2.3.6.3　for语句　　041
2.3.7　函数　　043
2.3.8　类　　045
2.3.9　库　　048
2.4 NumPy　　049
2.4.1　NumPy数组　　049
2.4.2　使用NumPy进行向量和矩阵的计算　　051
2.4.3　数组和多维数组的生成　　053
2.4.4　切片　　054
2.4.5　广播　　056
2.5 面向深度学习的库　　058
2.5.1　TensorFlow　　058
2.5.2　Keras　　059
2.5.3　参考Theano　　060
2.6 小结　　063
第3章　神经网络　　065
3.1 什么是神经网络　　065
3.1.1　脑和神经元　　065
3.1.2　深度学习和神经网络　　066
3.2 作为电路的神经网络　　067
3.2.1　简单的模型化　　067
3.2.2　逻辑电路　　069
3.2.2.1　逻辑门　　069
3.2.2.2　与门　　069
3.2.2.3　或门　　072
3.2.2.4　非门　　074
3.3 简单感知机　　075
3.3.1　模型化　　075
3.3.2　实现　　077
3.4 逻辑回归　　081
3.4.1　阶跃函数与sigmoid函数　　081
3.4.2　模型化　　082
3.4.2.1　似然函数与交叉熵误差函数　　082
3.4.2.2　梯度下降法　　084
3.4.2.3　随机梯度下降法与小批量梯度下降法　　085
3.4.3　实现　　086
3.4.3.1　使用TensorFlow的实现　　086
3.4.3.2　使用Keras的实现　　092
3.4.4　拓展sigmoid函数与概率密度函数、累积分布函数　　096
3.4.5　拓展梯度下降法和局部最优解　　099
3.5 多分类逻辑回归　　101
3.5.1　softmax函数　　101
3.5.2　模型化　　102
3.5.3　实现　　106
3.5.3.1　使用TensorFlow的实现　　106
3.5.3.2　使用Keras的实现　　110
3.6 多层感知机　　111
3.6.1　非线性分类　　111
3.6.1.1　异或门　　111
3.6.1.2　逻辑门的组合　　113
3.6.2　模型化　　115
3.6.3　实现　　119
3.6.3.1　使用TensorFlow的实现　　119
3.6.3.2　使用Keras的实现　　122
3.7 模型的评估　　123
3.7.1　从分类到预测　　123
3.7.2　预测的评估　　124
3.7.3　简单的实验　　126
3.8 小结　　131
第4章　深度神经网络　　133
4.1 进入深度学习之前的准备　　133
4.2 训练过程中的问题　　138
4.2.1　梯度消失问题　　138
4.2.2　过拟合问题　　141
4.3 训练的高效化　　142
4.3.1　激活函数　　143
4.3.1.1　双曲正切函数　　143
4.3.1.2　ReLU　　145
4.3.1.3　Leaky ReLU　　147
4.3.1.4　Parametric ReLU　　149
4.3.2　Dropout　　152
4.4 代码的设计　　157
4.4.1　基本设计　　157
4.4.1.1　使用TensorFlow的实现　　157
4.4.1.2　使用Keras的实现　　160
4.4.1.3　拓展对TensorFlow模型进行类封装　　161
4.4.2　训练的可视化　　166
4.4.2.1　使用TensorFlow的实现　　167
4.4.2.2　使用Keras的实现　　172
4.5 高级技术　　176
4.5.1　数据的正则化与权重的初始化　　176
4.5.2　学习率的设置　　179
4.5.2.1　动量　　179
4.5.2.2　Nesterov动量　　180
4.5.2.3　Adagrad　　181
4.5.2.4　Adadelta　　182
4.5.2.5　RMSprop　　184
4.5.2.6　Adam　　185
4.5.3　早停法　　187
4.5.4　Batch Normalization　　190
4.6 小结　　195
第5章　循环神经网络　　197
5.1 基本概念　　197
5.1.1　时间序列数据　　197
5.1.2　过去的隐藏层　　199
5.1.3　基于时间的反向传播算法　　202
5.1.4　实现　　204
5.1.4.1　准备时间序列数据　　205
5.1.4.2　使用TensorFlow的实现　　207
5.1.4.3　使用Keras的实现　　214
5.2 LSTM　　215
5.2.1　LSTM 块　　215
5.2.2　CEC、输入门和输出门　　217
5.2.2.1　稳态误差　　217
5.2.2.2　输入权重冲突和输出权重冲突　　219
5.2.3　遗忘门　　220
5.2.4　窥视孔连接　　222
5.2.5　模型化　　223
5.2.6　实现　　227
5.2.7　长期依赖信息的训练评估——Adding Problem　　229
5.3 GRU　　232
5.3.1　模型化　　232
5.3.2　实现　　233
5.4 小结　　235
第6章　循环神经网络的应用　　237
6.1 双向循环神经网络　　237
6.1.1　未来的隐藏层　　237
6.1.2　前向、后向传播　　239
6.1.3　MNIST的预测　　241
6.1.3.1　转换为时间序列数据　　241
6.1.3.2　使用TensorFlow的实现　　242
6.1.3.3　使用Keras的实现　　245
6.2 循环神经网络编码器- 解码器　　246
6.2.1　序列到序列模型　　246
6.2.2　简单的问答系统　　247
6.2.2.1　设置问题——加法的训练　　247
6.2.2.2　数据的准备　　248
6.2.2.3　使用TensorFlow的实现　　251
6.2.2.4　使用Keras的实现　　260
6.3 注意力模型　　261
6.3.1　时间的权重　　261
6.3.2　LSTM中的注意力机制　　263
6.4 记忆网络　　265
6.4.1　记忆外部化　　265
6.4.2　应用于问答系统　　266
6.4.2.1　bAbi任务　　266
6.4.2.2　模型化　　267
6.4.3　实现　　269
6.4.3.1　数据的准备　　269
6.4.3.2　使用TensorFlow的实现　　272
6.5 小结　　276
附录　　279
A.1 模型的保存和读取　　279
A.1.1　使用TensorFlow时的处理　　279
A.1.2　使用Keras时的处理　　284
A.2 TensorBoard　　285
A.3 tf.contrib.learn　　292
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>详解深度学习：基于TensorFlow和Keras学习RNN
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>走向TensorFlow 2.0：深度学习应用编程快速入门
目录
第1章  Python基础编程入门	1
1.1  Python的历史	1
1.1.1  Python版本的演进	1
1.1.2  Python的工程应用情况	2
1.2  Python的基本数据类型	2
1.3  Python数据处理工具之Pandas	6
1.3.1  数据读取和存储	7
1.3.2  数据查看和选取	8
1.3.3  数据处理	11
1.4  Python图像处理工具之PIL	14
1.4.1  PIL简介	14
1.4.2  PIL接口详解	14
1.4.3  PIL图像处理实践	18
第2章  TensorFlow 2.0快速入门	21
2.1  TensorFlow 2.0简介	21
2.2  TensorFlow 2.0环境搭建	22
2.2.1  CPU环境搭建	22
2.2.2  基于Docker的GPU环境搭建	23
2.3  TensorFlow 2.0基础知识	25
2.3.1  TensorFlow 2.0 Eager模式简介	25
2.3.2  TensorFlow 2.0 AutoGraph简介	26
2.3.3  TensorFlow 2.0低阶API基础编程	26
2.4  TensorFlow 2.0高阶API（tf.keras）	32
2.4.1  tf.keras高阶API概览	32
2.4.2  tf.keras高阶API编程	34
第3章  基于CNN的图像识别应用编程实践	36
3.1  CNN相关基础理论	36
3.1.1  卷积神经网络概述	36
3.1.2  卷积神经网络结构	36
3.1.3  卷积神经网络三大核心概念	38
3.2  TensorFlow 2.0 API详解	38
3.2.1  tf.keras.Sequential	39
3.2.2  tf.keras.layers.Conv2D	41
3.2.3  tf.keras.layers.MaxPool2D	42
3.2.4  tf.keras.layers.Flatten与tf.keras.layer.Dense	42
3.2.5  tf.keras.layers.Dropout	43
3.2.6  tf.keras.optimizers.Adam	43
3.3  项目工程结构设计	44
3.4  项目实现代码详解	44
3.4.1  工具类实现	45
3.4.2  cnnModel实现	46
3.4.3  执行器实现	48
3.4.4  Web应用实现	52
第4章  基于Seq2Seq的中文聊天机器人编程实践	55
4.1  NLP基础理论知识	55
4.1.1  语言模型	55
4.1.2  循环神经网络	57
4.1.3  Seq2Seq模型	59
4.2  TensorFlow 2.0 API详解	61
4.2.1  tf.keras.preprocessing.text.Tokenizer	61
4.2.2  tf.keras.preprocessing.sequence.pad_sequences	62
4.2.3  tf.data.Dataset.from_tensor_slices	63
4.2.4  tf.keras.layers.Embedding	63
4.2.5  tf.keras.layers.GRU	63
4.2.6  tf.keras.layers.Dense	65
4.2.7  tf.expand_dims	65
4.2.8  tf.keras.optimizers.Adam	65
4.2.9  tf.keras.losses.SparseCategoricalCrossentropy	66
4.2.10  tf.math.logical_not	66
4.2.11  tf.concat	66
4.2.12  tf.bitcast	67
4.3  项目工程结构设计	67
4.4  项目实现代码详解	68
4.4.1  工具类实现	68
4.4.2  data_util实现	69
4.4.3  seq2seqModel实现	71
4.4.4  执行器实现	77
4.4.5  Web应用实现	83
第5章  基于CycleGAN的图像风格迁移应用编程实践	85
5.1  GAN基础理论	85
5.1.1  GAN的基本思想	85
5.1.2  GAN的基本工作机制	86
5.1.3  GAN的常见变种及应用场景	86
5.2  CycleGAN的算法原理	88
5.3  TensorFlow 2.0 API详解	88
5.3.1  tf.keras.Sequential	88
5.3.2  tf.keras.Input	91
5.3.3  tf.keras.layers.BatchNormalization	91
5.3.4  tf.keras.layers.Dropout	92
5.3.5  tf.keras.layers.Concatenate	93
5.3.6  tf.keras.layers.LeakyReLU	93
5.3.7  tf.keras.layers.UpSampling2D	93
5.3.8  tf.keras.layers.Conv2D	93
5.3.9  tf.optimizers.Adam	94
5.4  项目工程结构设计	95
5.5  项目实现代码详解	96
5.5.1  工具类实现	96
5.5.2  CycleganModel实现	100
5.5.3  执行器实现	105
5.5.4  Web应用实现	109
第6章  基于Transformer的文本情感分析编程实践	111
6.1  Transformer相关理论知识	111
6.1.1  Transformer基本结构	111
6.1.2  注意力机制	112
6.1.3  位置编码	116
6.2  TensorFlow 2.0 API详解	117
6.2.1  tf.keras.preprocessing.text.Tokenizer	117
6.2.2  tf.keras.preprocessing.sequence.pad_sequences	118
6.2.3  tf.data.Dataset.from_tensor_slices	118
6.2.4  tf.keras.layers.Embedding	118
6.2.5  tf.keras.layers.Dense	119
6.2.6  tf.keras.optimizers.Adam	119
6.2.7  tf.optimizers.schedules.LearningRateSchedule	120
6.2.8  tf.keras.layers.Conv1D	120
6.2.9  tf.nn.moments	121
6.3  项目工程结构设计	121
6.4  项目实现代码详解	122
6.4.1  工具类实现	122
6.4.2  data_util实现	124
6.4.3  textClassiferMode实现	128
6.4.4  执行器实现	138
6.4.5  Web应用实现	142
第7章  基于TensorFlow Serving的模型部署实践	144
7.1  TensorFlow Serving框架简介	144
7.1.1  Servable	145
7.1.2  Source	145
7.1.3  Loader	145
7.1.4  Manager	145
7.2  TensorFlow Serving环境搭建	146
7.2.1  基于Docker搭建TensorFlow Serving环境	146
7.2.2  基于Ubuntu 16.04搭建TensorFlow Serving环境	146
7.3  API详解	147
7.3.1  tf.keras.models.load_model	147
7.3.2  tf.keras.experimental.export_saved_model	147
7.3.3  tf.keras.backend.set_learning_phase	148
7.4  项目工程结构设计	148
7.5  项目实现代码详解	149
7.5.1  工具类实现	149
7.5.2  模型文件导出模块实现	150
7.5.3  模型文件部署模块实现	150
7.5.4  Web应用模块实现	152
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>走向TensorFlow 2.0：深度学习应用编程快速入门
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习
1 深度学习简介1
1.1 人工智能、机器学习和深度学习  1
1.1.1 引言  1
1.1.2 人工智能、机器学习和深度学习三者的关系  2
1.2 神经网络  3
1.2.1 感知器  3
1.2.2 激活函数  5
1.2.3 损失函数  8
1.2.4 梯度下降和随机梯度下降  8
1.2.5 反向传播算法简述  11
1.2.6 其他神经网络  12
1.3 学习方法建议  13
1.3.1 网络资源  13
1.3.2 TensorFlow 官方深度学习教程  14
1.3.3 开源社区  15
1.4 TensorLayer  15
1.4.1 深度学习框架概况  15
1.4.2 TensorLayer 概括  16
1.4.3 实验环境配置  17
2 多层感知器19
2.1 McCulloch-Pitts 神经元模型  19
2.1.1 人工神经网络到底能干什么？到底在干什么  21
2.1.2 什么是激活函数？什么是偏值  22
2.2 感知器  23
2.2.1 什么是线性分类器  24
2.2.2 线性分类器有什么优缺点  26
2.2.3 感知器实例和异或问题（XOR 问题）  26
2.3 多层感知器  30
2.4 实现手写数字分类  32
2.5 过拟合  40
2.5.1 什么是过拟合  40
2.5.2 Dropout  41
2.5.3 批规范化  42
2.5.4 L1、L2 和其他正则化方法  42
2.5.5 Lp 正则化的图形化解释  44
2.6 再实现手写数字分类  46
2.6.1 数据迭代器  46
2.6.2 通过all_drop 启动与关闭Dropout  47
2.6.3 通过参数共享实现训练测试切换  50
3 自编码器54
3.1 稀疏性  54
3.2 稀疏自编码器  56
3.3 实现手写数字特征提取  59
3.4 降噪自编码器  65
3.5 再实现手写数字特征提取  68
3.6 堆栈式自编码器及其实现  72
4 卷积神经网络80
4.1 卷积原理  80
4.1.1 卷积操作  81
4.1.2 张量  84
4.1.3 卷积层  85
4.1.4 池化层  87
4.1.5 全连接层  89
4.2 经典任务  90
4.2.1 图像分类  90
4.2.2 目标检测  91
4.2.3 语义分割  94
4.2.4 实例分割  94
4.3 经典卷积网络  95
4.3.1 LeNet  95
4.3.2 AlexNet  96
4.3.3 VGGNet  96
4.3.4 GoogLeNet  98
4.3.5 ResNet  99
4.4 实现手写数字分类  100
4.5 数据增强与规范化  104
4.5.1 数据增强  104
4.5.2 批规范化  106
4.5.3 局部响应归一化  107
4.6 实现CIFAR10 分类  108
4.6.1 方法1：tl.prepro 做数据增强  108
4.6.2 方法2：TFRecord 做数据增强  114
4.7 反卷积神经网络  120
5 词的向量表达121
5.1 目的与原理  121
5.2 Word2Vec  124
5.2.1 简介  124
5.2.2 Continuous Bag-Of-Words（CBOW）模型  124
5.2.3 Skip Gram（SG）模型  129
5.2.4 Hierarchical Softmax  132
5.2.5 Negative Sampling  135
5.3 实现Word2Vec  136
5.3.1 简介  136
5.3.2 实现  136
5.4 重载预训练矩阵  144
6 递归神经网络148
6.1 为什么需要它  148
6.2 不同的RNNs  151
6.2.1 简单递归网络  151
6.2.2 回音网络  152
6.3 长短期记忆  153
6.3.1 LSTM 概括  153
6.3.2 LSTM 详解  157
6.3.3 LSTM 变种  159
6.4 实现生成句子  160
6.4.1 模型简介  160
6.4.2 数据迭代  163
6.4.3 损失函数和更新公式  164
6.4.4 生成句子及Top K 采样  167
6.4.5 接下来还可以做什么  169
7 深度增强学习171
7.1 增强学习  172
7.1.1 概述  172
7.1.2 基于价值的增强学习  173
7.1.3 基于策略的增强学习  176
7.1.4 基于模型的增强学习  177
7.2 深度增强学习  179
7.2.1 深度Q 学习  179
7.2.2 深度策略网络  181
7.3 更多参考资料  187
7.3.1 书籍  187
7.3.2 在线课程  187
8 生成对抗网络188
8.1 何为生成对抗网络  189
8.2 深度卷积对抗生成网络  190
8.3 实现人脸生成  191
8.4 还能做什么  198
9 高级实现技巧202
9.1 与其他框架对接  202
9.1.1 无参数层  203
9.1.2 有参数层  203
9.2 自定义层  204
9.2.1 无参数层  204
9.2.2 有参数层  205
9.3 建立词汇表  207
9.4 补零与序列长度  209
9.5 动态递归神经网络  210
9.6 实用小技巧  211
9.6.1 屏蔽显示  211
9.6.2 参数名字前缀  212
9.6.3 获取特定参数  213
9.6.4 获取特定层输出  213
10 实例一：使用预训练卷积网络214
10.1 高维特征表达  214
10.2 VGG 网络  215
10.3 连接TF-Slim  221
11 实例二：图像语义分割及其医学图像应用225
11.1 图像语义分割概述  225
11.1.1 传统图像分割算法简介  227
11.1.2 损失函数与评估指标  229
11.2 医学图像分割概述  230
11.3 全卷积神经网络和U-Net 网络结构  232
11.4 医学图像应用：实现脑部肿瘤分割  234
11.4.1 数据与数据增强  235
11.4.2 U-Net 网络  238
11.4.3 损失函数  239
11.4.4 开始训练  241
12 实例三：由文本生成图像244
12.1 条件生成对抗网络之GAN-CLS  245
12.2 实现句子生成花朵图片  246
13 实例四：超高分辨率复原260
13.1 什么是超高分辨率复原  260
13.2 网络结构  261
13.3 联合损失函数  264
13.4 训练网络  269
13.5 使用测试  277
14 实例五：文本反垃圾280
14.1 任务场景  280
14.2 网络结构  281
14.3 词的向量表示  282
14.4 Dynamic RNN 分类器  283
14.5 训练网络  284
14.5.1 训练词向量  284
14.5.2 文本的表示  290
14.5.3 训练分类器  291
14.5.4 模型导出  296
14.6 TensorFlow Serving 部署  299
14.7 客户端调用  301
14.8 其他常用方法  306
中英对照表及其缩写309
参考文献316
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度学习
