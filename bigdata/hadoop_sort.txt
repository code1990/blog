>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>OpenStack开源云王者归来 : 云计算、虚拟化、Nova、Swift、Quantum和Hadoop
第1篇 基础篇
第1章 OpenStack概述
1.1 云计算简介
1.1.1 什么是云计算
1.1.2 什么是云存储
1.1.3 私有云与公有云
1.2 为什么使用云计算
1.2.1 方案1：简单的服务部署
1.2.2 方案2：分布式服务部署
1.2.3 方案3：基于虚拟化的服务部署
1.2.4 方案4：云计算的解决方案
1.3 OpenStack架构
1.3.1 OpenStack与云计算
1.3.2 OpenStack发展与现状
1.3.3 OpenStack优势
1.3.4 OpenStack学习建议
1.4 OpenStack各个组件及功能
1.4.1 虚拟机管理系统Nova
1.4.2 磁盘存储系统Glance与Swift
1.4.3 虚拟网络管理Quantum
1.4.4 OpenStack三大组件
1.5 小结
第2章 虚拟化技术
2.1 虚拟化技术简介
2.1.1 KVM
2.1.2 Xen
2.1.3 Libvirt
2.2 安装Libvirt虚拟化工具
2.2.1 安装KVM
2.2.2 安装Libvirt
2.3 虚拟机配置文件详解
2.3.1 xml描述hypervisor
2.3.2 虚拟机整体信息
2.3.3 系统信息
2.3.4 硬件资源特性
2.3.5 突发事件处理
2.3.6 raw格式image
2.3.7 qcow2格式image
2.3.8 格式的选择
2.3.9 多个image
2.3.10 虚拟光盘
2.3.11 虚拟网络
2.3.12 vnc配置
2.4 制作image
2.4.1 virt-manager创建image
2.4.2 virsh命令创建image
2.5 快速启动虚拟机
2.5.1 手动安装
2.5.2 直接复制
2.5.3 qcow2快速创建
2.5.4 修改qcow2 image
2.5.5 大批量创建虚拟机
2.6 虚拟机桌面显示
2.6.1 准备工作
2.6.2 创建Windows 7 Image
2.6.3 创建Windows 7虚拟机
2.6.4 spice桌面显示
2.7 常见错误与分析
2.8 小结
2.8.1 常用的virsh命令
2.8.2 磁盘快照管理
第2篇 安装篇
第3章 安装Keystone安全认证服务
3.1 Keystone简介
3.2 搭建局域网源
3.2.1 局域网apt-get源搭建方法
3.2.2 局域网python源搭建方法
3.2.3 Ubuntu-12.10局域网源
……
第4章 安装Swift存储服务
第5章 安装Glance镜像服务
第6章 安装Quantum虚拟网络服务
第7章 安装Cinder块存储服务
第8章 安装Nova虚拟机管理系统
第9章 安装Dashboard Web界面
第10章 OpenStack部署示例
第3篇 剖析篇
第11章 OpenStack服务分析
第12章 Keystone的安全认证
第13章 Swift存储服务
第14章 Quantum虚拟网络
第15章 Nova框架
第16章 Nova Compute服务
第4篇 扩展篇
第17章 从OpenStack到云应用
第18章 基于Nova的扩展
第19章 添加自定义组件
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>OpenStack开源云王者归来 : 云计算、虚拟化、Nova、Swift、Quantum和Hadoop
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop MapReduce实战手册
第1章 搭建Hadoop并在集群中运行 1
1.1 简介 1
1.2 在你的机器上安装Hadoop 2
1.3 写WordCountMapReduce示例程序，打包并使用独立的Hadoop运行它 3
1.4 给WordCount MapReduce程序增加combiner步骤 8
1.5 安装HDFS 9
1.6 使用HDFS监控UI 14
1.7 HDFS的基本命令行文件操作 15
1.8 在分布式集群环境中设置Hadoop 17
1.9 在分布式集群环境中运行WordCount程序 22
1.10 使用MapReduce监控UI 24
第2章 HDFS进阶 26
2.1 简介 26
2.2 HDFS基准测试 27
2.3 添加一个新的DataNode 28
2.4 DataNode下架 30
2.5 使用多个磁盘/卷以及限制HDFS的磁盘使用情况 32
2.6 设置HDFS块大小 33
2.7 设置文件冗余因子 34
2.8 使用HDFS的Java API 35
2.9 使用HDFS的C API（libhdfs） 40
2.10 挂载HDFS（Fuse-DFS） 45
2.11 在HDFS中合并文件 48
第3章 高级Hadoop MapReduce运维 49
3.1 简介 49
3.2 调优集群部署的Hadoop配置 49
3.3 运行基准测试来验证Hadoop的安装 52
3.4 复用Java虚拟机以提高性能 54
3.5 容错和推测执行 54
3.6 调试脚本——分析任务失败 55
3.7 设置失败百分比以及跳过不良记录 59
3.8 共享用户的Hadoop集群——使用公平调度器和其他调度器 61
3.9 Hadoop的安全性——整合使用Kerberos 62
3.10 使用Hadoop的工具接口 69
第4章 开发复杂的Hadoop MapReduce应用程序 72
4.1 简介 72
4.2 选择合适的Hadoop数据类型 73
4.3 实现自定义的Hadoop Writable数据类型 75
4.4 实现自定义Hadoop key类型 79
4.5 从mapper中输出不同值类型的数据 83
4.6 为输入数据格式选择合适的Hadoop InputFormat 87
4.7 添加新的输入数据格式的支持——实现自定义的InputFormat 90
4.8 格式化MapReduce计算的结果——使用Hadoop的OutputFormat 94
4.9 Hadoop的中间（map到reduce）数据分区 96
4.10 将共享资源传播和分发到MapReduce作业的任务中——Hadoop DistributedCache 98
4.11 在Hadoop上使用传统应用程序——Hadoop Streaming 103
4.12 添加MapReduce作业之间的依赖关系 106
4.13 用于报告自定义指标的Hadoop计数器 108
第5章 Hadoop生态系统 110
5.1 简介 110
5.2 安装HBase 111
5.3 使用Java客户端API随机存取数据 114
5.4 基于HBase（表输入/输出）运行MapReduce作业 116
5.5 安装Pig 120
5.6 运行第一条Pig命令 121
5.7 使用Pig执行集合操作（join，union）与排序 123
5.8 安装Hive 125
5.9 使用Hive运行SQL风格的查询 127
5.10 使用Hive执行join 129
5.11 安装Mahout 132
5.12 使用Mahout运行K-means 133
5.13 可视化K-means结果 136
第6章 分析 138
6.1 简介 138
6.2 使用MapReduce的简单分析 139
6.3 使用MapReduce执行Group-By 143
6.4 使用MapReduce计算频率分布和排序 146
6.5 使用GNU Plot绘制Hadoop计算结果 148
6.6 使用MapReduce计算直方图 151
6.7 使用MapReduce计算散点图 154
6.8 用Hadoop解析复杂的数据集 158
6.9 使用MapReduce连接两个数据集 164
第7章 搜索和索引 170
7.1 简介 170
7.2 使用Hadoop MapReduce生成倒排索引 170
7.3 使用Apache Nutch构建域内网络爬虫 175
7.4 使用Apache Solr索引和搜索网络文档 180
7.5 配置Apache HBase作为Apache Nutch的后端数据存储 182
7.6 在Hadoop集群上部署Apache HBase 185
7.7 使用Hadoop/HBase集群构建Apache Nutch全网爬虫服务 188
7.8 用于索引和搜索的ElasticSearch 191
7.9 生成抓取网页的内链图 193
第8章 聚类、推荐和关系发现 197
8.1 简介 197
8.2 基于内容的推荐 198
8.3 层次聚类 204
8.4 对亚马逊销售数据集进行聚类操作 208
8.5 基于协同过滤的推荐 212
8.6 使用朴素贝叶斯分类器的分类 216
8.7 使用Adwords平衡算法给广告分配关键字 222
第9章 海量文本数据处理 231
9.1 简介 231
9.2 使用Hadoop Streaming和Python预处理数据（抽取、清洗和格式转换） 231
9.3 使用Hadoop Streaming进行数据去重 235
9.4 使用importtsv和批量加载工具把大型数据集加载到Apache HBase数据存储中 237
9.5 创建用于文本数据的TF向量和TF-IDF向量 242
9.6 聚类文本数据 246
9.7 使用隐含狄利克雷分布（LDA）发现主题 249
9.8 使用Mahout的朴素贝叶斯分类器分类文件 252
第10章 云端部署——在云上使用Hadoop 255
10.1 简介 255
10.2 使用亚马逊弹性MapReduce运行Hadoop MapReduce计算 256
10.3 使用亚马逊EC2竞价实例来执行EMR作业流以节约开支 259
10.4 使用EMR执行Pig脚本 261
10.5 使用EMR执行Hive脚本 263
10.6 使用命令行界面创建亚马逊EMR作业流 267
10.7 使用EMR在亚马逊EC2云上部署Apache HBase集群 270
10.8 使用EMR引导操作来配置亚马逊EMR作业的虚拟机 275
10.9 使用Apache Whirr在云环境中部署Apache Hadoop集群 277
10.10 使用Apache Whirr在云环境中部署Apache HBase集群 281
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop MapReduce实战手册
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop YARN权威指南
译者序
推荐序一
推荐序二
前　言
第1章　Apache Hadoop YARN：简明历史及基本原理 1
1.1　引言 1
1.2　Apache Hadoop 2
1.3　阶段0：Ad Hoc集群时期 3
1.4　阶段1：Hadoop on Demand 3
1.4.1　HOD世界中的HDFS 5
1.4.2　HOD的特色及优势 5
1.4.3　HOD的缺点 6
1.5　阶段2：共享计算集群的黎明 8
1.5.1　共享集群的演进 8
1.5.2　使用共享MapReduce集群的问题 13
1.6　阶段3：YARN的出现 15
1.7　小结 16
第2章　Apache Hadoop YARN安装快速入门 17
2.1　准备开始 18
2.2　配置单节点YARN集群的步骤 18
2.2.1　第1步：下载Apache Hadoop 18
2.2.2　第2步：设置JAVA_HOME 19
2.2.3　第3步：创建用户和用户组 19
2.2.4　第4步：创建数据和日志目录 19
2.2.5　第5步：配置core-site.xml 19
2.2.6　第6步：配置hdfs-site.xml 20
2.2.7　第7步：配置mapred-site.xml 21
2.2.8　第8步：配置yarn-site.xml 21
2.2.9　第9步：调整Java堆大小 21
2.2.10　第10步：格式化HDFS 22
2.2.11　第11步：启动HDFS服务 22
2.2.12　第12步：启动YARN服务 23
2.2.13　第13步：通过Web接口验证正在运行的服务 24
2.3　运行MapReduce示例程序 25
2.4　小结 26
第3章　Apache Hadoop YARN的核心概念 27
3.1　不只是MapReduce 27
3.2　Apache Hadoop MapReduce 29
3.2.1　支持非MapReduce应用的需求 30
3.2.2　解决可扩展性 30
3.2.3　提高资源使用率 30
3.2.4　用户敏捷性 30
3.3　Apache Hadoop YARN 31
3.4　YARN组件 32
3.4.1　ResourceManager 32
3.4.2　ApplicationMaster 32
3.4.3　资源模型 33
3.4.4　ResourceRequest和Container 33
3.4.5　Container规范 34
3.5　小结 34
第4章　YARN组件的功能概述 35
4.1　体系架构概述 35
4.2　ResourceManager 37
4.3　YARN调度组件 38
4.3.1　FIFO调度器 38
4.3.2　Capacity调度器 38
4.3.3　Fair调度器 39
4.4　Container 40
4.5　NodeManager 40
4.6　ApplicationMaster 41
4.7　YARN资源模型 41
4.7.1　客户端资源请求 42
4.7.2　ApplicationMaster Container的分配 42
4.7.3　ApplicationMaster与Container管理器的通信 44
4.8　管理应用程序的依赖文件 44
4.8.1　LocalResource的定义 44
4.8.2　LocalResource时间戳 45
4.8.3　LocalResource类型 46
4.8.4　LocalResource的可见性 46
4.8.5　LocalResource的生命周期 47
4.9　小结 47
第5章　安装Apache Hadoop YARN 49
5.1　基础知识 49
5.2　系统准备 50
5.2.1　第1步：安装EPEL和pdsh 50
5.2.2　第2步：生成和分发ssh密钥 51
5.3　基于脚本安装Hadoop 2 51
5.3.1　JDK选项 52
5.3.2　第1步：下载并解压脚本 52
5.3.3　第2步：设置脚本里的变量 52
5.3.4　第3步：提供节点名字 53
5.3.5　第4步：运行脚本 54
5.3.6　第5步：验证安装 54
5.4　基于脚本的卸载 57
5.5　配置文件处理 57
5.6　配置文件设置 57
5.6.1　core-site.xml 57
5.6.2　hdfs-site.xml 58
5.6.3　mapred-site.xml 58
5.6.4　yarn-site.xml 59
5.7　启动脚本 59
5.8　用Apache Ambari安装Hadoop 60
5.8.1　基于Ambari安装Hadoop 61
5.8.2　第1步：检查要求 61
5.8.3　第2步：安装Ambari服务器 62
5.8.4　第3步：安装和启动Ambari代理 62
5.8.5　第4步：启动Ambari服务器 62
5.8.6　第5步：安装HDP2.X集群 63
5.9　小结 70
第6章　Apache Hadoop YARN的管理 71
6.1　基于脚本的配置 71
6.2　监控集群健康：Nagios 76
6.2.1　监控基本的Hadoop服务 77
6.2.2　监控JVM 80
6.3　实时监控系统：Ganglia 82
6.4　使用Ambari管理 83
6.5　JVM分析 88
6.6　基本的YARN管理 90
6.6.1　YARN的管理工具 91
6.6.2　增加或关闭YARN节点 92
6.6.3　Capacity调度器的配置 92
6.6.4　YARN的Web代理 92
6.6.5　使用JobHistoryServer 93
6.6.6　更新用户到用户组的映射 93
6.6.7　更新超级用户代理群组映射 93
6.6.8　更新ResourceManager管理的ACL 93
6.6.9　重新加载服务级授权策略文件 94
6.6.10　管理YARN作业 94
6.6.11　设置Container的内存 94
6.6.12　设置Container核数 94
6.6.13　设置MapReduce配置项 95
6.6.14　用户日志管理 95
6.7　小结 97
第7章　Apache Hadoop YARN的架构指南 98
7.1　概述 98
7.2　ResourceManager 99
7.2.1　ResourceManager组件概述 100
7.2.2　客户端和ResourceManager交互 100
7.2.3　应用程序和ResourceManager的通信 102
7.2.4　节点和ResourceManager的通信 103
7.2.5　ResourceManager核心组件 104
7.2.6　ResourceManager安全相关的组件 105
7.3　NodeManager 109
7.3.1　NodeManager各组件概述 109
7.3.2　NodeManager组件 110
7.3.3　NodeManager安全组件 116
7.3.4　NodeManager的重要功能 116
7.4　ApplicationMaster 117
7.4.1　概述 117
7.4.2　活跃 119
7.4.3　资源需求 119
7.4.4　调度 120
7.4.5　调度协议和本地性 121
7.4.6　启动Container 123
7.4.7　完成的Container 124
7.4.8　ApplicationMaster失败和恢复 124
7.4.9　协调和输出提交 124
7.4.10　为客户端提供信息 125
7.4.11　安全 125
7.4.12　ApplicationMaster退出时进行清理 125
7.5　YARN Container 125
7.5.1　Container运行环境 126
7.5.2　与ApplicationMaster通信 127
7.6　应用程序开发者的摘要 127
7.7　小结 128
第8章　YARN中的Capacity调度器 129
8.1　Capacity调度器介绍 129
8.1.1　多租户弹性 130
8.1.2　安全 130
8.1.3　资源感知 130
8.1.4　细粒度调度 130
8.1.5　本地化 131
8.1.6　调度策略 131
8.2　Capacity调度器配置 131
8.3　队列 132
8.4　层级队列 132
8.4.1　关键特性 132
8.4.2　队列间的调度 132
8.4.3　定义层级队列 133
8.5　队列访问控制 134
8.6　层级队列Capacity管理 135
8.7　用户级别限制 137
8.8　预订 139
8.9　队列的状态 140
8.10　应用程序的限制 141
8.11　用户接口 141
8.12　小结 142
第9章　Apache Hadoop YARN下的MapReduce 143
9.1　运行Hadoop YARN MapReduce实例 143
9.1.1　可利用的实例列表 143
9.1.2　运行Pi实例 144
9.1.3　使用Web GUI监控实例 146
9.1.4　运行terasort测试 151
9.1.5　运行TestDFSIO基准测试 151
9.2　MapReduce兼容性 152
9.3　MapReduce ApplicationMaster 153
9.3.1　启用ApplicationMaster的重启 153
9.3.2　启用已完成任务的恢复 153
9.3.3　JobHistory服务 153
9.4　计算一个节点的容量 154
9.5　Shuffle服务的变动 155
9.6　运行已有的第1版Hadoop的应用程序 155
9.6.1　org.apache.hadoop.mapred API的二进制兼容性 155
9.6.2　org.apache.hadoop.mapreduce API的源码兼容性 155
9.6.3　命令行脚本的兼容性 156
9.6.4　MRv1和早期MRv2（0.23.x）应用程序兼容性的权衡 156
9.7　运行第1版MapReduce现有的代码 157
9.7.1　在YARN上运行Apache Pig脚本 157
9.7.2　在YARN上运行Apache Hive查询 157
9.7.3　在YARN上运行Apache Oozie工作流 157
9.8　高级特性 158
9.8.1　Uber作业 158
9.8.2　可插拔的Shuffle和Sort 158
9.9　小结 159
第10章　Apache Hadoop YARN应用程序范例 160
10.1　YARN客户端 161
10.2　ApplicationMaster 175
10.3　小结 192
第11章　使用Apache Hadoop YARN Distributed-Shell 193
11.1　使用YARN Distributed-Shell 193
11.1.1　简单例子 194
11.1.2　使用更多Container 195
11.1.3　带有shell命令参数的Distributed-Shell 195
11.2　Distributed-Shell内部实现 197
11.2.1　应用的常量定义 198
11.2.2　Client 198
11.2.3　ApplicationMaster 201
11.2.4　普通Container 205
11.3　小结 205
第12章　Apache Hadoop YARN框架 206
12.1　Distributed-Shell 206
12.2　Hadoop MapReduce 206
12.3　Apache Tez 207
12.4　Apache Giraph 207
12.5　Hoya：HBase on YARN 208
12.6　Dryad on YARN 208
12.7　Apache Spark 208
12.8　Apache Storm 209
12.9　REEF：Retainable Evaluator Execution Framework 209
12.10　Hamster：Hadoop and MPI on the Same Cluster 210
12.11　小结 210
附录A　补充内容和代码下载 211
附录B　YARN的安装脚本 212
附录C　YARN的管理脚本 224
附录D　Nagios模块 229
附录E　资源及附加资料 235
附录F　HDFS快速参考 237
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop YARN权威指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop构建数据仓库实践
目 录
第1章 数据仓库简介
1.1 什么是数据仓库 1
1.1.1 数据仓库的定义 1
1.1.2 建立数据仓库的原因 3
1.2 操作型系统与分析型系统 5
1.2.1 操作型系统 5
1.2.2 分析型系统 8
1.2.3 操作型系统和分析型系统对比 9
1.3 数据仓库架构 10
1.3.1 基本架构 10
1.3.2 主要数据仓库架构 12
1.3.3 操作数据存储 16
1.4 抽取-转换-装载 17
1.4.1 数据抽取 17
1.4.2 数据转换 19
1.4.3 数据装载 20
1.4.4 开发ETL系统的方法 21
1.4.5 常见ETL工具 21
1.5 数据仓库需求 22
1.5.1 基本需求 22
1.5.2 数据需求 23
1.6 小结 24
第2章 数据仓库设计基础
2.1 关系数据模型 25
2.1.1 关系数据模型中的结构 25
2.1.2 关系完整性 28
2.1.3 规范化 30
2.1.4 关系数据模型与数据仓库 33
2.2 维度数据模型 34
2.2.1 维度数据模型建模过程 35
2.2.2 维度规范化 36
2.2.3 维度数据模型的特点 37
2.2.4 星型模式 38
2.2.5 雪花模式 40
2.3 Data Vault模型 42
2.3.1 Data Vault模型简介 42
2.3.2 Data Vault模型的组成部分 43
2.3.3 Data Vault模型的特点 44
2.3.4 Data Vault模型的构建 44
2.3.5 Data Vault模型实例 46
2.4 数据集市 49
2.4.1 数据集市的概念 50
2.4.2 数据集市与数据仓库的区别 50
2.4.3 数据集市设计 50
2.5 数据仓库实施步骤 51
2.6 小结 54
第3章 Hadoop生态圈与数据仓库
3.1 大数据定义 55
3.2 Hadoop简介 56
3.2.1 Hadoop的构成 57
3.2.2 Hadoop的主要特点 58
3.2.3 Hadoop架构 58
3.3 Hadoop基本组件 59
3.3.1 HDFS 60
3.3.2 MapReduce 65
3.3.3 YARN 72
3.4 Hadoop生态圈的其他组件 77
3.5 Hadoop与数据仓库 81
3.5.1 关系数据库的可扩展性瓶颈 82
3.5.2 CAP理论 84
3.5.3 Hadoop数据仓库工具 85
3.6 小结 88
第4章 安装Hadoop
4.1 Hadoop主要发行版本 89
4.1.1 Cloudera Distribution for Hadoop（CDH） 89
4.1.2 Hortonworks Data Platform（HDP） 90
4.1.3 MapR Hadoop 90
4.2 安装Apache Hadoop 91
4.2.1 安装环境 91
4.2.2 安装前准备 92
4.2.3 安装配置Hadoop 93
4.2.4 安装后配置 97
4.2.5 初始化及运行 97
4.3 配置HDFS Federation 99
4.4 离线安装CDH及其所需的服务 104
4.4.1 CDH安装概述 104
4.4.2 安装环境 106
4.4.3 安装配置 106
4.4.4 Cloudera Manager许可证管理 114
4.5 小结 115
第5章 Kettle与Hadoop
5.1 Kettle概述 117
5.2 Kettle连接Hadoop 119
5.2.1 连接HDFS 119
5.2.2 连接Hive 124
5.3 导出导入Hadoop集群数据 128
5.3.1 把数据从HDFS抽取到RDBMS 128
5.3.2 向Hive表导入数据 132
5.4 执行Hive的HiveQL语句 134
5.5 MapReduce转换示例 135
5.6 Kettle提交Spark作业 143
5.6.1 安装Spark 143
5.6.2 配置Kettle向Spark集群提交作业 146
5.7 小结 149
第6章 建立数据仓库示例模型
6.1 业务场景 150
6.2 Hive相关配置 152
6.2.1 选择文件格式 152
6.2.2 支持行级更新 159
6.2.3 Hive事务支持的限制 164
6.3 Hive表分类 164
6.4 向Hive表装载数据 169
6.5 建立数据库表 174
6.6 装载日期维度数据 179
6.7 小结 180
第7章 数据抽取
7.1 逻辑数据映射 182
7.2 数据抽取方式 185
7.3 导出成文本文件 191
7.4 分布式查询 196
7.5 使用Sqoop抽取数据 200
7.5.1 Sqoop简介 200
7.5.2 CDH 5.7.0中的Sqoop 203
7.5.3 使用Sqoop抽取数据 203
7.5.4 Sqoop优化 207
7.6 小结 208
第8章 数据转换与装载
8.1 数据清洗 210
8.2 Hive简介 214
8.2.1 Hive的体系结构 215
8.2.2 Hive的工作流程 216
8.2.3 Hive服务器 218
8.2.4 Hive客户端 221
8.3 初始装载 231
8.4 定期装载 236
8.5 Hive优化 246
8.6 小结 254
第9章 定期自动执行ETL作业
9.1 crontab 256
9.2 Oozie简介 260
9.2.1 Oozie的体系结构 260
9.2.2 CDH 5.7.0中的Oozie 262
9.3 建立定期装载工作流 262
9.4 建立协调器作业定期自动执行工作流 271
9.5 Oozie优化 275
9.6 小结 276
第10章 维度表技术
10.1 增加列 278
10.2 维度子集 285
10.3 角色扮演维度 292
10.4 层次维度 298
10.4.1 固定深度的层次 299
10.4.2 递归 302
10.4.3 多路径层次 310
10.4.4 参差不齐的层次 312
10.5 退化维度 313
10.6 杂项维度 316
10.7 维度合并 323
10.8 分段维度 329
10.9 小结 335
第11章 事实表技术
11.1 事实表概述 336
11.2 周期快照 337
11.3 累积快照 343
11.4 无事实的事实表 349
11.5 迟到的事实 354
11.6 累积度量 360
11.7 小结 366
第12章 联机分析处理
12.1 联机分析处理简介 367
12.1.1 概念 367
12.1.2 分类 368
12.1.3 性能 371
12.2 Impala简介 371
12.3 Hive、SparkSQL、Impala比较 377
12.3.1 Spark SQL简介 377
12.3.2 Hive、Spark SQL、Impala比较 379
12.3.3 Hive、Spark SQL、Impala性能对比 382
12.4 联机分析处理实例 387
12.5 Apache Kylin与OLAP 399
12.5.1 Apache Kylin架构 399
12.5.2 Apache Kylin安装 401
12.6 小结 407
第13章 数据可视化
13.1 数据可视化简介 408
13.2 Hue简介 410
13.2.1 Hue功能快速预览 411
13.2.2 配置元数据存储 412
13.3 Zeppelin简介 415
13.3.1 Zeppelin架构 415
13.3.2 Zeppelin安装配置 416
13.3.3 在Zeppelin中添加MySQL翻译器 421
13.4 Hue、Zeppelin比较 425
13.5 数据可视化实例 426
13.6 小结 434
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop构建数据仓库实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战
前言
第1章 Hadoop简介
第2章 Hadoop的安装与配置
第3章 Hadoop应用案例分析
第4章 MapReduce计算模型
第5章 开发MapReduce应用程序
第6章 MapReduce应用案例
第7章 MapReduce工作机制
第8章 HadoopI/O
第9章 HDFS详解
第10章 Hadoop的管理
第11章 Hive详解
第12章 HBase详解
第13章 Mahout详解
第14章 Pig详解
第15章 ZooKeepet详解
第16章 Avro详解
第17章 Chukwa详解
第18章 Hadoop的常用插件与开发
附录A 云计算在线检测平台
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析Hadoop Common和HDFS架构设计与实现原理
前　言
第一部分　环境准备
第1章　源代码环境准备/ 2
1.1　什么是Hadoop / 2
1.1.1　Hadoop简史/ 2
1.1.2　Hadoop的优势/ 3
1.1.3　Hadoop生态系统/ 4
1.2　准备源代码阅读环境/ 8
1.2.1　安装与配置JDK / 8
1.2.2　安装Eclipse / 9
1.2.3　安装辅助工具Ant/ 12
1.2.4　安装类UNIX Shell环境Cygwin / 13
1.3　准备Hadoop源代码/ 15
1.3.1　下载Hadoop / 15
1.3.2　创建Eclipse项目/ 16
1.3.3　Hadoop源代码组织/ 18
1.4　小结/ 19
第二部分　Common的实现
第2章　Hadoop配置信息处理/ 22
2.1　配置文件简介/ 22
2.1.1　Windows操作系统的配置文件/ 22
2.1.2　Java配置文件/ 23
2.2　Hadoop Configuration详解/ 24
2.2.1　Hadoop配置文件的格式/ 24
2.2.2　Configuration的成员变量/ 26
2.2.3　资源加载/ 27
2.2.4　使用get*和set*访问/设置配置项/ 32
2.3　Configurable接口/ 34
2.4　小结/ 35
第3章　序列化与压缩/ 36
3.1　序列化/ 36
3.1.1　Java内建序列化机制/ 36
3.1.2　Hadoop序列化机制/ 38
3.1.3　Hadoop序列化机制的特征/ 39
3.1.4　Hadoop Writable机制/ 39
3.1.5　典型的Writable类详解/ 41
3.1.6　Hadoop序列化框架/ 48
3.2　压缩/ 49
3.2.1　Hadoop压缩简介/ 50
3.2.2　Hadoop压缩API应用实例/ 51
3.2.3　Hadoop压缩框架/ 52
3.2.4　Java本地方法/ 61
3.2.5　支持Snappy压缩/ 65
3.3　小结/ 69
第4章　Hadoop远程过程调用/ 70
4.1　远程过程调用基础知识/ 70
4.1.1　RPC原理/ 70
4.1.2　RPC机制的实现/ 72
4.1.3　Java远程方法调用/ 73
4.2　Java动态代理/ 78
4.2.1　创建代理接口/ 78
4.2.2　调用转发/ 80
4.2.3　动态代理实例/ 81
4.3　Java NIO/ 84
4.3.1　Java基本套接字/ 84
4.3.2　Java NIO基础/ 86
4.3.3　Java NIO实例：回显服务器/ 93
4.4　Hadoop中的远程过程调用/ 96
4.4.1　利用Hadoop IPC构建简单的分布式系统/ 96
4.4.2　Hadoop IPC的代码结构/ 100
4.5　Hadoop IPC连接相关过程/ 104
4.5.1　IPC连接成员变量/ 104
4.5.2　建立IPC连接/ 106
4.5.3　数据分帧和读写/ 111
4.5.4　维护IPC连接/ 114
4.5.5　关闭IPC连接/ 116
4.6　Hadoop IPC方法调用相关过程/ 118
4.6.1　Java接口与接口体/ 119
4.6.2　IPC方法调用成员变量/ 121
4.6.3　客户端方法调用过程/ 123
4.6.4　服务器端方法调用过程/ 126
4.7　Hadoop IPC上的其他辅助过程/ 135
4.7.1　RPC.getProxy()和RPC.stopProxy() / 136
4.7.2　RPC.getServer()和Server的启停/ 138
4.8　小结/ 141
第5章　Hadoop文件系统/ 142
5.1　文件系统/ 142
5.1.1　文件系统的用户界面/ 142
5.1.2　文件系统的实现/ 145
5.1.3　文件系统的保护控制/ 147
5.2　Linux文件系统/ 150
5.2.1　Linux本地文件系统/ 150
5.2.2　虚拟文件系统/ 153
5.2.3　Linux文件保护机制/ 154
5.2.4　Linux文件系统API/ 155
5.3　分布式文件系统/ 159
5.3.1　分布式文件系统的特性/ 159
5.3.2　基本NFS体系结构/ 160
5.3.3　NFS支持的文件操作/ 160
5.4　Java文件系统/ 162
5.4.1　Java文件系统API / 162
5.4.2　URI和URL / 164
5.4.3　Java输入/输出流/ 166
5.4.4　随机存取文件/ 169
5.5　Hadoop抽象文件系统/ 170
5.5.1　Hadoop文件系统API / 170
5.5.2　Hadoop输入/输出流/ 175
5.5.3　Hadoop文件系统中的权限/ 179
5.5.4　抽象文件系统中的静态方法/ 180
5.5.5　Hadoop文件系统中的协议处理器/ 184
5.6　Hadoop具体文件系统/ 188
5.6.1　FileSystem层次结构/ 189
5.6.2　RawLocalFileSystem的实现/ 191
5.6.3　ChecksumFileSystem的实现/ 196
5.6.4　RawInMemoryFileSystem的实现/ 210
5.7　小结/ 213
第三部分　Hadoop分布式文件系统
第6章　HDFS概述/ 216
6.1　初识HDFS / 216
6.1.1　HDFS主要特性/ 216
6.1.2　HDFS体系结构/ 217
6.1.3　HDFS源代码结构/ 221
6.2　基于远程过程调用的接口/ 223
6.2.1　与客户端相关的接口/ 224
6.2.2　HDFS各服务器间的接口/ 236
6.3　非远程过程调用接口/ 244
6.3.1　数据节点上的非IPC接口/ 245
6.3.2　名字节点和第二名字节点上的非IPC接口/ 252
6.4　HDFS主要流程/ 254
6.4.1　客户端到名字节点的文件与目录操作/ 254
6.4.2　客户端读文件/ 256
6.4.3　客户端写文件/ 257
6.4.4　数据节点的启动和心跳/ 258
6.4.5　第二名字节点合并元数据/ 259
6.5　小结/ 261
第7章　数据节点实现/ 263
7.1　数据块存储/ 263
7.1.1　数据节点的磁盘目录文件结构/ 263
7.1.2　数据节点存储的实现/ 266
7.1.3　数据节点升级/ 269
7.1.4　文件系统数据集的工作机制/ 276
7.2　流式接口的实现/ 285
7.2.1　DataXceiverServer和DataXceiver / 286
7.2.2　读数据/ 289
7.2.3　写数据/ 298
7.2.4　数据块替换、数据块拷贝和读数据块检验信息/ 313
7.3　作为整体的数据节点/ 314
7.3.1　数据节点和名字节点的交互/ 314
7.3.2　数据块扫描器/ 319
7.3.3　数据节点的启停/ 321
7.4　小结/ 326
第8章　名字节点实现/ 327
8.1　文件系统的目录树/ 327
8.1.1　从i-node到INode/ 327
8.1.2　命名空间镜像和编辑日志/ 333
8.1.3　第二名字节点/ 351
8.1.4　FSDirectory的实现/ 361
8.2　数据块和数据节点管理/ 365
8.2.1　数据结构/ 366
8.2.2　数据节点管理/ 378
8.2.3　数据块管理/ 392
8.3　远程接口ClientProtocol的实现/ 412
8.3.1　文件和目录相关事务/ 412
8.3.2　读数据使用的方法/ 415
8.3.3　写数据使用的方法/ 419
8.3.4　工具dfsadmin依赖的方法/ 443
8.4　名字节点的启动和停止/ 444
8.4.1　安全模式/ 444
8.4.2　名字节点的启动/ 449
8.4.3　名字节点的停止/ 454
8.5　小结/ 454
第9章　HDFS客户端/ 455
9.1　认识DFSClient / 455
9.1.1　DFSClient的构造和关闭/ 455
9.1.2　文件和目录、系统管理相关事务/ 457
9.1.3　删除HDFS文件/目录的流程/ 459
9.2　输入流/ 461
9.2.1　读数据前的准备：打开文件/ 463
9.2.2　读数据/ 465
9.2.3　关闭输入流/ 475
9.2.4　读取HDFS文件数据的流程/ 475
9.3　输出流/ 478
9.3.1　写数据前的准备：创建文件/ 481
9.3.2　写数据：数据流管道的建立/ 482
9.3.3　写数据：数据包的发送/ 486
9.3.4　写数据：数据流管道出错处理/ 493
9.3.5　写数据：租约更新/ 496
9.3.6　写数据：DFSOutputStream.sync()的作用/ 497
9.3.7　关闭输出流/ 499
9.3.8　向HDFS文件写入数据的流程/ 500
9.4　DistributedFileSystem的实现/ 506
9.5　HDFS常用工具/ 508
9.5.1　FsShell / 508
9.5.2　DFSAdmin / 510
9.6　小结/ 511
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析Hadoop Common和HDFS架构设计与实现原理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据处理系统 : Hadoop源代码情景分析
第1章 大数据与Hadoop
1.1 什么是大数据
1.2 大数据的用途
1.3 并行计算
1.4 数据流
1.5 函数式程序设计与Lambda演算
1.6 MapReduce
1.7 大数据处理平台
1.8 Hadoop的由来和发展
1.9 Hadoop的MapReduce计算框架
1.10 Hadoop的分布式容错文件系统HDFS
第2章 研究方法
2.1 摘要卡片
2.2 情景分析
2.3 面向对象的程序设计
2.4 怎样阅读分析Hadoop的代码
第3章 Hadoop集群和YARN
3.1 Hadoop集群
3.2 Hadoop系统的结构
3.3 Hadoop的YARN框架
3.4 状态机
3.5 资源管理器ResourceManager
3.6 资源调度器ResourceScheduler
第4章 Hadoop的RPC机制
4.1 RPC与RMI
4.2 ProtoBuf
4.3 Java的Reflection机制
4.4 RM节点上的RPC服务
4.5 RPC客户端的创建
第5章 Hadoop作业的提交
5.1 从“地方”到“中央”
5.2 示例一：采用老API的ValueAggregatorJob
5.3 示例二：采用新API的WordCount
5.4 示例三：采用ToolRunner的QuasiMonteCarlo
5.5 从Job.submit()开始的第二段流程
5.6 YARNRunner和ResourceMgrDelegate
第6章 作业的调度与指派
6.1 作业的受理
6.2 NM节点的心跳和容器周转
6.3 容器的分配
第7章 NodeManager与任务投运
7.1 AMLauncher与任务投运
7.2 MRAppMaster或AM的创建
7.3 资源本地化
7.4 容器的投运
第8章 MRAppMaster与作业投运
8.1 MRAppMaster
8.2 App资源与容器
8.3 容器的跨节点投送和启动
8.4 目标节点上的容器投运
8.5 Uber模式下的本地容器分配与投运
8.6 任务的启动
8.7 MapTask的运行
8.8 ReduceTask的投运
第9章 YARN子系统的计算框架
9.1 MapReduce框架
9.2 Streaming框架
9.3 Chain框架
9.4 Client与ApplicationMaster
第10章 MapReduce框架中的数据流
10.1 数据流和工作流
10.2 Mapper的输入
10.3 Mapper的输出缓冲区MapOutputBuffer
10.4 作为Collector的MapOutputBuffer
10.5 环形缓冲区kvbuffer
10.6 对MapoutputBuffer的输出
10.7 Sort和Spill
10.8 Map计算的终结与Spill文件的合并
10.9 Reduce阶段
10.10 Merge
10.11 Reduce阶段的输入和输出
第11章 Hadoop的文件系统HDFS
11.1 文件的分布与容错
11.2 目录节点NameNode
11.3 FSNamesystem
11.4 文件系统目录FSDirectory
11.5 文件系统映像FsImage
11.6 文件系统更改记录FSEditLog
11.7 FSEditLog与Journal
11.8 EditLog记录的重演
11.9 版本升级与故障恢复
第12章 HDFS的DataNode
12.1 DataNode
12.2 数据块的存储
12.3 RamDisk复份的持久化存储
12.4 目录扫描线程DirectoryScanner
12.5 数据块扫描线程DataBlockScanner
第13章 DataNode与NameNode的互动
13.1 DataNode与NameNode的互动
13.2 心跳HeartBeat
13.3 BlockReport
第14章 DataNode间的互动
14.1 数据块的接收和存储
14.2 命令DNA_TRANSFER的执行
第15章 HDFS的文件访问
15.1 DistributedFileSystem和DFSClient
15.2 FsShell
15.3 HDFS的打开文件流程
15.4 HDFS的读文件流程
15.5 HDFS的创建文件流程
15.6 文件租约
15.7 HDFS的写文件流程
15.8 实例
第16章 Hadoop的容错机制
16.1 容错与高可用
16.2 HDFS的HA机制
16.3 NameNode的倒换
16.4 Zookeeper与自动倒换
16.5 YARN的HA机制
第17章 Hadoop的安全机制
17.1 大数据集群的安全问题
17.2 UGI、Token和ACL
17.3 UGI的来源和流转
17.4 Token的使用
第18章 Hadoop的人机界面
18.1 Hadoop的命令行界面
18.2 Hadoop的Web界面
18.3 Dependency Inject和Annotation
18.4 对网页的访问
第19章 Hadoop的部署和启动
19.1 Hadoop的运维脚本
19.2 Hadoop的部署与启动
19.3 Hadoop的日常使用
19.4 Hadoop平台的关闭
第20章 Spark的优化与改进
20.1 Spark与Hadoop
20.2 RDD与Stage——概念与思路
20.3 RDD的存储和引用
20.4 DStream
20.5 拓扑的灵活性和多样性
20.6 性能的提升
20.7 使用的方便性
20.8 几个重要的类及其作用
参考资料
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据处理系统 : Hadoop源代码情景分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop应用开发技术详解
前言
第1章 Hadoop概述
1.1 Hadoop起源
1.1.1 Google与Hadoop模块
1.1.2 为什么会有Hadoop
1.1.3 Hadoop版本介绍
1.2 Hadoop生态系统
1.3 Hadoop常用项目介绍
1.4 Hadoop在国内的应用
1.5 本章小结
第2章 Hadoop安装
2.1 Hadoop环境安装配置
2.1.1 安装VMware
2.1.2 安装Ubuntu
2.1.3 安装VMwareTools
2.1.4 安装JDK
2.2 Hadoop安装模式
2.2.1 单机安装
2.2.2 伪分布式安装
2.2.3 分布式安装
2.3 如何使用Hadoop
2.3.1 Hadoop的启动与停止
2.3.2 Hadoop配置文件
2.4 本章小结
第3章 MapReduce快速入门
3.1 WordCount实例准备开发环境
3.1.1 使用Eclipse创建一个Java工程
3.1.2 导入Hadoop的JAR文件
3.2 MapReduce代码的实现
3.2.1 编写WordMapper类
3.2.2 编写WordReducer类
3.2.3 编写WordMain驱动类
3.3 打包、部署和运行
3.3.1 打包成JAR文件
3.3.2 部署和运行
3.3.3 测试结果
3.4 本章小结
第4章 Hadoop分布式文件系统详解
4.1 认识HDFS
4.1.1 HDFS的特点
4.1.2 Hadoop文件系统的接口
4.1.3 HDFS的Web服务
4.2 HDFS架构
4.2.1 机架
4.2.2 数据块
4.2.3 元数据节点
4.2.4 数据节点
4.2.5 辅助元数据节点
4.2.6 名字空间
4.2.7 数据复制
4.2.8 块备份原理
4.2.9 机架感知
4.3 Hadoop的RPC机制
4.3.1 RPC的实现流程
4.3.2 RPC的实体模型
4.3.3 文件的读取
4.3.4 文件的写入
4.3.5 文件的一致模型
4.4 HDFS的HA机制
4.4.1 HA集群
4.4.2 HA架构
4.4.3 为什么会有HA机制
4.5 HDFS的Federation机制
4.5.1 单个NameNode的HDFS架构的局限性
4.5.2 为什么引入Federation机制
4.5.3 Federation架构
4.5.4 多个名字空间的管理问题
4.6 Hadoop文件系统的访问
4.6.1 安全模式
4.6.2 HDFS的Shell访问
4.6.3 HDFS处理文件的命令
4.7 JavaAPI接口
4.7.1 HadoopURL读取数据
4.7.2 FileSystem类
4.7.3 FileStatus类
4.7.4 FSDataInputStream类
4.7.5 FSDataOutputStream类
4.7.6 列出HDFS下所有的文件
4.7.7 文件的匹配
4.7.8 PathFilter对象
4.8 维护HDFS
4.8.1 追加数据
4.8.2 并行复制
4.8.3 升级与回滚
4.8.4 添加节点
4.8.5 删除节点
4.9 HDFS权限管理
4.9.1 用户身份
4.9.2 权限管理的原理
4.9.3 设置权限的Shell命令
4.9.4 超级用户
4.9.5 HDFS权限配置参数
4.10 本章小结
第5章 Hadoop文件I/O详解
5.1 Hadoop文件的数据结构
5.1.1 SequenceFile存储
5.1.2 MapFile存储
5.1.3 SequenceFile转换为MapFile
5.2 HDFS数据完整性
5.2.1 校验和
5.2.2 数据块检测程序
5.3 文件序列化
5.3.1 进程间通信对序列化的要求
5.3.2 Hadoop文件的序列化
5.3.3 Writable接口
5.3.4 WritableComparable接口
5.3.5 自定义Writable接口
5.3.6 序列化框架
5.3.7 数据序列化系统Avro
5.4 Hadoop的Writable类型
5.4.1 Writable类的层次结构
5.4.2 Text类型
5.4.3 NullWritable类型
5.4.4 ObjectWritable类型
5.4.5 GenericWritable类型
5.5 文件压缩
5.5.1 Hadoop支持的压缩格式
5.5.2 Hadoop中的编码器和解码器
5.5.3 本地库
5.5.4 可分割压缩LZO
5.5.5 压缩文件性能比较
5.5.6 Snappy压缩
5.5.7 gzip、LZO和Snappy比较
5.6 本章小结
第6章 MapReduce工作原理
6.1 MapReduce的函数式编程概念
6.1.1 列表处理
6.1.2 Mapping数据列表
6.1.3 Reducing数据列表
6.1.4 Mapper和Reducer如何工作
6.1.5 应用实例：词频统计
6.2 MapReduce框架结构
6.2.1 MapReduce模型
6.2.2 MapReduce框架组成
6.3 MapReduce运行原理
6.3.1 作业的提交
6.3.2 作业初始化
6.3.3 任务的分配
6.3.4 任务的执行
6.3.5 进度和状态的更新
6.3.6 MapReduce的进度组成
6.3.7 任务完成
6.4 MapReduce容错
6.4.1 任务失败
6.4.2 TaskTracker失败
6.4.3 JobTracker失败
6.4.4 子任务失败
6.4.5 任务失败反复次数的处理方法
6.5 Shuffle阶段和Sort阶段
6.5.1 Map端的Shuffle
6.5.2 Reduce端的Shuffle
6.5.3 Shuffle过程参数调优
6.6 任务的执行
6.6.1 推测执行
6.6.2 任务JVM重用
6.6.3 跳过坏的记录
6.6.4 任务执行的环境
6.7 作业调度器
6.7.1 先进先出调度器
6.7.2 容量调度器
6.7.3 公平调度器
6.8 自定义Hadoop调度器
6.8.1 Hadoop调度器框架
6.8.2 编写Hadoop调度器
6.9 YARN介绍
6.9.1 异步编程模型
6.9.2 YARN支持的计算框架
6.9.3 YARN架构
6.9.4 YARN工作流程
6.10 本章小结
第7章 Eclipse插件的应用
7.1 编译Hadoop源码
7.1.1 下载Hadoop源码
7.1.2 准备编译环境
7.1.3 编译common组件
7.2 Eclipse安装MapReduce插件
7.2.1 查找MapReduce插件
7.2.2 新建一个Hadooplocation
7.2.3 Hadoop插件操作HDFS
7.2.4 运行MapReduce的驱动类
7.3 MapReduce的Debug调试
7.3.1 进入Debug运行模式
7.3.2 Debug调试具体操作
7.4 单元测试框架MRUnit
7.4.1 认识MRUnit框架
7.4.2 准备测试案例
7.4.3 Mapper单元测试
7.4.4 Reducer单元测试
7.4.5 MapReduce单元测试
7.5 本章小结
第8章 MapReduce编程开发
8.1 WordCount案例分析
8.1.1 MapReduce工作流程
8.1.2 WordCount的Map过程
8.1.3 WordCount的Reduce过程
8.1.4 每个过程产生的结果
8.1.5 Mapper抽象类
8.1.6 Reducer抽象类
8.1.7 MapReduce驱动
8.1.8 MapReduce最小驱动
8.2 输入格式
8.2.1 InputFormat接口
8.2.2 InputSplit类
8.2.3 RecordReader类
8.2.4 应用实例：随机生成100个小数并求最大值
8.3 输出格式
8.3.1 OutputFormat接口
8.3.2 RecordWriter类
8.3.3 应用实例：把首字母相同的单词放到一个文件里
8.4 压缩格式
8.4.1 如何在MapReduce中使用压缩
8.4.2 Map作业输出结果的压缩
8.5 MapReduce优化
8.5.1 Combiner类
8.5.2 Partitioner类
8.5.3 分布式缓存
8.6 辅助类
8.6.1 读取Hadoop配置文件
8.6.2 设置Hadoop的配置文件属性
8.6.3 GenericOptionsParser选项
8.7 Streaming接口
8.7.1 Streaming工作原理
8.7.2 Streaming编程接口参数
8.7.3 作业配置属性
8.7.4 应用实例：抓取网页的标题
8.8 本章小结
第9章 MapReduce高级应用
9.1 计数器
9.1.1 默认计数器
9.1.2 自定义计数器
9.1.3 获取计数器
9.2 MapReduce二次排序
9.2.1 二次排序原理
9.2.2 二次排序的算法流程
9.2.3 代码实现
9.3 MapReduce中的Join算法
9.3.1 Reduce端Join
9.3.2 Map端Join
9.3.3 半连接SemiJoin
9.4 MapReduce从MySQL读写数据
9.4.1 读数据
9.4.2 写数据
9.5 Hadoop系统调优
9.5.1 小文件优化
9.5.2 Map和Reduce个数设置
9.6 本章小结
第10章 数据仓库工具Hive
10.1 认识Hive
10.1.1 Hive工作原理
10.1.2 Hive数据类型
10.1.3 Hive的特点
10.1.4 Hive下载与安装
10.2 Hive架构
10.2.1 Hive用户接口
10.2.2 Hive元数据库
10.2.3 Hive的数据存储
10.2.4 Hive解释器
10.3 Hive文件格式
10.3.1 TextFile格式
10.3.2 SequenceFile格式
10.3.3 RCFile文件格式
10.3.4 自定义文件格式
10.4 Hive操作
10.4.1 表操作
10.4.2 视图操作
10.4.3 索引操作
10.4.4 分区操作
10.4.5 桶操作
10.5 Hive复合类型
10.5.1 Struct类型
10.5.2 Array类型
10.5.3 Map类型
10.6 Hive的JOIN详解
10.6.1 JOIN操作语法
10.6.2 JOIN原理
10.6.3 外部JOIN
10.6.4 Map端JOIN
10.6.5 JOIN中处理NULL值的语义区别
10.7 Hive优化策略
10.7.1 列裁剪
10.7.2 MapJoin操作
10.7.3 GroupBy操作
10.7.4 合并小文件
10.8 Hive内置操作符与函数
10.8.1 字符串函数
10.8.2 集合统计函数
10.8.3 复合类型操作
10.9 Hive用户自定义函数接口
10.9.1 用户自定义函数UDF
10.9.2 用户自定义聚合函数UDAF
10.10 Hive的权限控制
10.10.1 角色的创建和删除
10.10.2 角色的授权和撤销
10.10.3 超级管理员权限
10.11 应用实例：使用JDBC开发Hive程序
10.11.1 准备测试数据
10.11.2 代码实现
10.12 本章小结
第11章 开源数据库HBase
11.1 认识HBase
11.1.1 HBase的特点
11.1.2 HBase访问接口
11.1.3 HBase存储结构
11.1.4 HBase存储格式
11.2 HBase设计
11.2.1 逻辑视图
11.2.2 框架结构及流程
11.2.3 Table和Region的关系
11.2.4 -ROOT-表和.META.表
11.3 关键算法和流程
11.3.1 Region定位
11.3.2 读写过程
11.3.3 Region分配
11.3.4 RegionServer上线和下线
11.3.5 Master上线和下线
11.4 HBase安装
11.4.1 HBase单机安装
11.4.2 HBase分布式安装
11.5 HBase的Shell操作
11.5.1 一般操作
11.5.2 DDL操作
11.5.3 DML操作
11.5.4 HBaseShell脚本
11.6 HBase客户端
11.6.1 JavaAPI交互
11.6.2 MapReduce操作HBase
11.6.3 向HBase中写入数据
11.6.4 读取HBase中的数据
11.6.5 Avro、REST和Thrift接口
11.7 本章小结
第12章 Mahout算法
12.1 Mahout的使用
12.1.1 安装Mahout
12.1.2 运行一个Mahout案例
12.2 Mahout数据表示
12.2.1 偏好Perference类
12.2.2 数据模型DataModel类
12.2.3 Mahout链接MySQL数据库
12.3 认识Taste框架
12.4 Mahout推荐器
12.4.1 基于用户的推荐器
12.4.2 基于项目的推荐器
12.4.3 SlopeOne推荐策略
12.5 推荐系统
12.5.1 个性化推荐
12.5.2 商品推荐系统案例
12.6 本章小结
附录A Hive内置操作符与函数
附录B HBase默认配置解释[1]
附录C Hadoop三个配置文件的参数含义说明
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop应用开发技术详解
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop海量数据处理（第2版） : 技术详解与项目实战
目录
基础篇：Hadoop基础
第1章 绪论 2
1.1 Hadoop和云计算 2
1.1.1 Hadoop的电梯演讲 2
1.1.2 Hadoop生态圈 3
1.1.3 云计算的定义 6
1.1.4 云计算的类型 7
1.1.5 Hadoop和云计算 8
1.2 Hadoop和大数据 9
1.2.1 大数据的定义 9
1.2.2 大数据的结构类型 10
1.2.3 大数据行业应用实例 12
1.2.4 Hadoop和大数据 13
1.2.5 其他大数据处理平台 14
1.3 数据挖掘和商业智能 15
1.3.1 数据挖掘的定义 15
1.3.2 数据仓库 17
1.3.3 操作数据库系统和数据仓库系统的区别 18
1.3.4 为什么需要分离的数据仓库 19
1.3.5 商业智能 19
1.3.6 大数据时代的商业智能 20
1.4 小结 21
第2章 环境准备 22
2.1 Hadoop的发行版本选择 22
2.1.1 Apache Hadoop 22
2.1.2 CDH 22
2.1.3 Hadoop的版本 23
2.1.4 如何选择Hadoop的版本 25
2.2 Hadoop架构 26
2.2.1 Hadoop HDFS架构 27
2.2.2 YARN架构 28
2.2.3 Hadoop架构 28
2.3 安装Hadoop 29
2.3.1 安装运行环境 30
2.3.2 修改主机名和用户名 36
2.3.3 配置静态IP地址 36
2.3.4 配置SSH无密码连接 37
2.3.5 安装JDK 38
2.3.6 配置Hadoop 39
2.3.7 格式化HDFS 42
2.3.8 启动Hadoop并验证安装 42
2.4 安装Hive 43
2.4.1 安装元数据库 44
2.4.2 修改Hive配置文件 44
2.4.3 验证安装 45
2.5 安装HBase 46
2.5.1 解压文件并修改Zookeeper相关配置 46
2.5.2 配置节点 46
2.5.3 配置环境变量 47
2.5.4 启动并验证 47
2.6 安装Sqoop 47
2.7 Cloudera Manager 48
2.8 小结 51
第3章 Hadoop的基石：HDFS 52
3.1 认识HDFS 52
3.1.1 HDFS的设计理念 54
3.1.2 HDFS的架构 54
3.1.3 HDFS容错 58
3.2 HDFS读取文件和写入文件 58
3.2.1 块的分布 59
3.2.2 数据读取 60
3.2.3 写入数据 61
3.2.4 数据完整性 62
3.3 如何访问HDFS 63
3.3.1 命令行接口 63
3.3.2 Java API 66
3.3.3 其他常用的接口 75
3.3.4 Web UI 75
3.4 HDFS中的新特性 76
3.4.1 NameNode HA 76
3.4.2 NameNode Federation 78
3.4.3 HDFS Snapshots 79
3.5 小结 79
第4章 YARN：统一资源管理和调平台 80
4.1 YARN是什么 80
4.2 统一资源管理和调度平台范型 81
4.2.1 集中式调度器 81
4.2.2 双层调度器 81
4.2.3 状态共享调度器 82
4.3 YARN的架构 82
4.3.1 ResourceManager 83
4.3.2 NodeManager 85
4.3.3 ApplicationMaster 87
4.3.4 YARN的资源表示模型Container 87
4.4 YARN的工作流程 88
4.5 YARN的调度器 89
4.5.1 YARN的资源管理机制 89
4.5.2 FIFO Scheduler 90
4.5.3 Capacity Scheduler 90
4.5.4 Fair Scheduler 91
4.6 YARN命令行 92
4.7 Apache Mesos 95
4.8 小结 96
第5章 分而治之的智慧：MapReduce 97
5.1 认识MapReduce 97
5.1.1 MapReduce的编程思想 98
5.1.2 MapReduce运行环境 100
5.1.3 MapReduce作业和任务 102
5.1.4 MapReduce的计算资源划分 102
5.1.5 MapReduce的局限性 103
5.2 Hello Word Count 104
5.2.1 Word Count的设计思路 104
5.2.2 编写Word Count 105
5.2.3 运行程序 107
5.2.4 还能更快吗 109
5.3 MapReduce的过程 109
5.3.1 从输入到输出 109
5.3.2 input 110
5.3.3 map及中间结果的输出 112
5.3.4 shuffle 113
5.3.5 reduce及最后结果的输出 115
5.3.6 sort 115
5.3.7 作业的进度组成 116
5.4 MapReduce的工作机制 116
5.4.1 作业提交 117
5.4.2 作业初始化 118
5.4.3 任务分配 118
5.4.4 任务执行 118
5.4.5 任务完成 118
5.4.6 推测执行 119
5.4.7 MapReduce容错 119
5.5 MapReduce编程 120
5.5.1 Writable类 120
5.5.2 编写Writable类 123
5.5.3 编写Mapper类 124
5.5.4 编写Reducer类 125
5.5.5 控制shuffle 126
5.5.6 控制sort 128
5.5.7 编写main函数 129
5.6 MapReduce编程实例：连接 130
5.6.1 设计思路 131
5.6.2 编写Mapper类 131
5.6.3 编写Reducer类 132
5.6.4 编写main函数 133
5.7 MapReduce编程实例：二次排序 134
5.7.1 设计思路 134
5.7.2 编写Mapper类 135
5.7.3 编写Partitioner类 136
5.7.4 编写SortComparator类 136
5.7.5 编写Reducer类 137
5.7.6 编写main函数 137
5.8 MapReduce编程实例：全排序 139
5.8.1 设计思路 139
5.8.2 编写代码 140
5.9 小结 141
第6章 SQL on Hadoop：Hive 142
6.1 认识Hive 142
6.1.1 从MapReduce到SQL 143
6.1.2 Hive架构 144
6.1.3 Hive与关系型数据库的区别 146
6.1.4 Hive命令的使用 147
6.2 数据类型和存储格式 149
6.2.1 基本数据类型 149
6.2.2 复杂数据类型 149
6.2.3 存储格式 150
6.2.4 数据格式 151
6.3 HQL：数据定义 152
6.3.1 Hive中的数据库 152
6.3.2 Hive中的表 154
6.3.3 创建表 154
6.3.4 管理表 156
6.3.5 外部表 156
6.3.6 分区表 156
6.3.7 删除表 158
6.3.8 修改表 158
6.4 HQL：数据操作 159
6.4.1 装载数据 159
6.4.2 通过查询语句向表中插入数据 160
6.4.3 利用动态分区向表中插入数据 160
6.4.4 通过CTAS加载数据 161
6.4.5 导出数据 161
6.5 HQL：数据查询 162
6.5.1 SELECT…FROM语句 162
6.5.2 WHERE语句 163
6.5.3 GROUP BY和HAVING语句 164
6.5.4 JOIN语句 164
6.5.5 ORDER BY和SORT BY语句 166
6.5.6 DISTRIBUTE BY和SORT BY语句 167
6.5.7 CLUSTER BY 167
6.5.8 分桶和抽样 168
6.5.9 UNION ALL 168
6.6 Hive函数 168
6.6.1 标准函数 168
6.6.2 聚合函数 168
6.6.3 表生成函数 169
6.7 Hive用户自定义函数 169
6.7.1 UDF 169
6.7.2 UDAF 170
6.7.3 UDTF 171
6.7.4 运行 173
6.8 小结 173
第7章 SQL to Hadoop : Sqoop 174
7.1 一个Sqoop示例 174
7.2 导入过程 176
7.3 导出过程 178
7.4 Sqoop的使用 179
7.4.1 codegen 180
7.4.2 create-hive-table 180
7.4.3 eval 181
7.4.4 export 181
7.4.5 help 182
7.4.6 import 182
7.4.7 import-all-tables 183
7.4.8 job 184
7.4.9 list-databases 184
7.4.10 list-tables 184
7.4.11 merge 184
7.4.12 metastore 185
7.4.13 version 186
7.5 小结 186
第8章 HBase:HadoopDatabase 187
8.1 酸和碱：两种数据库事务方法论 187
8.1.1 ACID 188
8.1.2 BASE 188
8.2 CAP定理 188
8.3 NoSQL的架构模式 189
8.3.1 键值存储 189
8.3.2 图存储 190
8.3.3 列族存储 191
8.3.4 文档存储 192
8.4 HBase的架构模式 193
8.4.1 行键、列族、列和单元格 193
8.4.2 HMaster 194
8.4.3 Region和RegionServer 195
8.4.4 WAL 195
8.4.5 HFile 195
8.4.6 Zookeeper 197
8.4.7 HBase架构 197
8.5 HBase写入和读取数据 198
8.5.1 Region定位 198
8.5.2 HBase写入数据 199
8.5.3 HBase读取数据 199
8.6 HBase基础API 200
8.6.1 创建表 201
8.6.2 插入 202
8.6.3 读取 203
8.6.4 扫描 204
8.6.5 删除单元格 206
8.6.6 删除表 207
8.7 HBase高级API 207
8.7.1 过滤器 208
8.7.2 计数器 208
8.7.3 协处理器 209
8.8 小结 214
第9章 Hadoop性能调优和运维 215
9.1 Hadoop客户端 215
9.2 Hadoop性能调优 216
9.2.1 选择合适的硬件 216
9.2.2 操作系统调优 218
9.2.3 JVM调优 219
9.2.4 Hadoop参数调优 219
9.3 Hive性能调优 225
9.3.1 JOIN优化 226
9.3.2 Reducer的数量 226
9.3.3 列裁剪 226
9.3.4 分区裁剪 226
9.3.5 GROUP BY优化 226
9.3.6 合并小文件 227
9.3.7 MULTI-GROUP BY和MULTI-INSERT 228
9.3.8 利用UNION ALL 特性 228
9.3.9 并行执行 228
9.3.10 全排序 228
9.3.11 Top N 229
9.4 HBase调优 229
9.4.1 通用调优 229
9.4.2 客户端调优 230
9.4.3 写调优 231
9.4.4 读调优 231
9.4.5 表设计调优 232
9.5 Hadoop运维 232
9.5.1 集群节点动态扩容和卸载 233
9.5.2 利用SecondaryNameNode恢复NameNode 234
9.5.3 常见的运维技巧 234
9.5.4 常见的异常处理 235
9.6 小结 236
应用篇：商业智能系统项目实战
第10章 在线图书销售商业智能系统 238
10.1 项目背景 238
10.2 功能需求 239
10.3 非功能需求 240
10.4 小结 240
第11章 系统结构设计 241
11.1 系统架构 241
11.2 功能设计 242
11.3 数据仓库结构 243
11.4 系统网络拓扑与硬件选型 246
11.4.1 系统网络拓扑 246
11.4.2 系统硬件选型 248
11.5 技术选型 249
11.5.1 平台选型 249
11.5.2 系统开发语言选型 249
11.6 小结 249
第12章 在开发之前 250
12.1 新建一个工程 250
12.1.1 安装Python 250
12.1.2 安装PyDev插件 251
12.1.3 新建PyDev项目 252
12.2 代码目录结构 253
12.3 项目的环境变量 253
12.4 如何调试 254
12.5 小结 254
第13章 实现数据导入导出模块 255
13.1 处理流程 255
13.2 导入方式 256
13.2.1 全量导入 256
13.2.2 增量导入 256
13.3 读取配置文件 257
13.4 SqoopUtil 261
13.5 整合 262
13.6 导入说明 262
13.7 导出模块 263
13.8 小结 265
第14章 实现数据分析工具模块 266
14.1 处理流程 266
14.2 读取配置文件 266
14.3 HiveUtil 268
14.4 整合 268
14.5 数据分析和报表 269
14.5.1 OLAP和Hive 269
14.5.2 OLAP和多维模型 270
14.5.3 选MySQL还是选HBase 272
14.6 小结 273
第15章 实现业务数据的数据清洗模块 274
15.1 ETL 274
15.1.1 数据抽取 274
15.1.2 数据转换 274
15.1.3 数据清洗工具 275
15.2 处理流程 275
15.3 数据去重 276
15.3.1 产生原因 276
15.3.2 去重方法 277
15.3.3 一个很有用的UDF： RowNum 277
15.3.4 第二种去重方法 279
15.3.5 进行去重 279
15.4 小结 282
第16章 实现点击流日志的数据清洗模块 283
16.1 数据仓库和Web 283
16.2 处理流程 285
16.3 字段的获取 285
16.4 编写MapReduce作业 288
16.4.1 编写IP地址解析器 288
16.4.2 编写Mapper类 291
16.4.3 编写Partitioner类 295
16.4.4 编写SortComparator类 295
16.4.5 编写Reducer类 297
16.4.6 编写main函数 298
16.4.7 通过Python调用jar文件 299
16.5 还能做什么 300
16.5.1 网站分析的指标 300
16.5.2 网站分析的决策支持 301
16.6 小结 301
第17章 实现购书转化率分析模块 302
17.1 漏斗模型 302
17.2 处理流程 303
17.3 读取配置文件 303
17.4 提取所需数据 304
17.5 编写转化率分析MapReduce作业 305
17.5.1 编写Mapper类 306
17.5.2 编写Partitioner类 308
17.5.3 编写SortComparator类 309
17.5.4 编写Reducer类 310
17.5.5 编写Driver类 312
17.5.6 通过Python模块调用jar文件 314
17.6 对中间结果进行汇总得到最终 结果 314
17.7 整合 316
17.8 小结 316
第18章 实现购书用户聚类模块 317
18.1 物以类聚 317
18.2 聚类算法 318
18.2.1 k-means算法 318
18.2.2 Canopy算法 319
18.2.3 数据向量化 320
18.2.4 数据归一化 321
18.2.5 相似性度量 322
18.3 用MapReduce实现聚类算法 323
18.3.1 Canopy算法与MapReduce 323
18.3.2 k-means算法与MapReduce 323
18.3.3 Apache Mahout 324
18.4 处理流程 324
18.5 提取数据并做归一化 325
18.6 维度相关性 327
18.6.1 维度的选取 327
18.6.2 相关系数与相关系数矩阵 328
18.6.3 计算相关系数矩阵 328
18.7 使用Mahout完成聚类 329
18.7.1 使用Mahout 329
18.7.2 解析Mahout的输出 332
18.7.3 得到聚类结果 334
18.8 得到最终结果 335
18.9 评估聚类结果 337
18.9.1 一份不适合聚类的数据 337
18.9.2 簇间距离和簇内距离 337
18.9.3 计算平均簇间距离 338
18.10 小结 339
第19章 实现调度模块 340
19.1 工作流 340
19.2 编写代码 341
19.3 crontab 342
19.4 让数据说话 343
19.5 小结 344
结束篇：总结和展望
第20章 总结和展望 346
20.1 总结 346
20.2 BDAS 347
20.3 Dremel系技术 348
20.4 Pregel系技术 349
20.5 Docker和Kubernetes 350
20.6 数据集成工具NiFi 350
20.7 小结 351
参考文献 352
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop海量数据处理（第2版） : 技术详解与项目实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据分析与挖掘实战
前　言
基　础　篇
第1章　数据挖掘基础2
1.1　某知名连锁餐饮企业的困惑2
1.2　从餐饮服务到数据挖掘3
1.3　数据挖掘的基本任务4
1.4　数据挖掘建模过程4
1.4.1　定义挖掘目标4
1.4.2　数据取样5
1.4.3　数据探索6
1.4.4　数据预处理12
1.4.5　挖掘建模14
1.4.6　模型评价14
1.5　餐饮服务中的大数据应用15
1.6　小结15
第2章　Hadoop基础16
2.1　概述16
2.1.1　Hadoop简介16
2.1.2　Hadoop生态系统17
2.2　安装与配置19
2.3　Hadoop原理26
2.3.1　Hadoop HDFS原理26
2.3.2　Hadoop MapReduce原理27
2.3.3　Hadoop YARN原理28
2.4　动手实践30
2.5　小结33
第3章　Hadoop生态系统：Hive34
3.1　概述34
3.1.1　Hive简介34
3.1.2　Hive安装与配置35
3.2　Hive原理38
3.2.1　Hive架构38
3.2.2　Hive的数据模型40
3.3　动手实践41
3.4　小结45
第4章　Hadoop生态系统：HBase46
4.1　概述46
4.1.1　HBase简介46
4.1.2　HBase安装与配置47
4.2　HBase原理50
4.2.1　HBase架构50
4.2.2　HBase与RDBMS51
4.2.3　HBase访问接口52
4.2.4　HBase数据模型53
4.3　动手实践54
4.4　小结61
第5章　大数据挖掘建模平台62
5.1　常用的大数据平台62
5.2　TipDM-HB大数据挖掘建模平台63
5.2.1　TipDM-HB大数据挖掘建模平台的功能63
5.2.2　TipDM-HB大数据挖掘建模平台操作流程及实例65
5.2.3　TipDM-HB大数据挖掘建模平台的特点67
5.3　小结68
第6章　挖掘建模69
6.1　分类与预测69
6.1.1　实现过程69
6.1.2　常用的分类与预测算法70
6.1.3　决策树71
6.1.4　Mahout中Random Forests算法的实现原理75
6.1.5　动手实践79
6.2　聚类分析83
6.2.1　常用聚类分析算法83
6.2.2　K-Means聚类算法84
6.2.3　Mahout中K-Means算法的实现原理88
6.2.4　动手实践90
6.3　关联规则93
6.3.1　常用的关联规则算法93
6.3.2　FP-Growth关联规则算法94
6.3.3　Mahout中Parallel Frequent Pattern Mining算法的实现原理98
6.3.4　动手实践100
6.4　协同过滤102
6.4.1　常用的协同过滤算法102
6.4.2　基于项目的协同过滤算法简介102
6.4.3　Mahout中Itembased Collaborative Filtering算法的实现原理103
6.4.4　动手实践106
6.5　小结109
实　战　篇
第7章　法律咨询数据分析与服务推荐112
7.1　背景与挖掘目标112
7.2　分析方法与过程114
7.2.1　数据抽取120
7.2.2　数据探索分析120
7.2.3　数据预处理125
7.2.4　模型构建130
7.3　上机实验139
7.4　拓展思考140
7.5　小结145
第8章　电商产品评论数据情感分析146
8.1　背景与挖掘目标146
8.2　分析方法与过程146
8.2.1　评论数据采集147
8.2.2　评论预处理150
8.2.3　文本评论分词155
8.2.4　构建模型155
8.3　上机实验167
8.4　拓展思考168
8.5　小结169
第9章　航空公司客户价值分析170
9.1　背景与挖掘目标170
9.2　分析方法与过程171
9.2.1　数据抽取174
9.2.2　数据探索分析174
9.2.3　数据预处理175
9.2.4　模型构建177
9.3　上机实验182
9.4　拓展思考183
9.5　小结183
第10章　基站定位数据商圈分析184
10.1　背景与挖掘目标184
10.2　分析方法与过程186
10.2.1　数据抽取186
10.2.2　数据探索分析187
10.2.3　数据预处理188
10.2.4　构建模型191
10.3　上机实验194
10.4　拓展思考195
10.5　小结195
第11章　互联网电影智能推荐196
11.1　背景与挖掘目标196
11.2　分析方法与过程197
11.2.1　数据抽取199
11.2.2　构建模型199
11.3　上机实验201
11.4　拓展思考202
11.5　小结203
第12章　家电故障备件储备预测分析204
12.1　背景与挖掘目标204
12.2　分析方法与过程206
12.2.1　数据探索分析207
12.2.2　数据预处理209
12.2.3　构建模型212
12.3　上机实验216
12.4　拓展思考217
12.5　小结217
第13章　市供水混凝投药量控制分析218
13.1　背景与挖掘目标218
13.2　分析方法与过程220
13.2.1　数据抽取221
13.2.2　数据探索分析221
13.2.3　数据预处理223
13.2.4　构建模型227
13.3　上机实验237
13.4　拓展思考238
13.5　小结239
第14章　基于图像处理的车辆压双黄线检测240
14.1　背景与挖掘目标240
14.2　分析方法与过程241
14.2.1　数据抽取242
14.2.2　数据探索分析242
14.2.3　数据预处理242
14.2.4　构建模型249
14.3　上机实验250
14.4　拓展思考250
14.5　小结251
高　级　篇
第15章　基于Mahout的大数据挖掘开发254
15.1　概述254
15.2　环境配置255
15.3　基于Mahout算法接口的二次开发258
15.3.1　Mahout算法实例258
15.3.2　Mahout算法接口的二次开发示例259
15.4　小结271
第16章　基于TipDM-HB的数据挖掘二次开发272
16.1　概述272
16.1.1　TipDM-HB大数据挖掘建模平台服务接口272
16.1.2　Apache CXF简介276
16.2　TipDM-HB大数据挖掘建模平台服务开发实例277
16.2.1　环境配置277
16.2.2　开发实例280
16.3　小结288
参考资料289
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据分析与挖掘实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战（第2版）
目录
前言
第1章　Hadoop简介/1
1.1　什么是Hadoop/2
1.1.1　Hadoop概述/2
1.1.2　Hadoop的历史/2
1.1.3　Hadoop的功能与作用/2
1.1.4　Hadoop的优势/3
1.1.5　Hadoop应用现状和发展趋势/3
1.2　Hadoop项目及其结构/3
1.3　Hadoop体系结构/6
1.4　Hadoop与分布式开发/7
1.5　Hadoop计算模型—MapReduce/10
1.6　Hadoop数据管理/10
1.6.1　HDFS的数据管理/10
1.6.2　HBase的数据管理/12
1.6.3　Hive的数据管理/13
1.7　Hadoop集群安全策略/15
1.8　本章小结/17
第2章　Hadoop的安装与配置/19
2.1　在Linux上安装与配置Hadoop/20
2.1.1　安装JDK 1.6/20
2.1.2　配置SSH免密码登录/21
2.1.3　安装并运行Hadoop/22
2.2　在Mac OSX上安装与配置Hadoop/24
2.2.1　安装Homebrew/24
2.2.2　使用Homebrew安装Hadoop/25
2.2.3　配置SSH和使用Hadoop/25
2.3　在Windows上安装与配置Hadoop/25
2.3.1　安装JDK 1.6或更高版本/25
2.3.2　安装Cygwin/25
2.3.3　配置环境变量/26
2.3.4　安装sshd服务/26
2.3.5　启动sshd服务/26
2.3.6　配置SSH免密码登录/26
2.3.7　安装并运行Hadoop/26
2.4　安装和配置Hadoop集群/27
2.4.1　网络拓扑/27
2.4.2　定义集群拓扑/27
2.4.3　建立和安装Cluster /28
2.5　日志分析及几个小技巧/34
2.6　本章小结/35
第3章　MapReduce计算模型/36
3.1　为什么要用MapReduce/37
3.2　MapReduce计算模型/38
3.2.1　MapReduce Job/38
3.2.2　Hadoop中的Hello World程序/38
3.2.3　MapReduce的数据流和控制流/46
3.3　MapReduce任务的优化/47
3.4　Hadoop流/49
3.4.1　Hadoop流的工作原理/50
3.4.2　Hadoop流的命令/51
3.4.3　两个例子/52
3.5　Hadoop Pipes/54
3.6　本章小结/56
第4章　开发MapReduce应用程序/57
4.1　系统参数的配置/58
4.2　配置开发环境/60
4.3　编写MapReduce程序/60
4.3.1　Map处理/60
4.3.2　Reduce处理/61
4.4　本地测试/62
4.5　运行MapReduce程序/62
4.5.1　打包/64
4.5.2　在本地模式下运行/64
4.5.3　在集群上运行/64
4.6　网络用户界面/65
4.6.1　JobTracker页面/65
4.6.2　工作页面/65
4.6.3　返回结果/66
4.6.4　任务页面/67
4.6.5　任务细节页面/67
4.7　性能调优/68
4.7.1　输入采用大文件/68
4.7.2　压缩文件/68
4.7.3　过滤数据/69
4.7.4　修改作业属性/71
4.8　MapReduce工作流/72
4.8.1　复杂的Map和Reduce函数/72
4.8.2　MapReduce Job中全局共享数据/74
4.8.3　链接MapReduce Job/75
4.9　本章小结/77
第5章　MapReduce应用案例/79
5.1　单词计数/80
5.1.1　实例描述/80
5.1.2　设计思路/80
5.1.3　程序代码/81
5.1.4　代码解读/82
5.1.5　程序执行/83
5.1.6　代码结果/83
5.1.7　代码数据流/84
5.2　数据去重/85
5.2.1　实例描述/85
5.2.2　设计思路/86
5.2.3　程序代码/86
5.3　排序/87
5.3.1　实例描述/87
5.3.2　设计思路/88
5.3.3　程序代码/89
5.4　单表关联/91
5.4.1　实例描述/91
5.4.2　设计思路/92
5.4.3　程序代码/92
5.5　多表关联/95
5.5.1　实例描述/95
5.5.2　设计思路/96
5.5.3　程序代码/96
5.6　本章小结/98
第6章　MapReduce工作机制/99
6.1　MapReduce作业的执行流程/100
6.1.1　MapReduce任务执行总流程/100
6.1.2　提交作业/101
6.1.3　初始化作业/103
6.1.4　分配任务/104
6.1.5　执行任务/106
6.1.6　更新任务执行进度和状态/107
6.1.7　完成作业/108
6.2　错误处理机制 /108
6.2.1　硬件故障/109
6.2.2　任务失败/109
6.3　作业调度机制/110
6.4　Shuffle和排序/111
6.4.1　Map端/111
6.4.2　Reduce端/113
6.4.3　shuffle过程的优化/114
6.5　任务执行/114
6.5.1　推测式执行/114
6.5.2　任务JVM重用/115
6.5.3　跳过坏记录/115
6.5.4　任务执行环境/116
6.6　本章小结/117
第7章　Hadoop I/O操作/118
7.1　I/O操作中的数据检查/119
7.2　数据的压缩 /126
7.2.1　Hadoop对压缩工具的选择/126
7.2.2　压缩分割和输入分割/127
7.2.3　在MapReduce程序中使用压缩/127
7.3　数据的I/O中序列化操作/128
7.3.1　Writable类/128
7.3.2　实现自己的Hadoop数据类型/137
7.4　针对Mapreduce的文件类/139
7.4.1　SequenceFile类/139
7.4.2　MapFile类/144
7.4.3　ArrayFile、SetFile和BloomMapFile/146
7.5　本章小结/148
第8章　下一代MapReduce：YARN/149
8.1　MapReduce V2设计需求/150
8.2　MapReduce V2主要思想和架构/151
8.3　MapReduce V2设计细节/153
8.4　MapReduce V2优势/156
8.5　本章小结/156
第9章　HDFS详解/157
9.1　Hadoop的文件系统/158
9.2　HDFS简介/160
9.3　HDFS体系结构/161
9.3.1　HDFS的相关概念/161
9.3.2　HDFS的体系结构/162
9.4　HDFS的基本操作/164
9.4.1　HDFS的命令行操作/164
9.4.2　HDFS的Web界面/165
9.5　HDFS常用Java API详解/166
9.5.1　使用Hadoop URL读取数据/166
9.5.2　使用FileSystem API读取数据/167
9.5.3　创建目录/169
9.5.4　写数据/169
9.5.5　删除数据/171
9.5.6　文件系统查询/171
9.6　HDFS中的读写数据流/175
9.6.1　文件的读取/175
9.6.2　文件的写入/176
9.6.3　一致性模型/178
9.7　HDFS命令详解/179
9.7.1　通过distcp进行并行复制/179
9.7.2　HDFS的平衡/180
9.7.3　使用Hadoop归档文件/180
9.7.4　其他命令/183
9.8　WebHDFS/186
9.8.1　WebHDFS的配置/186
9.8.2　WebHDFS命令/186
9.9　本章小结/190
第10章　Hadoop的管理/191
10.1　HDFS文件结构/192
10.2　Hadoop的状态监视和管理工具/196
10.2.1　审计日志/196
10.2.2　监控日志/196
10.2.3　Metrics/197
10.2.4　Java管理扩展 /199
10.2.5　Ganglia/200
10.2.6　Hadoop管理命令/202
10.3　Hadoop集群的维护/206
10.3.1　安全模式/206
10.3.2　Hadoop的备份/207
10.3.3　Hadoop的节点管理/208
10.3.4　系统升级/210
10.4　本章小结/212
第11章　Hive详解/213
11.1　Hive简介/214
11.1.1　Hive的数据存储/214
11.1.2　Hive的元数据存储/216
11.2　Hive的基本操作/216
11.2.1　在集群上安装Hive/216
11.2.2　配置MySQL存储Hive元数据/218
11.2.3　配置Hive/220
11.3　Hive QL详解/221
11.3.1　数据定义（DDL）操作/221
11.3.2　数据操作（DML）/231
11.3.3　SQL操作/233
11.3.4　Hive QL使用实例/235
11.4　Hive网络（Web UI）接口/237
11.4.1　Hive网络接口配置/237
11.4.2　Hive网络接口操作实例/238
11.5　Hive的JDBC接口//241
11.5.1　Eclipse环境配置/241
11.5.2　程序实例/241
11.6　Hive的优化/244
11.7　本章小结/246
第12章　HBase详解/247
12.1　HBase简介/248
12.2　HBase的基本操作/249
12.2.1　HBase的安装/249
12.2.2　运行HBase /253
12.2.3　HBase Shell/255
12.2.4　HBase配置/258
12.3　HBase体系结构/260
12.3.1　HRegion/260
12.3.2　HRegion服务器/261
12.3.3　HBase Master服务器/262
12.3.4　ROOT表和META表/262
12.3.5　ZooKeeper/263
12.4　HBase数据模型/263
12.4.1　数据模型/263
12.4.2　概念视图/264
12.4.3　物理视图/264
12.5　HBase与RDBMS/265
12.6　HBase与HDFS/266
12.7　HBase客户端/266
12.8　Java API /267
12.9　HBase编程 /273
12.9.1　使用Eclipse开发HBase应用程序/273
12.9.2　HBase编程/275
12.9.3　HBase与MapReduce/278
12.10　模式设计/280
12.10.1　模式设计应遵循的原则/280
12.10.2　学生表/281
12.10.3　事件表/282
12.11　本章小结/283
第13章　Mahout详解/284
13.1　Mahout简介/285
13.2　Mahout的安装和配置/285
13.3　Mahout API简介/288
13.4　Mahout中的频繁模式挖掘/290
13.4.1　什么是频繁模式挖掘/290
13.4.2　Mahout中的频繁模式挖掘/290
13.5　Mahout中的聚类和分类/292
13.5.1　什么是聚类和分类/292
13.5.2　Mahout中的数据表示/293
13.5.3　将文本转化成向量/294
13.5.4　Mahout中的聚类、分类算法/295
13.5.5　算法应用实例/299
13.6　Mahout应用：建立一个推荐引擎/304
13.6.1　推荐引擎简介/304
13.6.2　使用Taste构建一个简单的推荐引擎/305
13.6.3　简单分布式系统下基于产品的推荐系统简介/307
13.7　本章小结/309
第14章　Pig详解/310
14.1　Pig简介/311
14.2　Pig的安装和配置 /311
14.2.1　Pig的安装条件/311
14.2.2　Pig的下载、安装和配置/312
14.2.3　Pig运行模式/313
14.3　Pig Latin语言/315
14.3.1　Pig Latin语言简介/315
14.3.2　Pig Latin的使用/316
14.3.3　Pig Latin的数据类型/318
14.3.4　Pig Latin关键字/319
14.4　用户定义函数 /323
14.4.1　编写用户定义函数/324
14.4.2　使用用户定义函数/325
14.5　Zebra简介 /326
14.5.1　Zebra的安装/326
14.5.2　Zebra的使用简介/327
14.6　Pig实例 /328
14.6.1　Local模式/328
14.6.2　MapReduce模式/330
14.7　Pig进阶/331
14.7.1　数据实例/331
14.7.2　Pig数据分析/332
14.8　本章小结/336
第15章　ZooKeeper详解/337
15.1　ZooKeeper简介/338
15.1.1　ZooKeeper的设计目标/338
15.1.2　数据模型和层次命名空间/339
15.1.3　ZooKeeper中的节点和临时节点/339
15.1.4　ZooKeeper的应用/340
15.2　ZooKeeper的安装和配置/340
15.2.1　安装ZooKeeper /340
15.2.2　配置ZooKeeper/346
15.2.3　运行ZooKeeper/348
15.3　ZooKeeper的简单操作/350
15.3.1　使用ZooKeeper命令的简单操作步骤/350
15.3.2　ZooKeeper API的简单使用/352
15.4　ZooKeeper的特性/355
15.4.1　ZooKeeper的数据模型/355
15.4.2　ZooKeeper会话及状态/356
15.4.3　ZooKeeper watches/357
15.4.4　ZooKeeper ACL/358
15.4.5　ZooKeeper的一致性保证/359
15.5　使用ZooKeeper进行Leader选举/359
15.6　ZooKeeper锁服务/360
15.6.1　ZooKeeper中的锁机制/360
15.6.2　ZooKeeper提供的一个写锁的实现/361
15.7　使用ZooKeeper创建应用程序 /363
15.7.1　使用Eclipse开发ZooKeeper应用程序/363
15.7.2　应用程序实例/365
15.8　BooKeeper/369
15.9　本章小结/371
第16章　Avro详解/372
16.1　Avro介绍/373
16.1.1　模式声明/374
16.1.2　数据序列化/378
16.1.3　数据排列顺序/380
16.1.4　对象容器文件 /381
16.1.5　协议声明/382
16.1.6　协议传输格式/383
16.1.7　模式解析/386
16.2　Avro的C/C++实现/387
16.3　Avro的Java实现/398
16.4　GenAvro（Avro IDL）语言/402
16.5　Avro SASL概述/406
16.6　本章小结/407
第17章　Chukwa详解/409
17.1　Chukwa简介/410
17.2　Chukwa架构/411
17.2.1　客户端及其数据模型/412
17.2.2　收集器/413
17.2.3　归档器和分离解析器/414
17.2.4　HICC/415
17.3　Chukwa的可靠性/415
17.4　Chukwa集群搭建/416
17.4.1　基本配置要求/416
17.4.2　Chukwa的安装/416
17.4.3　Chukwa的运行/419
17.5　Chukwa数据流的处理/424
17.6　Chukwa与其他监控系统比较/425
17.7　本章小结/426
本章参考资料/426
第18章　Hadoop的常用插件与开发/428
18.1　Hadoop Studio的介绍和使用/429
18.1.1　Hadoop Studio的介绍/429
18.1.2　Hadoop Studio的安装配置/430
18.1.3　Hadoop Studio的使用举例/430
18.2　Hadoop Eclipse的介绍和使用/436
18.2.1　Hadoop Eclipse的介绍/436
18.2.2　Hadoop Eclipse的安装配置/437
18.2.3　Hadoop Eclipse的使用举例/438
18.3　Hadoop Streaming的介绍和使用/440
18.3.1　Hadoop Streaming的介绍/440
18.3.2　Hadoop Streaming的使用举例/444
18.3.3　使用Hadoop Streaming常见的问题/446
18.4　Hadoop Libhdfs的介绍和使用/448
18.4.1　Hadoop Libhdfs的介绍/448
18.4.2　Hadoop Libhdfs的安装配置/448
18.4.3　Hadoop Libhdfs API简介/448
18.4.4　Hadoop Libhdfs的使用举例/449
18.5　本章小结/450
第19章　企业应用实例/452
19.1　Hadoop在Yahoo!的应用/453
19.2　Hadoop在eBay的应用/455
19.3　Hadoop在百度的应用/457
19.4　即刻搜索中的Hadoop/460
19.4.1　即刻搜索简介/460
19.4.2　即刻Hadoop应用架构/460
19.4.3　即刻Hadoop应用分析/463
19.5　Facebook中的Hadoop和HBase/463
19.5.1　Facebook中的任务特点/464
19.5.2　MySQL VS Hadoop+HBase/466
19.5.3　Hadoop和HBase的实现/467
19.6　本章小结/472
本章参考资料/472
附录A　云计算在线检测平台/474
附录B　Hadoop安装、运行与使用说明/484
附录C　使用DistributedCache的MapReduce程序/491
附录D　使用ChainMapper和ChainReducer的MapReduce程序/495
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战（第2版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据挖掘从入门到进阶实战（视频教学版）
前言
第1章 集群及开发环境搭建 1
1.1 环境准备 1
1.1.1 基础软件下载 1
1.1.2 准备Linux操作系统 2
1.2 安装Hadoop 4
1.2.1 基础环境配置 4
1.2.2 Zookeeper部署 7
1.2.3 Hadoop部署 9
1.2.4 效果验证 21
1.2.5 集群架构详解 24
1.3 Hadoop版Hello World 25
1.3.1 Hadoop Shell介绍 25
1.3.2 WordCount初体验 27
1.4 开发环境 28
1.4.1 搭建本地开发环境 28
1.4.2 运行及调试预览 31
1.5 小结 34
第2章 实战：快速构建一个Hadoop项目并线上运行 35
2.1 构建一个简单的项目工程 35
2.1.1 构建Java Project结构工程 35
2.1.2 构建Maven结构工程 36
2.2 操作分布式文件系统（HDFS） 39
2.2.1 基本的应用接口操作 39
2.2.2 在高可用平台上的使用方法 42
2.3 利用IDE提交MapReduce作业 43
2.3.1 在单点上的操作 43
2.3.2 在高可用平台上的操作 46
2.4 编译应用程序并打包 51
2.4.1 编译Java Project工程并打包 51
2.4.2 编译Maven工程并打包 55
2.5 部署与调度 58
2.5.1 部署应用 58
2.5.2 调度任务 59
2.6 小结 60
第3章 Hadoop套件实战 61
3.1 Sqoop——数据传输工具 61
3.1.1 背景概述 61
3.1.2 安装及基本使用 62
3.1.3 实战：在关系型数据库与分布式文件系统之间传输数据 64
3.2 Flume——日志收集工具 66
3.2.1 背景概述 67
3.2.2 安装与基本使用 67
3.2.3 实战：收集系统日志并上传到分布式文件系统（HDFS）上 72
3.3 HBase——分布式数据库 74
3.3.1 背景概述 74
3.3.2 存储架构介绍 75
3.3.3 安装与基本使用 75
3.3.4 实战：对HBase业务表进行增、删、改、查操作 79
3.4 Zeppelin——数据集分析工具 85
3.4.1 背景概述 85
3.4.2 安装与基本使用 85
3.4.3 实战：使用解释器操作不同的数据处理引擎 88
3.5 Drill——低延时SQL查询引擎 92
3.5.1 背景概述 93
3.5.2 安装与基本使用 93
3.5.3 实战：对分布式文件系统（HDFS）使用SQL进行查询 95
3.5.4 实战：使用SQL查询HBase数据库 99
3.5.5 实战：对数据仓库（Hive）使用类实时统计、查询操作 101
3.6 Spark——实时流数据计算 104
3.6.1 背景概述 104
3.6.2 安装部署及使用 105
3.6.3 实战：对接Kafka消息数据，消费、计算及落地 108
3.7 小结 114
第4章 Hive编程——使用SQL提交MapReduce任务到Hadoop集群 115
4.1 环境准备与Hive初识 115
4.1.1 背景介绍 115
4.1.2 基础环境准备 116
4.1.3 Hive结构初识 116
4.1.4 Hive与关系型数据库（RDBMS） 118
4.2 安装与配置Hive 118
4.2.1 Hive集群基础架构 119
4.2.2 利用HAProxy实现Hive Server负载均衡 120
4.2.3 安装分布式Hive集群 123
4.3 可编程方式 126
4.3.1 数据类型 126
4.3.2 存储格式 128
4.3.3 基础命令 129
4.3.4 Java编程语言操作数据仓库（Hive） 131
4.3.5 实践Hive Streaming 134
4.4 运维和监控 138
4.4.1 基础命令 138
4.4.2 监控工具Hive Cube 140
4.5 小结 143
第5章 游戏玩家的用户行为分析——特征提取 144
5.1 项目应用概述 144
5.1.1 场景介绍 144
5.1.2 平台架构与数据采集 145
5.1.3 准备系统环境和软件 147
5.2 分析与设计 148
5.2.1 整体分析 148
5.2.2 指标与数据源分析 149
5.2.3 整体设计 151
5.3 技术选型 153
5.3.1 套件选取简述 154
5.3.2 套件使用简述 154
5.4 编码实践 157
5.4.1 实现代码 157
5.4.2 统计结果处理 163
5.4.3 应用调度 169
5.5 小结 174
第6章 Hadoop平台管理与维护 175
6.1 Hadoop分布式文件系统（HDFS） 175
6.1.1 HDFS特性 175
6.1.2 基础命令详解 176
6.1.3 解读NameNode Standby 179
6.2 Hadoop平台监控 182
6.2.1 Hadoop日志 183
6.2.2 常用分布式监控工具 187
6.3 平台维护 196
6.3.1 安全模式 196
6.3.2 节点管理 198
6.3.3 HDFS快照 200
6.4 小结 203
第7章 Hadoop异常处理解决方案 204
7.1 定位异常 204
7.1.1 跟踪日志 204
7.1.2 分析异常信息 208
7.1.3 阅读开发业务代码 209
7.2 解决问题的方式 210
7.2.1 搜索关键字 211
7.2.2 查看Hadoop JIRA 212
7.2.3 阅读相关源码 213
7.3 实战案例分析 216
7.3.1 案例分析1：启动HBase失败 216
7.3.2 案例分析2：HBase表查询失败 219
7.3.3 案例分析3：Spark的临时数据不自动清理 222
7.4 小结 223
第8章 初识Hadoop核心源码 224
8.1 基础准备与源码编译 224
8.1.1 准备环境 224
8.1.2 加载源码 228
8.1.3 编译源码 230
8.2 初识Hadoop 2 233
8.2.1 Hadoop的起源 233
8.2.2 Hadoop 2源码结构图 234
8.2.3 Hadoop模块包 235
8.3 MapReduce框架剖析 236
8.3.1 第一代MapReduce框架 236
8.3.2 第二代MapReduce框架 238
8.3.3 两代MapReduce框架的区别 239
8.3.4 第二代MapReduce框架的重构思路 240
8.4 序列化 241
8.4.1 序列化的由来 242
8.4.2 Hadoop序列化 243
8.4.3 Writable实现类 245
8.5 小结 247
第9章 Hadoop通信机制和内部协议 248
9.1 Hadoop RPC概述 248
9.1.1 通信模型 248
9.1.2 Hadoop RPC特点 250
9.2 Hadoop RPC的分析与使用 251
9.2.1 基础结构 251
9.2.2 使用示例 257
9.2.3 其他开源RPC框架 264
9.3 通信协议 266
9.3.1 MapReduce通信协议 266
9.3.2 RPC协议的实现 273
9.4 小结 277
第10章 Hadoop分布式文件系统剖析 278
10.1 HDFS介绍 278
10.1.1 HDFS概述 278
10.1.2 其他分布式文件系统 282
10.2 HDFS架构剖析 283
10.2.1 设计特点 283
10.2.2 命令空间和节点 285
10.2.3 数据备份剖析 289
10.3 数据迁移实战 292
10.3.1 HDFS跨集群迁移 292
10.3.2 HBase集群跨集群数据迁移 297
10.4 小结 301
第11章 ELK实战案例——游戏应用实时日志分析平台 302
11.1 Logstash——实时日志采集、分析和传输 302
11.1.1 Logstash介绍 302
11.1.2 Logstash安装 306
11.1.3 实战操作 308
11.2 Elasticsearch——分布式存储及搜索引擎 309
11.2.1 应用场景 309
11.2.2 基本概念 310
11.2.3 集群部署 312
11.2.4 实战操作 317
11.3 Kibana——可视化管理系统 323
11.3.1 Kibana特性 324
11.3.2 Kibana安装 324
11.3.3 实战操作 328
11.4 实时日志分析平台案例 331
11.4.1 案例概述 331
11.4.2 平台体系架构与剖析 332
11.4.3 实战操作 334
11.5 小结 339
第12章 Kafka实战案例——实时处理游戏用户数据 340
12.1 应用概述 340
12.1.1 Kafka回顾 340
12.1.2 项目简述 347
12.1.3 Kafka工程准备 348
12.2 项目的分析与设计 349
12.2.1 项目背景和价值概述 349
12.2.2 生产模块 350
12.2.3 消费模块 352
12.2.4 体系架构 352
12.3 项目的编码实践 354
12.3.1 生产模块 354
12.3.2 消费模块 356
12.3.3 数据持久化 362
12.3.4 应用调度 364
12.4 小结 369
第13章 Hadoop拓展——Kafka剖析 370
13.1 Kafka开发与维护 370
13.1.1 接口 370
13.1.2 新旧API编写 372
13.1.3 Kafka常用命令 380
13.2 运维监控 383
13.2.1 监控指标 384
13.2.2 Kafka开源监控工具——Kafka Eagle 384
13.3 Kafka源码分析 391
13.3.1 源码工程环境构建 391
13.3.2 分布式选举算法剖析 394
13.3.3 Kafka Offset解读 398
13.3.4 存储机制和副本 398
13.4 小结 402
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据挖掘从入门到进阶实战（视频教学版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南:大数据的存储与分析(第4版)(修订版)(升级版)
第Ⅰ部分 Hadoop基础知识
第1章 初识Hadoop 3
1.1 数据！数据！ 3
1.2 数据的存储与分析 5
1.3 查询所有数据 6
1.4 不仅仅是批处理 7
1.5 相较于其他系统的优势 8
1.5.1 关系型数据库管理系统 8
1.5.2 网格计算 10
1.5.3 志愿计算 11
1.6 Apache Hadoop发展简史 12
1.7 本书包含的内容 16
第2章 关于MapReduce 19
2.1 气象数据集 19
2.2 使用Unix工具来分析数据 21
2.3 使用Hadoop来分析数据 22
2.3.1 map和reduce 23
2.3.2 Java MapReduce 24
2.4 横向扩展 31
2.4.1 数据流 31
2.4.2 biner函数 35
2.4.3 运行分布式的MapReduce作业 37
2.5 Hadoop Streaming 37
2.5.1 Ruby版本 38
2.5.2 Python版本 40
第3章 Hadoop分布式文件系统 42
3.1 HDFS的设计 42
3.2 HDFS的概念 44
3.2.1 数据块 44
3.2.2 namenode和datanode 45
3.2.3 块缓存 46
3.2.4 联邦HDFS 47
3.2.5 HDFS的高可用性 47
3.3 命令行接口 50
3.4 Hadoop文件系统 52
3.5 Java接口 56
3.5.1 从Hadoop URL读取数据 56
3.5.2 通过FileSystem API读取数据 58
3.5.3 写入数据 61
3.5.4 目录 63
3.5.5 查询文件系统 63
3.5.6 删除数据 68
3.6 数据流 68
3.6.1 剖析文件读取 68
3.6.2 剖析文件写入 71
3.6.3 一致模型 74
3.7 通过distcp并行复制 76
第4章 关于YARN 78
4.1 剖析YARN应用运行机制 79
4.1.1 资源请求 80
4.1.2 应用生命期 81
4.1.3 构建YARN应用 81
4.2 YARN与MapReduce 1相比 82
4.3 YARN中的调度 85
4.3.1 调度选项 85
4.3.2 容量调度器配置 87
4.3.3 公平调度器配置 89
4.3.5 延迟调度 93
4.3.5 主导资源公平性 94
4.4 延伸阅读 95
第5章 Hadoop的I／O操作 96
5.1 数据完整性 96
5.1.1 HDFS的数据完整性 97
5.1.2 LocalFileSystem 98
5.1.3 ChecksumFileSystem 98
5.2 压缩 99
5.2.1 codec 100
5.2.2 压缩和输入分片 105
5.2.3 在MapReduce中使用压缩 106
5.3 序列化 109
5.3.1 Writable接口 110
5.3.2 Writable类 112
5.3.3 实现定制的Writable集合 121
5.3.4 序列化框架 125
5.4 基于文件的数据结构 127
5.4.1 关于SequenceFile 127
5.4.2 关于MapFile 135
5.4.3 其他文件格式和面向列的格式 136
第Ⅱ部分 关于MapReduce
第6章 MapReduce应用开发 141
6.1 用于配置的API 142
6.1.1 资源合并 143
6.1.2 变量扩展 144
6.2 配置开发环境 144
6.2.1 管理配置 146
6.2.2 辅助类GenericOptionsParser，Tool和ToolRunner 149
6.3 用MRUnit来写单元测试 152
6.3.1 关于Mapper 152
6.3.2 关于Reducer 156
6.4 本地运行测试数据 156
6.4.1 在本地作业运行器上运行作业 156
6.4.2 测试驱动程序 158
6.5 在集群上运行 160
6.5.1 打包作业 160
6.5.2 启动作业 162
6.5.3 MapReduce的Web界面 165
6.5.4 获取结果 167
6.5.5 作业调试 168
6.5.6 Hadoop日志 171
6.5.7 远程调试 173
6.6 作业调优 174
6.7 MapReduce的工作流 176
6.7.1 将问题分解成MapReduce作业 177
6.7.2 关于JobControl 178
6.7.3 关于Apache Oozie 179
第7章 MapReduce的工作机制 184
7.1 剖析MapReduce作业运行机制 184
7.1.1 作业的提交 185
7.1.2 作业的初始化 186
7.1.3 任务的分配 187
7.1.4 任务的执行 188
7.1.5 进度和状态的更新 189
7.1.6 作业的完成 191
7.2 失败 191
7.2.1 任务运行失败 191
7.2.2 application master运行失败 193
7.2.3 节点管理器运行失败 193
7.2.4 资源管理器运行失败 194
7.3 shuffle和排序 195
7.3.1 map端 195
7.3.2 reduce端 197
7.3.3 配置调优 199
7.4 任务的执行 201
7.4.1 任务执行环境 201
7.4.2 推测执行 202
7.4.3 关于OutputCommitters 204
第8章 MapReduce的类型与格式 207
8.1 MapReduce的类型 207
8.1.1 默认的MapReduce作业 212
8.1.2 默认的Streaming作业 216
8.2 输入格式 218
8.2.1 输入分片与记录 218
8.2.2 文本输入 229
8.2.3 二进制输入 233
8.2.4 多个输入 234
8.2.5 数据库输入（和输出） 235
8.3 输出格式 236
8.3.1 文本输出 236
8.3.2 二进制输出 237
8.3.3 多个输出 237
8.3.4 延迟输出 242
8.3.5 数据库输出 242
第9章 MapReduce的特性 243
9.1 计数器 243
9.1.1 内置计数器 243
9.1.2 用户定义的Java计数器 248
9.1.3 用户定义的Streaming计数器 251
9.2 排序 252
9.2.1 准备 252
9.2.2 部分排序 253
9.2.3 全排序 255
9.2.4 辅助排序 259
9.3 连接 264
9.3.1 map端连接 266
9.3.2 reduce端连接 266
9.4 边数据分布 270
9.4.1 利用JobConf来配置作业 270
9.4.2 分布式缓存 270
9.5 MapReduce库类 276
第Ⅲ部分 Hadoop的操作
第10章 构建Hadoop集群 279
10.1 集群规范 280
10.1.1 集群规模 281
10.1.2 网络拓扑 282
10.2 集群的构建和安装 284
10.2.1 安装Java 284
10.2.2 创建Unix 用户账号 284
10.2.3 安装Hadoop 284
10.2.4 SSH配置 285
10.2.5 配置Hadoop 286
10.2.6 格式化HDFS 文件系统 286
10.2.7 启动和停止守护进程 286
10.2.8 创建用户目录 288
10.3 Hadoop配置 288
10.3.1 配置管理 289
10.3.2 环境设置 290
10.3.3 Hadoop守护进程的关键属性 293
10.3.4 Hadoop守护进程的地址和端口 300
10.3.5 Hadoop的其他属性 303
10.4 安全性 305
10.4.1 Kerberos和Hadoop 306
10.4.2 委托令牌 308
10.4.3 其他安全性改进 309
10.5 利用基准评测程序测试Hadoop集群 311
10.5.1 Hadoop基准评测程序 311
10.5.2 用户作业 313
第11章 管理Hadoop 314
11.1 HDFS 314
11.1.1 永久性数据结构 314
11.1.2 安全模式 320
11.1.3 日志审计 322
11.1.4 工具 322
11.2 监控 327
11.2.1 日志 327
11.2.2 度量和JMX（Java管理扩展） 328
11.3 维护 329
11.3.1 日常管理过程 329
11.3.2 委任和解除节点 331
11.3.3 升级 334
第Ⅳ部分 Hadoop相关开源项目
第12章 关于Avro 341
12.1 Avro数据类型和模式 342
12.2 内存中的序列化和反序列化特定API 347
12.3 Avro数据文件 349
12.4 互操作性 351
12.4.1 Python API 351
12.4.2 Avro工具集 352
12.5 模式解析 352
12.6 排列顺序 354
12.7 关于Avro MapReduce 356
12.8 使用Avro MapReduce进行排序 359
12.9 其他语言的Avro 362
第13章 关于Parquet 363
13.1 数据模型 364
13.2 Parquet文件格式 367
13.3 Parquet的配置 368
13.4 Parquet文件的读／写 369
13.4.1 Avro、Protocol Buffers和Thrift 371
13.4.2 投影模式和读取模式 373
13.5 Parquet MapReduce 374
第14章 关于Flume 377
14.1 安装Flume 378
14.2 示例 378
14.3 事务和可靠性 380
14.4 HDFS Sink 382
14.5 扇出 385
14.5.1 交付保证 386
14.5.2 复制和复用选择器 387
14.6 通过代理层分发 387
14.7 Sink组 391
14.8 Flume与应用程序的集成 395
14.9 组件编目 395
14.10 延伸阅读 397
第15章 关于Sqoop 398
15.1 获取Sqoop 398
15.2 Sqoop连接器 400
15.3 一个导入的例子 401
15.4 生成代码 404
15.5 深入了解数据库导入 405
15.5.1 导入控制 407
15.5.2 导入和一致性 408
15.5.3 增量导入 408
15.5.4 直接模式导入 408
15.6 使用导入的数据 409
15.7 导入大对象 412
15.8 执行导出 414
15.9 深入了解导出功能 416
15.9.1 导出与事务 417
15.9.2 导出和SequenceFile 418
15.10 延伸阅读 419
第16章 关于Pig 420
16.1 安装与运行Pig 421
16.1.1 执行类型 422
16.1.2 运行Pig程序 423
16.1.3 Grunt 424
16.1.4 Pig Latin编辑器 424
16.2 示例 425
16.3 与数据库进行比较 428
16.4 PigLatin 429
16.4.1 结构 430
16.4.2 语句 431
16.4.3 表达式 436
16.4.4 类型 437
16.4.5 模式 438
16.4.6 函数 443
16.4.7 宏 445
16.5 用户自定义函数 446
16.5.1 过滤UDF 447
16.5.2 计算UDF 450
16.5.3 加载UDF 452
16.6 数据处理操作 455
16.6.1 数据的加载和存储 455
16.6.2 数据的过滤 455
16.6.3 数据的分组与连接 458
16.6.4 数据的排序 463
16.6.5 数据的组合和切分 465
16.7 Pig实战 465
16.7.1 并行处理 465
16.7.2 匿名关系 466
16.7.3 参数代换 467
16.8 延伸阅读 468
第17章 关于Hive 469
17.1 安装Hive 470
Hive的shell环境 471
17.2 示例 472
17.3 运行Hive 473
17.3.1 配置Hive 473
17.3.2 Hive服务 476
17.3.3 Metastore 478
17.4 Hive与传统数据库相比 480
17.4.1 读时模式vs.写时模式 480
17.4.2 更新、事务和索引 481
17.4.3 其他SQL—on—Hadoop技术 482
17.5 HiveQL 483
17.5.1 数据类型 484
17.5.2 操作与函数 487
17.6 表 488
17.6.1 托管表和外部表 488
17.6.2 分区和桶 490
17.6.3 存储格式 494
17.6.4 导入数据 498
17.6.5 表的修改 500
17.6.6 表的丢弃 501
17.7 查询数据 501
17.7.1 排序和聚集 501
17.7.2 MapReduce脚本 502
17.7.3 连接 503
17.7.4 子查询 506
17.7.5 视图 507
17.8 用户定义函数 508
17.8.1 写UDF 510
17.8.2 写UDAF 512
17.9 延伸阅读 516
第18章 关于Crunch 517
18.1 示例 518
18.2 Crunch核心API 521
18.2.1 基本操作 522
18.2.2 类型 527
18.2.3 源和目标 530
18.2.4 函数 532
18.2.5 物化 535
18.3 管线执行 537
18.3.1 运行管线 538
18.3.2 停止管线 539
18.3.3 查看Crunch计划 540
18.3.4 迭代算法 543
18.3.5 给管线设置检查点 544
18.4 Crunch库 545
18.5 延伸阅读 547
第19章 关于Spark 548
19.1 安装Spark 549
19.2 示例 549
19.2.1 Spark应用、作业、阶段和任务 551
19.2.2 Scala独立应用 552
19.2.3 Java示例 553
19.2.4 Python示例 554
19.3 弹性分布式数据集 555
19.3.1 创建 555
19.3.2 转换和动作 557
19.3.3 持久化 561
19.3.4 序列化 563
19.4 共享变量 564
19.4.1 广播变量 564
19.4.2 累加器 565
19.5 剖析Spark作业运行机制 565
19.5.1 作业提交 566
19.5.2 DAG的构建 566
19.5.3 任务调度 569
19.5.4 任务执行 570
19.6 执行器和集群管理器 570
19.7 延伸阅读 574
第20章 关于HBase 575
20.1 HBase基础 575
20.2 概念 576
20.2.1 数据模型的“旋风之旅” 576
20.2.2 实现 578
20.3 安装 581
20.4 客户端 584
20.4.1 Java 584
20.4.2 MapReduce 588
20.4.3 REST和Thrift 589
20.5 创建在线查询应用 589
20.5.1 模式设计 590
20.5.2 加载数据 591
20.5.3 在线查询 595
20.6 HBase和RDBMS的比较 598
20.6.1 成功的服务 599
20.6.2 HBase 600
20.7 Praxis 601
20.7.1 HDFS 601
20.7.2 用户界面 602
20.7.3 度量 602
20.7.4 计数器 602
20.8 延伸阅读 602
第21章 关于ZooKeeper 604
21.1 安装和运行ZooKeeper 605
21.2 示例 607
21.2.1 ZooKeeper中的组成员关系 608
21.2.2 创建组 608
21.2.3 加入组 611
21.2.4 列出组成员 612
21.2.5 删除组 614
21.3 ZooKeeper服务 615
21.3.1 数据模型 615
21.3.2 操作 618
21.3.3 实现 622
21.3.4 一致性 624
21.3.5 会话 626
21.3.6 状态 628
21.4 使用ZooKeeper来构建应用 629
21.4.1 配置服务 629
21.4.2 可复原的ZooKeeper应用 633
21.4.3 锁服务 637
21.4.4 更多分布式数据结构和协议 639
21.5 生产环境中的ZooKeeper 640
21.5.1 可恢复性和性能 641
21.5.2 配置 642
21.6 延伸阅读 643
第Ⅴ部分 案例学习
第22章 医疗公司塞纳（Cerner）的可聚合数据 647
22.1 从多CPU到语义集成 647
22.2 进入Apache Crunch 648
22.3 建立全貌 649
22.4 集成健康医疗数据 651
22.5 框架之上的可组合性 654
22.6 下一步 655
第23章 生物数据科学：用软件拯救生命 657
23.1 DNA的结构 659
23.2 遗传密码：将DNA字符转译为蛋白质 660
22.3 将DNA想象成源代码 661
23.4 人类基因组计划和参考基因组 663
22.5 DNA测序和比对 664
23.6 ADAM，一个可扩展的基因组分析平台 666
23.7 使用Avro接口描述语言进行自然语言编程 666
23.8 使用Parquet进行面向列的存取 668
23.9 一个简单例子：用Spark和ADAM做k—mer计数 669
23.10 从个性化广告到个性化医疗 672
23.11 联系我们 673
第24章 开源项目Cascading 674
24.1 字段、元组和管道 675
24.2 操作 678
24.3 Taps，Schemes和Flows 680
24.4 Cascading实践应用 681
24.5 灵活性 684
24.6 ShareThis中的Hadoop和Cascading 685
24.7 总结 689
附录A 安装Apache Hadoop 691
附录B 关于CDH 697
附录C 准备NCDC气象数据 699
附录D 新版和旧版JavaMapReduce API 702
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南:大数据的存储与分析(第4版)(修订版)(升级版)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop安全 : 大数据平台隐私保护
序　　xi
前言　　xii
第1章　引言　　1
1.1　安全概览　　1
1.1.1　机密性　　2
1.1.2　完整性　　2
1.1.3　可用性　　2
1.1.4　验证、授权和审计　　3
1.2　Hadoop 安全：简史　　5
1.3　Hadoop 组件和生态系统　　5
1.3.1　Apache HDFS　　6
1.3.2　Apache YARN　　7
1.3.3　Apache MapReduce　　8
1.3.4　Apache Hive　　9
1.3.5　Cloudera Impala　　9
1.3.6　Apache Sentry　　10
1.3.7　Apache　HBase　　11
1.3.8　Apache Accumulo　　11
1.3.9　Apache Solr　　13
1.3.10　Apache Oozie　　13
1.3.11　Apache ZooKeeper　　13
1.3.12　Apache Flume　　13
1.3.13　Apache Sqoop　　14
1.3.14　Cloudera　Hue　　14
1.4　小结　　14
第一部分　安全架构
第2章　保护分布式系统　　16
2.1　威胁种类　　17
2.1.1　非授权访问／伪装　　17
2.1.2　内在威胁　　17
2.1.3　拒绝服务　　18
2.1.4　数据威胁　　18
2.2　威胁和风险评估　　18
2.2.1　用户评估　　19
2.2.2　环境评估　　19
2.3　漏洞　　19
2.4　深度防御　　20
2.5　小结　　21
第3章　系统架构　　22
3.1　运行环境　　22
3.2　网络安全　　23
3.2.1　网络划分　　23
3.2.2　网络防火墙　　24
3.2.3　入侵检测和防御　　25
3.3　Hadoop 角色和隔离策略　　27
3.3.1　主节点　　28
3.3.2　工作节点　　29
3.3.3　管理节点　　29
3.3.4　边界节点　　30
3.4　操作系统安全　　31
3.4.1　远程访问控制　　31
3.4.2　主机防火墙　　31
3.4.3　SELinux　　33
3.5　小结　　34
第4章　Kerberos　　35
4.1　为什么是Kerberos　　35
4.2　Kerberos 概览　　36
4.3　Kerberos 工作流：一个简单示例　　37
4.4　Kerberos 信任　　38
4.5　MIT Kerberos　　39
4.5.1　服务端配置　　41
4.5.2　客户端配置　　44
4.6　小结　　46
第二部分　验证、授权和审计
第5章　身份和验证　　48
5.1　身份　　48
5.1.1　将Kerberos 主体映射为用户名　　49
5.1.2　Hadoop 用户到组的映射　　50
5.1.3　Hadoop 用户配置　　54
5.2　身份验证　　54
5.2.1　Kerberos　　55
5.2.2　用户名和密码验证　　56
5.2.3　令牌　　56
5.2.4　用户模拟　　59
5.2.5　配置　　60
5.3　小结　　70
第6章　授权　　71
6.1　HDFS 授权　　71
HDFS 扩展ACL　　72
6.2　服务级授权　　74
6.3　MapReduce 和YARN 的授权　　85
6.3.1　MapReduce（MR1）　　86
6.3.2　YARN　(MR2)　　87
6.6　HBase 和Accumulo 的授权　　95
6.6.1　系统、命名空间和表级授权　　95
6.6.2　列级别和单元级别授权　　99
6.7　小结　　99
第7章　Apache Sentry（孵化中）　　100
7.1　Sentry 概念　　100
7.2　Sentry 服务　　102
7.3　Hive 授权　　105
7.4　Impala 授权　　110
7.5　Solr 授权　　112
7.6　Sentry 特权模型　　113
7.6.1　SQL 特权模型　　114
7.6.2　Solr 特权模型　　116
7.7　Sentry 策略管理　　118
7.7.1　SQL 命令　　118
7.7.2　SQL 策略文件　　121
7.7.3　Solr 策略文件　　123
7.7.4　策略文件的验证和校验　　124
7.7.5　从策略文件迁移　　126
7.8　小结　　127
第8章　审计　　128
8.1　HDFS 审计日志　　129
8.2　MapReduce 审计日志　　130
8.3　YARN 审计日志　　132
8.4　Hive 审计日志　　134
8.5　Cloudera　Impala 审计日志　　134
8.6　HBase 审计日志　　135
8.7　Accumulo 审计日志　　137
8.8　Sentry 审计日志　　139
8.9　日志聚合　　140
8.10　小结　　141
第三部分　数据安全
第9章　数据保护　　144
9.1　加密算法　　144
9.2　静态数据加密　　145
9.2.1　加密和密钥管理　　146
9.2.2　HDFS 静态数据加密　　146
9.2.3　MapReduce2 中间数据加密　　151
9.2.4　Impala 磁盘溢出加密　　152
9.2.5　全盘加密　　152
9.2.6　文件系统加密　　154
9.2.7　Hadoop 中重要数据的安全考虑　　155
9.3　动态数据加密　　156
9.3.1　传输层安全　　156
9.3.2　Hadoop 动态数据加密　　157
9.4　数据销毁和删除　　162
9.5　小结　　163
第10章　数据导入安全　　164
10.1　导入数据的完整性　　165
10.2　数据导入的机密性　　166
10.2.1　Flume 加密　　167
10.2.2　Sqoop 加密　　173
10.3　导入工作流　　178
10.4　企业架构　　179
10.5　小结　　180
第11章　数据提取和客户端访问安全　　181
11.1　Hadoop 命令行接口　　182
11.2　保护应用安全　　183
11.3　HBase　　184
11.3.1　HBase shell　　184
11.3.2　HBase REST 网关　　186
11.3.3　HBase Thrift 网关　　189
11.4　Accumulo　　190
11.4.1　Accumulo shell　　190
11.4.2　Accumulo 代理服务　　192
11.5　Oozie　　192
11.6　Sqoop　　194
11.7　SQL 访问　　195
11.7.1　Impala　　195
11.7.2　Hive　　200
11.8　WebHDFS/HttpFS　　208
11.9　小结　　209
第12章　Cloudera Hue　　210
12.1　Hue HTTPS　　211
12.2　Hue 身份验证　　212
12.2.1　SPNEGO 后端　　212
12.2.2　SAML 后端　　213
12.2.3　LDAP 后端　　215
12.3　Hue 授权　　218
12.4　Hue SSL 客户端配置　　219
12.5　小结　　219
第四部分　综合应用
第13章　案例分析　　222
13.1　案例分析：Hadoop 数据仓库　　222
13.1.1　环境搭建　　223
13.1.2　用户体验　　226
13.1.3　小结　　229
13.2　案例分析：交互式HBase　Web 应用　　230
13.2.1　设计与架构　　230
13.2.2　安全需求　　231
13.2.3　集群配置　　232
13.2.4　实现中的注意事项　　236
13.2.5　小结　　237
后记　　238
关于作者　　240
关于封面　　240
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop安全 : 大数据平台隐私保护
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度剖析Hadoop HDFS
前言
第一部分 核心设计篇
第1章 HDFS的数据存储2
1.1 HDFS内存存储2
1.1.1 HDFS内存存储原理2
1.1.2 Linux 虚拟内存盘4
1.1.3 HDFS的内存存储流程分析4
1.1.4 LAZY_PERSIST内存存储的使用14
1.2 HDFS异构存储15
1.2.1 异构存储类型16
1.2.2 异构存储原理17
1.2.3 块存储类型选择策略22
1.2.4 块存储策略集合24
1.2.5 块存储策略的调用27
1.2.6 HDFS异构存储策略的不足之处28
1.2.7 HDFS存储策略的使用30
1.3 小结31
第2章 HDFS的数据管理与策略选择32
2.1 HDFS缓存与缓存块32
2.1.1 HDFS物理层面缓存块33
2.1.2 缓存块的生命周期状态34
2.1.3 CacheBlock、UnCacheBlock场景触发36
2.1.4 CacheBlock、UnCacheBlock缓存块的确定38
2.1.5 系统持有的缓存块列表如何更新39
2.1.6 缓存块的使用40
2.1.7 HDFS缓存相关配置40
2.2 HDFS中心缓存管理42
2.2.1 HDFS缓存适用场景43
2.2.2 HDFS缓存的结构设计43
2.2.3 HDFS缓存管理机制分析45
2.2.4 HDFS中心缓存疑问点55
2.2.5 HDFS CacheAdmin命令使用56
2.3 HDFS快照管理58
2.3.1 快照概念59
2.3.2 HDFS中的快照相关命令59
2.3.3 HDFS内部的快照管理机制60
2.3.4 HDFS的快照使用71
2.4 HDFS副本放置策略72
2.4.1 副本放置策略概念与方法72
2.4.2 副本放置策略的有效前提73
2.4.3 默认副本放置策略的分析73
2.4.4 目标存储好坏的判断82
2.4.5 chooseTargets的调用83
2.4.6 BlockPlacementPolicyWithNodeGroup继承类84
2.4.7 副本放置策略的结果验证85
2.5 HDFS内部的认证机制85
2.5.1 BlockToken认证85
2.5.2 HDFS的Sasl认证91
2.5.3 BlockToken认证与HDFS的Sasl认证对比97
2.6 HDFS内部的磁盘目录服务98
2.6.1 HDFS的三大磁盘目录检测扫描服务98
2.6.2 DiskChecker：坏盘检测服务99
2.6.3 DirectoryScanner：目录扫描服务104
2.6.4 VolumeScanner：磁盘目录扫描服务110
2.7 小结116
第3章 HDFS的新颖功能特性117
3.1 HDFS视图文件系统：ViewFileSystem117
3.1.1 ViewFileSystem： 视图文件系统118
3.1.2 ViewFileSystem内部实现原理119
3.1.3 ViewFileSystem的使用125
3.2 HDFS的Web文件系统：WebHdfsFileSystem126
3.2.1 WebHdfsFileSystem的REST API操作127
3.2.2 WebHdfsFileSystem的流程调用129
3.2.3 WebHdfsFileSystem执行器调用130
3.2.4 WebHDFS的OAuth2认证133
3.2.5 WebHDFS的使用135
3.3 HDFS数据加密空间：Encryption zone136
3.3.1 Encryption zone原理介绍136
3.3.2 Encryption zone源码实现136
3.3.3 Encryption zone的使用144
3.4 HDFS纠删码技术145
3.4.1 纠删码概念145
3.4.2 纠删码技术的优劣势146
3.4.3 Hadoop纠删码概述147
3.4.4 纠删码技术在Hadoop中的实现148
3.5 HDFS对象存储：Ozone152
3.5.1 Ozone介绍153
3.5.2 Ozone的高层级设计154
3.5.3 Ozone的实现细节157
3.5.4 Ozone的使用157
3.6 小结158
第二部分 细节实现篇
第4章 HDFS的块处理160
4.1 HDFS块检查命令fsck160
4.1.1 fsck参数使用160
4.1.2 fsck过程调用161
4.1.3 fsck原理分析162
4.1.4 fsck使用场景171
4.2 HDFS如何检测并删除多余副本块171
4.2.1 多余副本块以及发生的场景172
4.2.2 OverReplication多余副本块处理172
4.2.3 多余副本块清除的场景调用177
4.3 HDFS数据块的汇报与处理179
4.3.1 块处理的五大类型179
4.3.2 toAdd：新添加的块181
4.3.3 toRemove：待移除的块184
4.3.4 toInvalidate：无效的块186
4.3.5 toCorrupt：损坏的块189
4.3.6 toUC：正在构建中的块191
4.4 小结193
第5章 HDFS的流量处理194
5.1 HDFS的内部限流194
5.1.1 数据的限流194
5.1.2 DataTransferThrottler限流原理196
5.1.3 数据流限流在Hadoop中的使用198
5.1.4 Hadoop限流优化点202
5.2 数据平衡204
5.2.1 Balancer和Dispatcher204
5.2.2 数据不平衡现象207
5.2.3 Balancer性能优化207
5.3 HDFS节点内数据平衡210
5.3.1 磁盘间数据不平衡现象及问题211
5.3.2 传统的磁盘间数据不平衡解决方案211
5.3.3 社区解决方案：DiskBalancer212
5.4 小结216
第6章 HDFS的部分结构分析217
6.1 HDFS镜像文件的解析与反解析217
6.1.1 HDFS的FsImage镜像文件218
6.1.2 FsImage的解析218
6.1.3 FsImage的反解析221
6.1.4 HDFS镜像文件的解析与反解析命令226
6.2 DataNode数据处理中心DataXceiver227
6.2.1 DataXceiver的定义和结构228
6.2.2 DataXceiver下游处理方法232
6.2.3 ShortCircuit232
6.2.4 DataXceiver的上游调用233
6.2.5 DataXceiver与DataXceiverServer234
6.3 HDFS邻近信息块：BlockInfoContiguous235
6.3.1 triplets对象数组236
6.3.2 BlockInfoContiguous的链表操作239
6.3.3 块迭代器BlockIterator244
6.4 小结246
第三部分 解决方案篇
第7章 HDFS的数据管理248
7.1 HDFS的读写限流方案248
7.1.1 限流方案实现要点以及可能造成的影响248
7.1.2 限流方案实现249
7.1.3 限流测试结果250
7.2 HDFS数据资源使用量分析以及趋势预测250
7.2.1 要获取哪些数据251
7.2.2 如何获取这些数据251
7.2.3 怎么用这些数据254
7.3 HDFS数据迁移解决方案257
7.3.1 数据迁移使用场景257
7.3.2 数据迁移要素考量258
7.3.3 HDFS数据迁移解决方案：DistCp259
7.3.4 DistCp优势特性260
7.3.5 Hadoop DistCp命令264
7.3.6 DistCp解决集群间数据迁移实例265
7.4 DataNode迁移方案265
7.4.1 迁移方案的目标266
7.4.2 DataNode更换主机名、ip地址时的迁移方案267
7.5 HDFS集群重命名方案268
7.6 HDFS的配置管理方案271
7.6.1 HDFS配置管理的问题271
7.6.2 现有配置管理工具272
7.6.3 运用Git来做配置管理272
7.7 小结273
第8章 HDFS的数据读写274
8.1 DataNode引用计数磁盘选择策略274
8.1.1 HDFS现有磁盘选择策略274
8.1.2 自定义磁盘选择策略279
8.2 Hadoop节点“慢磁盘”监控282
8.2.1 慢磁盘的定义以及如何发现282
8.2.2 慢磁盘监控284
8.3 小结287
第9章 HDFS的异常场景288
9.1 DataNode慢启动问题288
9.1.1 DataNode慢启动现象288
9.1.2 代码追踪分析290
9.1.3 参数可配置化改造293
9.2 Hadoop中止下线操作后大量剩余复制块问题295
9.2.1 节点下线操作的含义及问题295
9.2.2 死节点“复活”297
9.2.3 Decommission下线操作如何运作299
9.2.4 中止下线操作后移除残余副本块解决方案303
9.3 DFSOutputStream的DataStreamer线程泄漏问题306
9.3.1 DFSOutputStream写数据过程及周边相关类、变量306
9.3.2 DataStreamer数据流对象307
9.3.3 ResponseProcessor回复获取类311
9.3.4 DataStreamer与DFSOutputStream的关系313
9.3.5 Streamer线程泄漏问题316
9.4 小结319
附录 如何向开源社区提交自己的代码320
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度剖析Hadoop HDFS
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>hadoop技术详解
第1章 简介 1
第2章 HDFS 6
2.1　目标和动机 6
2.2　设计 7
2.3　守护进程 8
2.4　读写数据 10
2.4.1　数据读取流程 10
2.4.2　数据写操作流程 11
2.5　管理文件系统元数据 13
2.6　NameNode的高可用性 14
2.7　NameNode联盟 16
2.8　访问与集成 17
2.8.1　命令行工具 18
2.8.2　用户空间文件系统（FUSE） 21
2.8.3　表示状态传输（REST）的支持 21
第3章 MapReduce 23
3.1　MapReduce的若干阶段 24
3.2　Hadoop MapReduce简介 30
3.2.1　后台程序 31
3.2.2　出错处理 33
3.3　YARN 35
第4章 规划一个Hadoop集群 37
4.1　挑选Hadoop的发行版本 37
4.1.1　Apache Hadoop 37
4.1.2　Cloudera的Apache Hadoop发行版本 38
4.1.3　版本和功能 38
4.1.4　我应该使用哪个版本 40
4.2　硬件选型 41
4.2.1　主节点硬件的选择 42
4.2.2　工作节点的硬件选择 43
4.2.3　集群的大小 45
4.2.4　刀片服务器、存储区域网络（SAN）和虚拟化 47
4.3　操作系统的选择和准备 49
4.3.1　部署规划 49
4.3.2　软件 50
4.3.3　主机名、DNS和标识 51
4.3.4　用户、组和特权 54
4.4　内核调整 56
4.4.1　vm.swappiness 56
4.4.2　vm.overcommit_memory 57
4.5　磁盘配置 58
4.5.1　选择文件系统 58
4.5.2　挂载选项 60
4.6　网络设计 60
4.6.1　Hadoop中的网络使用：回顾 60
4.6.2　1 Gb与10 Gb网络 62
4.6.3　典型的网络拓扑 63
第5章　安装和配置 67
5.1　安装Hadoop 67
5.1.1　Apache Hadoop 68
5.1.2　CDH 72
5.2　配置概述 76
5.3　环境变量和Shell脚本 80
5.4　日志配置 82
5.5　HDFS 84
5.5.1　识别和定位 84
5.5.2　优化与调整 86
5.5.3　格式化NameNode 89
5.5.4　创建/tmp目录 91
5.6　NameNode的高可靠性 92
5.6.1　隔离（Fencing）选项 93
5.6.2　基本配置 95
5.6.3　自动失效备援配置 96
5.6.4　格式化和引导NameNode启动 99
5.7　NameNode联盟（Federation） 105
5.8　MapReduce 113
5.8.1　识别和定位 113
5.8.2　优化和调整 115
5.9　机架拓扑 122
5.10　安全 125
第6章　用户标识、身份验证和授权 126
6.1　用户标识 127
6.2　Kerberos和Hadoop 128
6.2.1　Kerberos 128
6.2.2　Hadoop上的Kerberos支持 130
6.3　授权 143
6.3.1　HDFS 144
6.3.2　MapReduce 146
6.3.3　其他工具和系统 149
6.4　集成试试 153
第7章　资源管理 156
7.1　何谓资源管理 156
7.2　HDFS配额 156
7.3　MapReduce 调度器 159
7.3.1　先进先出（FIFO）调度器 160
7.3.2　公平调度器 162
7.3.3　计算能力调度器（Capacity Scheduler） 174
7.3.4　未来发展 181
第8章　集群维护 183
8.1　Hadoop流程管理 183
8.1.1　用初始化脚本管理进程 183
8.1.2　手动管理进程 184
8.2　HDFS维护任务 184
8.2.1　添加一个DataNode 184
8.2.2　卸载DataNode 185
8.2.3　用fsck来检查文件系统的一致性 185
8.2.4　HDFS块数据均衡 190
8.2.5　处理坏磁盘 192
8.3　MapReduce维护任务 193
8.3.1　添加tasktracker 193
8.3.2　卸载tasktracker 193
8.3.3　终结MapReduce 作业 194
8.3.4　终结MapReduce任务 194
8.3.5　处理列入黑名单的tasktracker 195
第9章　故障分析与排查 196
9.1　鉴别诊断（Differential Diagnosis） 196
9.2 故障和问题 197
9.2.1　人类（自己） 198
9.2.2　配置错误 198
9.2.3　硬件故障 199
9.2.4　资源枯竭 200
9.2.5　主机标识和命名 200
9.2.6　网络分区 200
9.3 “计算机插好了么？” 201
9.4　治疗和护理 203
9.5　实战案例 206
9.5.1　神秘的瓶颈 206
9.5.2　127.0.0.1这个地址不存在 209
第10章　监控 213
10.1　概览 213
10.2　Hadoop度量(Metrics) 214
10.2.1　Apache Hadoop 0.20.0和CDH3 (metrics1) 214
10.2.2　Apache Hadoop 0.20.203及之后的版本、CDH4(metrics2) 221
10.2.3　SNMP 222
10.3　健康监控 222
10.3.1　主机级别的检查 223
10.3.2　所有Hadoop进程 225
10.3.3　HDFS检查 226
10.3.4　MapReduce检查 229
第11章　备份与恢复 232
11.1　数据备份 232
11.1.1　分布式拷贝（distcp） 233
11.1.2　并行提取数据 235
11.2　NameNode元数据 237
附录　弃用的配置属性 239
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>hadoop技术详解
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>R与Hadoop大数据分析实战
目　　录
译者序
前言
审校者简介
致谢
第1章　R和Hadoop入门 1
1.1　安装R 2
1.2　安装RStudio 3
1.3　R语言的功能特征 3
1.3.1　使用R程序包 3
1.3.2　执行数据操作 3
1.3.3　日渐增多的社区支持 4
1.3.4　R语言数据建模 4
1.4　Hadoop的安装 5
1.4.1　不同的Hadoop模式 6
1.4.2　Hadoop的安装步骤 6
1.5　Hadoop的特点 12
1.5.1　HDFS简介 13
1.5.2　MapReduce简介 13
1.6　HDFS和MapReduce架构 14
1.6.1　HDFS架构 14
1.6.2　MapReduce架构 15
1.6.3　通过图示了解HDFS和MapReduce架构 15
1.7　Hadoop的子项目 16
1.8　小结 19
第2章　编写Hadoop MapReduce程序 20
2.1　MapReduce基础概念 20
2.2　Hadoop MapReduce技术简介 22
2.2.1　MapReduce中包含的实体 22
2.2.2　MapReduce中的主要执行进程 23
2.2.3　MapReduce的局限 25
2.2.4　MapReduce 可以解决的问题 26
2.2.5　使用Hadoop编程时用到不同的Java概念 26
2.3　Hadoop MapReduce原理 27
2.3.1　MapReduce对象 27
2.3.2　MapReduce中实现Map阶段的执行单元数目 28
2.3.3　MapReduce中实现Reduce阶段的执行单元数目 28
2.3.4　MapReduce的数据流 28
2.3.5　深入理解HadoopMapReduce 30
2.4　编写Hadoop MapReduce示例程序 32
2.4.1　MapReduce job运行的步骤 33
2.4.2　MapReduce可解决的商业问题 38
2.5　在R环境中编写Hadoop MapReduce程序的方式 39
2.5.1　RHadoop 39
2.5.2　RHIPE 40
2.5.3　Hadoop streaming 40
2.6　小结 40
第3章　集成R和Hadoop 41
3.1　RHIPE 42
3.1.1　安装RHIPE 42
3.1.2　RHIPE架构 44
3.1.3　RHIPE实例 45
3.1.4　RHIPE参考函数 48
3.2　RHadoop 51
3.2.1　RHadoop架构 51
3.2.2　安装RHadoop 52
3.2.3　RHadoop案例 53
3.2.4　RHadoop参考函数 56
3.3　小结 58
第4章　Hadoop Streaming中使用R 59
4.1　Hadoop Streaming基础概念 59
4.2　使用R运行Hadoop streaming 62
4.2.1　MapReduce应用程序基础 63
4.2.2　如何编写MapReduce应用程序 65
4.2.3　如何运行MapReduce应用程序 67
4.2.4　如何浏览MapRecuce应用程序的输出 69
4.2.5　Hadoop MapReduce脚本的基础R函数 70
4.2.6　管理Hadoop MapReduce任务 71
4.3　R语言扩展包HadoopStreaming介绍 72
4.3.1　hsTableReader函数 73
4.3.2　hsKeyValReader函数 75
4.3.3　hasLineReader函数 75
4.3.4　运行Hadoop streaming任务 78
4.3.5　执行Hadoop Streaming任务 79
4.4　小结 79
第5章　利用R和Hadoop学习数据分析 80
5.1　数据分析项目生命周期 80
5.1.1　问题定义 81
5.1.2　设计数据需求 81
5.1.3　数据预处理 81
5.1.4　数据分析 82
5.1.5　数据可视化 82
5.2　数据分析问题 83
5.2.1　展示网页分类 83
5.2.2　计算股市变动频率 92
5.2.3　案例研究：预测推土机售价 98
5.3　小结 107
第6章　应用机器学习做大数据分析 108
6.1　机器学习介绍 108
6.2　有监督机器学习算法 109
6.2.1　线性回归 109
6.2.2　logistic回归 115
6.3　无监督机器学习算法 118
6.4　推荐算法 123
6.4.1　在R中产生推荐商品的步骤 125
6.4.2　使用R和Hadoop产生推荐商品 128
6.5　小结 131
第7章　从各种数据库中导入与导出数据 132
7.1　文件型数据库 134
7.1.1　不同类型的文件 134
7.1.2　安装R包 134
7.1.3　将数据导入R 134
7.1.4　从R导出数据 135
7.2　MySQL 135
7.2.1　安装MySQL 135
7.2.2　安装RMySQL 136
7.2.3　列出数据表及其结构 136
7.2.4　导入数据进R 136
7.2.5　数据操纵 137
7.3　Excel 137
7.3.1　安装Excel 138
7.3.2　导入数据进R 138
7.3.3　R和Excel的数据操纵 138
7.3.4　导出数据到Excel 138
7.4　MongoDB 138
7.4.1　安装MongoDB 139
7.4.2　安装rmongodb 141
7.4.3　导入数据进R 141
7.4.4　数据操纵 142
7.5　SQLite 143
7.5.1　SQLite的特性 143
7.5.2　安装SQLite 144
7.5.3　安装RSQLite 144
7.5.4　将数据导师入R 144
7.5.5　数据操纵 145
7.6　PostgreSQL 145
7.6.1　PostgreSQL的特性 145
7.6.2　安装PostgreSQL 145
7.6.3　安装RPostgreSQL 146
7.6.4　从R导出数据 146
7.7　Hive 147
7.7.1　Hive的特性 147
7.7.2　安装Hive 147
7.7.3　安装RHive 149
7.7.4　RHive操作 149
7.8　HBase 150
7.8.1　HBase的特性 150
7.8.2　安装HBase 151
7.8.3　安装Thrift 152
7.8.4　安装RHBase 153
7.8.5　导入数据进R 153
7.8.6　数据操纵 153
7.9　小结 154
附录　参考资源 155
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>R与Hadoop大数据分析实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>敏捷数据科学 : 用Hadoop创建数据分析应用
第1 部分 起步 ............................................................... 1
第1 章 理论 .................................................................. 3
敏捷大数据 ............................................................................................................3
Big Words 定义 ......................................................................................................4
敏捷大数据团队 .....................................................................................................5
认识机遇和问题 ..............................................................................................6
敏捷大数据流程 ................................................................................................... 11
代码检查和结对编程 ...........................................................................................12
敏捷的场所：开发的效率 ....................................................................................13
协作空间 .......................................................................................................14
私人空间 .......................................................................................................14
个人空间 .......................................................................................................14
用大幅打印件明确表达想法 ................................................................................15
第2 章 数据 ............................................................... 17
电子邮件 ..............................................................................................................17
处理原始数据 ......................................................................................................18
原始的电子邮件 ............................................................................................18
结构化与半结构化数据 .................................................................................18
SQL ......................................................................................................................20
NoSQL .................................................................................................................24
序列化 ...........................................................................................................24
从演变的模式中抽取和展示特征 ..................................................................25
数据流水线 ...................................................................................................26
数据透视 ..............................................................................................................27
社交网络 .......................................................................................................28
时间序列 .......................................................................................................30
自然语言 .......................................................................................................31
概率 ...............................................................................................................33
小结 .....................................................................................................................35
第3 章 敏捷开发工具 ................................................... 37
可扩展性= 简洁...................................................................................................37
敏捷大数据处理 ...................................................................................................38
设置运行Python 的虚拟环境 ...............................................................................39
使用Avro 对事件进行序列化 ..............................................................................40
在Python 中使用Avro ..................................................................................40
收集数据 ..............................................................................................................42
使用Pig 处理数据................................................................................................44
安装Pig .........................................................................................................45
使用MongoDB 发布数据 ....................................................................................49
安装MongoDB ..............................................................................................49
安装MongoDB 的Java 驱动程序 .................................................................50
安装mongo-hadoop .......................................................................................50
用Pig 向MongoDB 推送数据 .......................................................................50
使用ElasticSearch 搜索数据 ................................................................................52
安装 ...............................................................................................................52
使用Wonderdog 整合ElasticSearch 和Pig ...................................................53
对工作流程的反思 ...............................................................................................55
轻量级的Web 应用 ..............................................................................................56
Python 和 Flask .............................................................................................56
展示数据 ..............................................................................................................58
安装Bootstrap ...............................................................................................58
启用Bootstrap ...............................................................................................59
使用d3.js 和nvd3.js 可视化数据 ..................................................................63
小结 .....................................................................................................................64
第4 章 在云端 ............................................................. 65
引言 .....................................................................................................................65
GitHub .................................................................................................................67
dotCloud ...............................................................................................................67
dotCloud Echo 服务 .......................................................................................68
Python 工作者服务 ........................................................................................71
Amazon Web Services ..........................................................................................71
Simple Storage Service ..................................................................................71
Elastic MapReduce ........................................................................................72
MongoDB 即服务 ..........................................................................................79
辅助工具（Instrumentation） ................................................................................81
Google Analytics ...........................................................................................81
Mortar Data ...................................................................................................82
第2 部分 登上金字塔 ................................................... 85
第5 章 收集和展示数据 ............................................... 89
整合软件栈 ..........................................................................................................90
收集并序列化收件箱 ...........................................................................................90
处理和发布邮件数据 ...........................................................................................91
在浏览器中显示邮件 ...........................................................................................93
用Flask 和pymongo 处理邮件数据 ..............................................................94
使用Jinja2 渲染HTML5 页面 ......................................................................94
敏捷检查点 ..........................................................................................................98
生成电子邮件清单 ...............................................................................................99
用MongoDB 显示邮件 .................................................................................99
对数据展示的分析 ...................................................................................... 101
搜索邮件 ............................................................................................................ 106
使用Pig，ElasticSearch 和Wonderdog 构建索引 ....................................... 106
在网页中搜索邮件数据 ............................................................................... 107
结论 ................................................................................................................... 108
第6 章 使用图表可视化数据 ....................................... 111
优秀的图表 ........................................................................................................ 112
抽取实体：邮件地址 ......................................................................................... 112
抽取邮件 ..................................................................................................... 112
对时间进行可视化 ............................................................................................. 116
结论 ................................................................................................................... 122
第7 章 利用报表探索数据 .......................................... 123
为数据添加联系 ................................................................................................. 126
用TF-IDF 从邮件中提取关键字 ........................................................................ 133
小结 ................................................................................................................... 138
第8 章 预测 .............................................................. 141
预测电子邮件的回复率 ...................................................................................... 142
个性化 ................................................................................................................ 147
小结 ................................................................................................................... 148
第9 章 驱动行动 ........................................................ 149
好邮件的属性 .................................................................................................... 150
使用朴素贝叶斯方法进行更好的预测 ............................................................... 150
P(Reply | From ∩ To) ........................................................................................ 150
P(Reply | Token) ................................................................................................. 151
实时预测 ............................................................................................................ 153
记录事件日志 .................................................................................................... 157
小结 ................................................................................................................... 157
索引 ........................................................................... 159
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>敏捷数据科学 : 用Hadoop创建数据分析应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop高级数据分析 : 使用Hadoop生态系统设计和构建大数据系统
目录
第Ⅰ部分概念
第1章概述：用Hadoop构建数据分析系统3
1.1构建DAS的必要性4
1.2HadoopCore及其简史4
1.3Hadoop生态系统概述5
1.4AI技术、认知计算、深度学习以及BDA6
1.5自然语言处理与BDAS6
1.6SQL与NoSQL查询处理6
1.7必要的数学知识7
1.8设计及构建BDAS的循环过程7
1.9如何利用Hadoop生态系统实现BDA10
1.10“图像大数据”(IABD)基本思想10
1.10.1使用的编程语言12
1.10.2Hadoop生态系统的多语言组件12
1.10.3Hadoop生态系统架构13
1.11有关软件组合件与框架的注意事项13
1.12ApacheLucene、Solr及其他：开源搜索组件14
1.13建立BDAS的架构15
1.14你需要了解的事情15
1.15数据可视化与报表17
1.15.1使用EclipseIDE作为开发环境18
1.15.2本书未讲解的内容19
1.16本章小结21
第2章Scala及Python进阶23
2.1动机：选择正确的语言定义应用23
2.2Scala概览24
2.3Python概览29
2.4错误诊断、调试、配置文件及文档31
2.4.1Python的调试资源32
2.4.2Python文档33
2.4.3Scala的调试资源33
2.5编程应用与示例33
2.6本章小结34
2.7参考文献34
第3章Hadoop及分析的标准工具集35
3.1库、组件及工具集：概览35
3.2在评估系统中使用深度学习方法38
3.3使用Spring框架及SpringData44
3.4数字与统计库：R、Weka及其他44
3.5分布式系统的OLAP技术44
3.6用于分析的Hadoop工具集：ApacheMahout及相关工具45
3.7ApacheMahout的可视化46
3.8ApacheSpark库与组件46
3.8.1可供选择的不同类型的shell46
3.8.2ApacheSpark数据流47
3.8.3SparklingWater与H2O机器学习48
3.9组件使用与系统建立示例48
3.10封包、测试和文档化示例系统50
3.11本章小结51
3.12参考文献51
第4章关系、NoSQL及图数据库53
4.1图查询语言：Cypher及Gremlin55
4.2Cypher示例55
4.3Gremlin示例56
4.4图数据库：ApacheNeo4J58
4.5关系数据库及Hadoop生态系统59
4.6Hadoop以及UA组件59
4.7本章小结63
4.8参考文献64
第5章数据管道及其构建方法65
5.1基本数据管道66
5.2ApacheBeam简介67
5.3ApacheFalcon简介68
5.4数据源与数据接收：使用ApacheTika构建数据管道68
5.5计算与转换70
5.6结果可视化及报告71
5.7本章小结74
5.8参考文献74
第6章Hadoop、Lucene、Solr与高级搜索技术75
6.1Lucene/Solr生态系统简介75
6.2Lucene查询语法76
6.3使用Solr的编程示例79
6.4使用ELK栈(Elasticsearch、Logstash、Kibana)85
6.5Solr与Elasticsearch：特点与逻辑93
6.6应用于Elasticsearch和Solr的SpringData组件95
6.7使用LingPipe和GATE实现定制搜索99
6.8本章小结108
6.9参考文献108
第Ⅱ部分架构及算法
第7章分析技术及算法概览111
7.1算法类型综述111
7.2统计/数值技术112
7.3贝叶斯技术113
7.4本体驱动算法114
7.5混合算法：组合算法类型115
7.6代码示例116
7.7本章小结119
7.8参考文献119
第8章规则引擎、系统控制与系统编排121
8.1规则系统JBossDrools介绍121
8.2基于规则的软件系统控制124
8.3系统协调与JBossDrools125
8.4分析引擎示例与规则控制126
8.5本章小结129
8.6参考文献129
第9章综合提升：设计一个完整的分析系统131
9.1本章小结136
9.2参考文献136
第Ⅲ部分组件与系统
第10章数据可视化：可视化与交互分析139
10.1简单的可视化139
10.2AngularJS和Friends简介143
10.3使用JHipster集成SpringXD
和AngularJS143
10.4使用d3.js、sigma.js及其他
工具152
10.5本章小结153
10.6参考文献153
第Ⅳ部分案例研究与应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop高级数据分析 : 使用Hadoop生态系统设计和构建大数据系统
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop 2.X HDFS源码剖析
第1章 HDFS 1
1.1 HDFS概述 1
1.1.1 HDFS体系结构 1
1.1.2 HDFS基本概念 2
1.2 HDFS通信协议 4
1.2.1 Hadoop RPC接口 4
1.2.2 流式接口 20
1.3 HDFS主要流程 22
1.3.1 HDFS客户端读流程 22
1.3.2 HDFS客户端写流程 24
1.3.3 HDFS客户端追加写流程 25
1.3.4 Datanode启动、心跳以及执行名字节点指令流程 26
1.3.5 HA切换流程 27
第2章 Hadoop RPC 29
2.1 概述 29
2.1.1 RPC框架概述 29
2.1.2 Hadoop RPC框架概述 30
2.2 Hadoop RPC的使用 36
2.2.1 Hadoop RPC使用概述 36
2.2.2 定义RPC协议 40
2.2.3 客户端获取Proxy对象 45
2.2.4 服务器获取Server对象 54
2.3 Hadoop RPC实现 63
2.3.1 RPC类实现 63
2.3.2 Client类实现 64
2.3.3 Server类实现 76
第3章 Namenode（名字节点） 88
3.1 文件系统目录树 88
3.1.1 INode相关类 89
3.1.2 Feature相关类 102
3.1.3 FSEditLog类 117
3.1.4 FSImage类 138
3.1.5 FSDirectory类 158
3.2 数据块管理 162
3.2.1 Block、Replica、BlocksMap 162
3.2.2 数据块副本状态 167
3.2.3 BlockManager类（done） 177
3.3 数据节点管理 211
3.3.1 DatanodeDescriptor 212
3.3.2 DatanodeStorageInfo 214
3.3.3 DatanodeManager 217
3.4 租约管理 233
3.4.1 LeaseManager.Lease 233
3.4.2 LeaseManager 234
3.5 缓存管理 246
3.5.1 缓存概念 247
3.5.2 缓存管理命令 247
3.5.3 HDFS集中式缓存架构 247
3.5.4 CacheManager类实现 248
3.5.5 CacheReplicationMonitor 250
3.6 ClientProtocol实现 251
3.6.1 创建文件 251
3.6.2 追加写文件 254
3.6.3 创建新的数据块 257
3.6.4 放弃数据块 265
3.6.5 关闭文件 266
3.7 Namenode的启动和停止 268
3.7.1 安全模式 268
3.7.2 HDFS High Availability 276
3.7.3 名字节点的启动 301
3.7.4 名字节点的停止 306
第4章 Datanode（数据节点） 307
4.1 Datanode逻辑结构 307
4.1.1 HDFS 1.X架构 307
4.1.2 HDFS Federation 308
4.1.3 Datanode逻辑结构 310
4.2 Datanode存储 312
4.2.1 Datanode升级机制 312
4.2.2 Datanode磁盘存储结构 315
4.2.3 DataStorage实现 317
4.3 文件系统数据集 334
4.3.1 Datanode上数据块副本的状态 335
4.3.2 BlockPoolSlice实现 335
4.3.3 FsVolumeImpl实现 342
4.3.4 FsVolumeList实现 345
4.3.5 FsDatasetImpl实现 348
4.4 BlockPoolManager 375
4.4.1 BPServiceActor实现 376
4.4.2 BPOfferService实现 389
4.4.3 BlockPoolManager实现 396
4.5 流式接口 398
4.5.1 DataTransferProtocol定义 398
4.5.2 Sender和Receiver 399
4.5.3 DataXceiverServer 403
4.5.4 DataXceiver 406
4.5.5 读数据 408
4.5.6 写数据（done） 423
4.5.7 数据块替换、数据块拷贝和读数据块校验 437
4.5.8 短路读操作 437
4.6 数据块扫描器 437
4.6.1 DataBlockScanner实现 438
4.6.2 BlockPoolSliceScanner实现 439
4.7 DirectoryScanner 442
4.8 DataNode类的实现 443
4.8.1 DataNode的启动 444
4.8.2 DataNode的关闭 446
第5章 HDFS客户端 447
5.1 DFSClient实现 447
5.1.1 构造方法 448
5.1.2 关闭方法 449
5.1.3 文件系统管理与配置方法 450
5.1.4 HDFS文件与目录操作方法 451
5.1.5 HDFS文件读写方法 452
5.2 文件读操作与输入流 452
5.2.1 打开文件 452
5.2.2 读操作——DFSInputStream实现 461
5.3 文件短路读操作 481
5.3.1 短路读共享内存 482
5.3.2 DataTransferProtocol 484
5.3.3 DFSClient短路读操作流程 488
5.3.4 Datanode短路读操作流程 509
5.4 文件写操作与输出流 512
5.4.1 创建文件 512
5.4.2 写操作——DFSOutputStream实现 516
5.4.3 追加写操作 543
5.4.4 租约相关 546
5.4.5 关闭输出流 548
5.5 HDFS常用工具 549
5.5.1 FsShell实现 550
5.5.2 DFSAdmin实现 552
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop 2.X HDFS源码剖析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法 : Hadoop/Spark大数据处理技巧
序 1
前言 3
第1章二次排序：简介 19
二次排序问题解决方案 21
MapReduce/Hadoop的二次排序解决方案 25
Spark的二次排序解决方案 29
第2章二次排序：详细示例 42
二次排序技术 43
二次排序的完整示例 46
运行示例——老版本Hadoop API 50
运行示例——新版本Hadoop API 52
第3章 Top 10 列表 54
Top N 设计模式的形式化描述 55
MapReduce/Hadoop实现：唯一键 56
Spark实现：唯一键 62
Spark实现：非唯一键 73
使用takeOrdered()的Spark Top 10 解决方案 84
MapReduce/Hadoop Top 10 解决方案：非唯一键 91
第4章左外连接 96
左外连接示例 96
MapReduce左外连接实现 99
Spark左外连接实现 105
使用leftOuterJoin()的Spark实现 117
第5章反转排序 127
反转排序模式示例 128
反转排序模式的MapReduce/Hadoop实现 129
运行示例 134
第6章移动平均 137
示例1：时间序列数据（股票价格） 137
示例2：时间序列数据（URL访问数） 138
形式定义 139
POJO移动平均解决方案 140
MapReduce/Hadoop移动平均解决方案 143
第7章购物篮分析 155
MBA目标 155
MBA的应用领域 157
使用MapReduce的购物篮分析 157
Spark解决方案 166
运行Spark实现的YARN 脚本 179
第8章共同好友 182
输入 183
POJO共同好友解决方案 183
MapReduce算法 184
解决方案1: 使用文本的Hadoop实现 187
解决方案2: 使用ArrayListOfLongsWritable 的Hadoop实现 189
Spark解决方案 191
第9章使用MapReduce实现推荐引擎 201
购买过该商品的顾客还购买了哪些商品 202
经常一起购买的商品 206
推荐连接 210
第10章基于内容的电影推荐 225
输入 226
MapReduce阶段1 226
MapReduce阶段2和阶段3 227
Spark电影推荐实现 234
第11章使用马尔可夫模型的智能邮件营销 .253
马尔可夫链基本原理 254
使用MapReduce的马尔可夫模型 256
Spark解决方案 269
第12章 K-均值聚类 282
什么是K-均值聚类? 285
聚类的应用领域 285
K-均值聚类方法非形式化描述：分区方法 286
K-均值距离函数 286
K-均值聚类形式化描述 287
K-均值聚类的MapReduce解决方案 288
K-均值算法Spark实现 292
第13章 k-近邻 296
kNN分类 297
距离函数 297
kNN示例 298
kNN算法非形式化描述 299
kNN算法形式化描述 299
kNN的类Java非MapReduce 解决方案 299
Spark的kNN算法实现 301
第14章朴素贝叶斯 315
训练和学习示例 316
条件概率 319
深入分析朴素贝叶斯分类器 319
朴素贝叶斯分类器：符号数据的MapReduce解决方案 322
朴素贝叶斯分类器Spark实现 332
使用Spark和Mahout 347
第15章情感分析 349
情感示例 350
情感分数：正面或负面 350
一个简单的MapReduce情感分析示例 351
真实世界的情感分析 353
第16章查找、统计和列出大图中的所有三角形 354
基本的图概念 355
三角形计数的重要性 356
MapReduce/Hadoop解决方案 357
Spark解决方案 364
第17章 K-mer计数 375
K-mer计数的输入数据 376
K-mer计数应用 376
K-mer计数MapReduce/Hadoop解决方案 377
K-mer计数Spark解决方案 378
第18章 DNA测序 390
DNA测序的输入数据 392
输入数据验证 393
DNA序列比对 393
DNA测试的MapReduce算法 394
第19章 Cox回归 413
Cox模型剖析 414
使用R的Cox回归 415
Cox回归应用 416
Cox回归 POJO解决方案 417
MapReduce输入 418
使用MapReduce的Cox回归 419
第20章 Cochran-Armitage趋势检验 426
Cochran-Armitage算法 427
Cochran-Armitage应用 432
MapReduce解决方案 435
第21章等位基因频率 443
基本定义 444
形式化问题描述 448
等位基因频率分析的MapReduce解决方案 449
MapReduce解决方案, 阶段1 449
MapReduce解决方案，阶段2 459
MapReduce解决方案, 阶段3 463
染色体X 和Y的特殊处理 466
第22章 T检验 468
对bioset完成T检验 469
MapReduce问题描述 472
输入 472
期望输出 473
MapReduce解决方案 473
Spark实现 476
第23章皮尔逊相关系数 488
皮尔逊相关系数公式 489
皮尔逊相关系数示例 491
皮尔逊相关系数数据集 492
皮尔逊相关系数POJO 解决方案 492
皮尔逊相关系数MapReduce解决方案 493
皮尔逊相关系数的Spark 解决方案 496
运行Spark程序的YARN 脚本 516
使用Spark计算斯皮尔曼相关系数 517
第24章 DNA碱基计数 520
FASTA 格式 521
FASTQ 格式 522
MapReduce解决方案：FASTA 格式 522
运行示例 524
MapReduce解决方案: FASTQ 格式 528
Spark 解决方案: FASTA 格式 533
Spark解决方案: FASTQ 格式 537
第25章 RNA测序 543
数据大小和格式 543
MapReduce工作流 544
RNA测序分析概述 544
RNA测序MapReduce算法 548
第26章基因聚合 553
输入 554
输出 554
MapReduce解决方案（按单个值过滤和按平均值过滤） 555
基因聚合的Spark解决方案 567
Spark解决方案：按单个值过滤 567
Spark解决方案：按平均值过滤 576
第27章线性回归 586
基本定义 587
简单示例 587
问题描述 588
输入数据 589
期望输出 590
使用SimpleRegression的MapReduce解决方案 590
Hadoop实现类 593
使用R线性模型的MapReduce解决方案 593
第28章 MapReduce和幺半群 600
概述 600
幺半群的定义 602
幺半群和非幺半群示例 603
MapReduce示例：非幺半群 606
MapReduce示例：幺半群 608
使用幺半群的Spark示例 612
使用幺半群的结论 618
函子和幺半群 619
第29章小文件问题 622
解决方案1：在客户端合并小文件 623
解决方案2：用CombineFileInputFormat解决小文件问题 629
其他解决方案 634
第30章 MapReduce的大容量缓存 635
实现方案 636
缓存问题形式化描述 637
一个精巧、可伸缩的解决方案 637
实现LRUMap缓存 640
使用LRUMap的MapReduce解决方案 646
第31章 Bloom过滤器 651Bloom
过滤器性质 651
一个简单的Bloom过滤器示例 653
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法 : Hadoop/Spark大数据处理技巧
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop深度学习
第1章　深度学习介绍　　1
1.1　开始深度学习之旅　　5
1.1.1　深度前馈网络　　6
1.1.2　各种学习算法　　6
1.2　深度学习的相关术语　　10
1.3　深度学习——一场人工智能革命　　12
1.4　深度学习网络的分类　　18
1.4.1　深度生成或无监督模型　　19
1.4.2　深度判别模型　　20
1.5　小结　　22
第2章　大规模数据的分布式深度学习　　23
2.1　海量数据的深度学习　　24
2.2　大数据深度学习面临的挑战　　27
2.2.1　海量数据带来的挑战（第一个V）　　28
2.2.2　数据多样性带来的挑战（第二个V）　　28
2.2.3　数据快速处理带来的挑战（第三个V）　　29
2.2.4　数据真实性带来的挑战（第四个V）　　29
2.3　分布式深度学习和Hadoop　　29
2.3.1　Map-Reduce　　31
2.3.2　迭代Map-Reduce　　31
2.3.3　YARN　　32
2.3.4　分布式深度学习设计的重要特征　　32
2.4　深度学习的开源分布式框架Deeplearning4j　　34
2.4.1　Deeplearning4j的主要特性　　34
2.4.2　Deeplearning4j功能总结　　35
2.5　在Hadoop YARN上配置Deeplearning4j　　35
2.5.1　熟悉Deeplearning4j　　36
2.5.2　为进行分布式深度学习集成Hadoop YARN和Spark　　40
2.5.3　Spark在Hadoop YARN上的内存分配规则　　40
2.6　小结　　44
第3章　卷积神经网络　　45
3.1　卷积是什么　　46
3.2　卷积神经网络的背景　　47
3.3　卷积神经网络的基本层　　48
3.3.1　卷积神经网络深度的重要性　　49
3.3.2　卷积层　　49
3.3.3　为卷积层选择超参数　　52
3.3.4　ReLU层　　56
3.3.5　池化层　　57
3.3.6　全连接层　　58
3.4　分布式深度卷积神经网络　　58
3.4.1　最受欢迎的深度神经网络及其配置　　58
3.4.2　训练时间——深度神经网络面临的主要挑战　　59
3.4.3　将Hadoop应用于深度卷积神经网络　　59
3.5　使用Deeplearning4j构建卷积层　　61
3.5.1　加载数据　　61
3.5.2　模型配置　　62
3.5.3　训练与评估　　63
3.6　小结　　64
第4章　循环神经网络　　65
4.1　循环网络与众不同的原因　　66
4.2　循环神经网络　　67
4.2.1　展开循环计算　　68
4.2.2　循环神经网络的记忆　　69
4.2.3　架构　　70
4.3　随时间反向传播　　71
4.4　长短期记忆　　73
4.4.1　随时间深度反向传播的问题　　73
4.4.2　长短期记忆　　73
4.5　双向循环神经网络　　75
4.5.1　循环神经网络的不足　　75
4.5.2　解决方案　　76
4.6　分布式深度循环神经网络　　77
4.7　用Deeplearning4j训练循环神经网络　　77
4.8　小结　　80
第5章　受限玻尔兹曼机　　81
5.1　基于能量的模型　　82
5.2　玻尔兹曼机　　83
5.2.1　玻尔兹曼机如何学习　　84
5.2.2　玻尔兹曼机的不足　　85
5.3　受限玻尔兹曼机　　85
5.3.1　基础架构　　85
5.3.2　受限玻尔兹曼机的工作原理　　86
5.4　卷积受限玻尔兹曼机　　88
5.5　深度信念网络　　90
5.6　分布式深度信念网络　　91
5.6.1　受限玻尔兹曼机的分布式训练　　91
5.6.2　深度信念网络的分布式训练　　92
5.7　用Deeplearning4j实现受限玻尔兹曼机和深度信念网络　　94
5.7.1　受限玻尔兹曼机　　94
5.7.2　深度信念网络　　95
5.8　小结　　97
第6章　自动编码器　　98
6.1　自动编码器　　98
6.2　稀疏自动编码器　　101
6.2.1　稀疏编码　　101
6.2.2　稀疏自动编码器　　102
6.3　深度自动编码器　　104
6.3.1　训练深度自动编码器　　104
6.3.2　使用Deeplearning4j实现深度自动编码器　　107
6.4　降噪自动编码器　　108
6.4.1　降噪自动编码器的架构　　109
6.4.2　堆叠式降噪自动编码器　　109
6.4.3　使用Deeplearning4j实现堆叠式降噪自动编码器　　110
6.5　自动编码器的应用　　112
6.6　小结　　112
第7章　用Hadoop玩转深度学习　　113
7.1　Hadoop中的分布式视频解码　　114
7.2　使用Hadoop进行大规模图像处理　　116
7.3　使用Hadoop进行自然语言处理　　117
7.3.1　Web爬虫　　118
7.3.2　自然语言处理的关键词提取和模块　　118
7.3.3　从页面评估相关关键词　　118
7.4　小结　　119
参考文献　　120
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop深度学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop Hacks：专家使用的实践技巧
前言
第1章 系统架构／运用技巧
运行HDFS环境的参数
运行MapReduce环境需要的参数
总结
文件描述符的设置
Java的安装
总结
本技巧中介绍的HA的构成
HA集群的构建过程
疑难解答
总结
可以获取的统计信息
总结
关于CDH3同一版本间的更新
总结
准备
理解操作
使用Oracle的操作确认
总结
Sqoop的PostgreSQL联合功能
在PostgreSQL中的使用
PostgreSQL联合的挑战
总结
什么是Azkaban
Azkaban的安装
总结
作业的定制
总结
第2章 应用程序开发技巧
第3章 HBase技巧
第4章 Hive技巧
第5章 Pig技巧
第6章 Mahout技巧
第7章 ZooKeeper技巧
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop Hacks：专家使用的实践技巧
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从零开始学Hadoop大数据分析（视频教学版）
前言
第1篇 Hadoop基础知识
第1章 初识Hadoop 2
1.1 大数据初探 2
1.1.1 大数据技术 2
1.1.2 大数据技术框架 3
1.1.3 大数据的特点 3
1.1.4 大数据在各个行业中的应用 4
1.1.5 大数据计算模式 4
1.1.6 大数据与云计算、物联网的关系 4
1.2 Hadoop简介 5
1.2.1 Hadoop应用现状 6
1.2.2 Hadoop简介与意义 6
1.3 小结 6
第2章 Hadoop的安装与配置 7
2.1 虚拟机的创建 7
2.2 安装Linux系统 10
2.3 配置网络信息 11
2.4 克隆服务器 12
2.5 SSH免密码登录 13
2.6 安装和配置JDK 15
2.6.1 上传安装包 15
2.6.2 安装JDK 16
2.6.3 配置环境变量 16
2.7 Hadoop环境变量配置 16
2.7.1 解压缩Hadoop压缩包 17
2.7.2 配置Hadoop的bin和sbin文件夹到环境变量中 17
2.7.3 修改/etc/hadoop/hadoop-env.sh 17
2.8 Hadoop分布式安装 17
2.8.1 伪分布式安装 17
2.8.2 完全分布式安装 19
2.9 小结 21
第3章 Hadoop分布式文件系统 22
3.1 DFS介绍 22
3.1.1 什么是DFS 22
3.1.2 DFS的结构 22
3.2 HDFS介绍 23
3.2.1 HDFS的概念及体系结构 23
3.2.2 HDFS的设计 23
3.2.3 HDFS的优点和缺点 24
3.2.4 HDFS的执行原理 24
3.2.5 HDFS的核心概念 25
3.2.6 HDFS读文件流程 27
3.2.7 HDFS写文件流程 28
3.2.8 Block的副本放置策略 29
3.3 Hadoop中HDFS的常用命令 30
3.3.1 对文件的操作 30
3.3.2 管理与更新 31
3.4 HDFS的应用 31
3.4.1 基于Shell的操作 31
3.4.2 基于Java API的操作 33
3.4.3 创建文件夹 34
3.4.4 递归显示文件 34
3.4.5 文件上传 35
3.4.6 文件下载 35
3.5 小结 36
第4章 基于Hadoop 3的HDFS高可用 37
4.1 Hadoop 3.x的发展 37
4.1.1 Hadoop 3新特性 37
4.1.2 Hadoop 3 HDFS集群架构 38
4.2 Hadoop 3 HDFS完全分布式搭建 39
4.2.1 安装JDK 40
4.2.2 配置JDK环境变量 40
4.2.3 配置免密码登录 40
4.2.4 配置IP和主机名字映射关系 41
4.2.5 SSH免密码登录设置 41
4.2.6 配置Hadoop 3.1.0 42
4.3 什么是HDFS高可用 47
4.3.1 HDFS高可用实现原理 47
4.3.2 HDFS高可用实现 48
4.4 搭建HDFS高可用 50
4.4.1 配置ZooKeeper 50
4.4.2 配置Hadoop配置文件 52
4.4.3 将配置文件复制到其他节点上 54
4.4.4 启动JN节点 54
4.4.5 格式化 55
4.4.6 复制元数据到node2节点上 55
4.4.7 格式化ZKFC 55
4.4.8 启动集群 56
4.4.9 通过浏览器查看集群状态 56
4.4.10 高可用测试 57
4.5 小结 58
第2篇 Hadoop核心技术
第5章 Hadoop的分布式协调服务——ZooKeeper 60
5.1 ZooKeeper的核心概念 60
5.1.1 Session会话机制 60
5.1.2 数据节点、版本与Watcher的关联 61
5.1.3 ACL策略 61
5.2 ZooKeeper的安装与运行 61
5.3 ZooKeeper服务器端的常用命令 63
5.4 客户端连接ZooKeeper的相关操作 64
5.4.1 查看ZooKeeper常用命令 64
5.4.2 connect命令与ls命令 65
5.4.3 create命令——创建节点 65
5.4.4 get命令——获取数据与信息 66
5.4.5 set命令——修改节点内容 66
5.4.6 delete命令——删除节点 67
5.5 使用Java API访问ZooKeeper 67
5.5.1 环境准备与创建会话实例 68
5.5.2 节点创建实例 69
5.5.3 Java API访问ZooKeeper实例 70
5.6 小结 73
第6章 分布式离线计算框架——MapReduce 74
6.1 MapReduce概述 74
6.1.1 MapReduce的特点 74
6.1.2 MapReduce的应用场景 75
6.2 MapReduce执行过程 76
6.2.1 单词统计实例 76
6.2.2 MapReduce执行过程 77
6.2.3 MapReduce的文件切片Split 77
6.2.4 Map过程和Reduce过程 78
6.2.5 Shuffle过程 78
6.3 MapReduce实例 79
6.3.1 WordCount本地测试实例 79
6.3.2 ETL本地测试实例 84
6.4 温度排序实例 86
6.4.1 时间和温度的封装类MyKey.Java 87
6.4.2 Map任务MyMapper.java 88
6.4.3 数据分组类MyGroup.Java 89
6.4.4 温度排序类MySort.java 89
6.4.5 数据分区MyPartitioner.java 90
6.4.6 Reducer任务MyReducer.java 90
6.4.7 主函数RunJob.java 91
6.5 小结 94
第7章 Hadoop的集群资源管理系统——YARN 95
7.1 为什么要使用YARN 95
7.2 YARN的基本架构 96
7.2.1 ResourceManager进程 96
7.2.2 ApplicationMaster和NodeManager 97
7.3 YARN工作流程 97
7.4 YARN搭建 98
7.5 小结 100
第8章 Hadoop的数据仓库框架——Hive 101
8.1 Hive的理论基础 101
8.1.1 什么是Hive 101
8.1.2 Hive和数据库的异同 102
8.1.3 Hive设计的目的与应用 104
8.1.4 Hive的运行架构 104
8.1.5 Hive的执行流程 105
8.1.6 Hive服务 106
8.1.7 元数据存储Metastore 106
8.1.8 Embedded模式 107
8.1.9 Local模式 108
8.1.10 Remote模式 109
8.2 Hive的配置与安装 109
8.2.1 安装MySQL 110
8.2.2 配置Hive 112
8.3 Hive表的操作 113
8.3.1 创建Hive表 114
8.3.2 导入数据 114
8.4 表的分区与分桶 115
8.4.1 表的分区 115
8.4.2 表的分桶 117
8.5 内部表与外部表 118
8.5.1 内部表 119
8.5.2 外部表 119
8.6 内置函数与自定义函数 121
8.6.1 内置函数实例 121
8.6.2 自定义UDAF函数实例 123
8.7 通过Java访问Hive 124
8.8 Hive优化 125
8.8.1 MapReduce优化 126
8.8.2 配置优化 126
8.9 小结 127
第9章 大数据快速读写——HBase 128
9.1 关于NoSQL 128
9.1.1 什么是NoSQL 128
9.1.2 NoSQL数据库的分类 129
9.1.3 NoSQL数据库的应用 129
9.1.4 关系型数据库与非关系型数据库的区别 130
9.2 HBase基础 130
9.2.1 HBase简介 130
9.2.2 HBase数据模型 131
9.2.3 HBase体系架构及组件 132
9.2.4 HBase执行原理 134
9.3 HBase安装 135
9.4 HBase的Shell操作 138
9.5 Java API访问HBase实例 139
9.5.1 创建表 139
9.5.2 插入数据 140
9.5.3 查询数据 141
9.6 小结 142
第10章 海量日志采集工具——Flume 143
10.1 什么是Flume 143
10.2 Flume的特点 143
10.3 Flume架构 144
10.4 Flume的主要组件 144
10.4.1 Event、Client与Agent——数据传输 145
10.4.2 Source—Event接收 145
10.4.3 Channel—Event传输 146
10.4.4 Sink—Event发送 147
10.4.5 其他组件 148
10.5 Flume安装 148
10.6 Flume应用典型实例 149
10.6.1 本地数据读取（conf1） 149
10.6.2 收集至HDFS 150
10.6.3 基于日期分区的数据收集 152
10.7 通过exec命令实现数据收集 153
10.7.1 安装工具 153
10.7.2 编辑配置文件conf4 155
10.7.3 运行Flume 156
10.7.4 查看生成的文件 156
10.7.5 查看HDFS中的数据 157
10.8 小结 158
第11章 Hadoop和关系型数据库间的数据传输工具——Sqoop 159
11.1 什么是Sqoop 159
11.2 Sqoop工作机制 159
11.3 Sqoop的安装与配置 161
11.3.1 下载Sqoop 161
11.3.2 Sqoop配置 162
11.4 Sqoop数据导入实例 163
11.4.1 向HDFS中导入数据 165
11.4.2 将数据导入Hive 167
11.4.3 向HDFS中导入查询结果 170
11.5 Sqoop数据导出实例 172
11.6 小结 173
第12章 分布式消息队列——Kafka 174
12.1 什么是Kafka 174
12.2 Kafka的架构和主要组件 174
12.2.1 消息记录的类别名——Topic 175
12.2.2 Producer与Consumer——数据的生产和消费 176
12.2.3 其他组件——Broker、Partition、Offset、Segment 177
12.3 Kafka的下载与集群安装 177
12.3.1 安装包的下载与解压 177
12.3.2 Kafka的安装配置 178
12.4 Kafka应用实例 181
12.4.1 Producer实例 181
12.4.2 Consumer实例 182
12.5 小结 184
第13章 开源的内存数据库——Redis 185
13.1 Redis简介 185
13.1.1 什么是Redis 185
13.1.2 Redis的特点 186
13.2 Redis安装与配置 186
13.3 客户端登录 187
13.3.1 密码为空登录 187
13.3.2 设置密码登录 188
13.4 Redis的数据类型 188
13.4.1 String类型 188
13.4.2 List类型 190
13.4.3 Hash类型 191
13.4.4 Set类型 194
13.5 小结 197
第14章 Ambari和CDH 198
14.1 Ambari的安装与集群管理 198
14.1.1 认识HDP与Ambari 198
14.1.2 Ambari的搭建 199
14.1.3 配置网卡与修改本机名 199
14.1.4 定义DNS服务器与修改hosts主机映射关系 200
14.1.5 关闭防火墙并安装JDK 200
14.1.6 升级OpenSSL安全套接层协议版本 201
14.1.7 关闭SELinux的强制访问控制 201
14.1.8 SSH免密码登录 202
14.1.9 同步NTP 202
14.1.10 关闭Linux的THP服务 204
14.1.11 配置UMASK与HTTP服务 204
14.1.12 安装本地源制作相关工具与Createrepo 205
14.1.13 禁止离线更新与制作本地源 205
14.1.14 安装Ambari-server与MySQL 208
14.1.15 安装Ambari 210
14.1.16 安装Agent与Ambari登录安装 211
14.1.17 安装部署问题解决方案 214
14.2 CDH的安装与集群管理 216
14.2.1 什么是CDH和Cloudera Manager介绍 216
14.2.2 Cloudera Manager与Ambari对比的优势 216
14.2.3 CDH安装和网卡配置 217
14.2.4 修改本机名与定义DNS服务器 217
14.2.5 修改hosts主机映射关系 218
14.2.6 关闭防火墙 218
14.2.7 安装JDK 219
14.2.8 升级OpenSSL安全套接层协议版本 219
14.2.9 禁用SELinux的强制访问功能 220
14.2.10 SSH 免密码登录 220
14.2.11 同步NTP安装 220
14.2.12 安装MySQL 222
14.2.13 安装Cloudera Manager 222
14.2.14 添加MySQL驱动包和修改Agent配置 223
14.2.15 初始化CM5数据库和创建cloudera-scm用户 223
14.2.16 准备Parcels 223
14.2.17 CDH的安装配置 224
14.3 小结 227
第15章 快速且通用的集群计算系统——Spark 228
15.1 Spark基础知识 228
15.1.1 Spark的特点 228
15.1.2 Spark和Hadoop的比较 229
15.2 弹性分布式数据集RDD 230
15.2.1 RDD的概念 230
15.2.2 RDD的创建方式 230
15.2.3 RDD的操作 230
15.2.4 RDD的执行过程 231
15.3 Spark作业运行机制 232
15.4 运行在YARN上的Spark 233
15.4.1 在YARN上运行Spark 233
15.4.2 Spark在YARN上的两种部署模式 233
15.5 Spark集群安装 234
15.5.1 Spark安装包的下载 234
15.5.2 Spark安装环境 236
15.5.3 Scala安装和配置 236
15.5.4 Spark分布式集群配置 238
15.6 Spark实例详解 241
15.6.1 网站用户浏览次数最多的URL统计 241
15.6.2 用户地域定位实例 243
15.7 小结 246
第3篇 Hadoop项目案例实战
第16章 基于电商产品的大数据业务分析系统实战 248
16.1 项目背景、实现目标和项目需求 248
16.2 功能与流程 249
16.2.1 用户信息 250
16.2.2 商品信息 251
16.2.3 购买记录 251
16.3 数据收集 252
16.3.1 Flume的配置文件 252
16.3.2 启动Flume 253
16.3.3 查看采集后的文件 253
16.3.4 通过后台命令查看文件 254
16.3.5 查看文件内容 255
16.3.6 上传user.list文件 256
16.3.7 上传brand.list目录 256
16.4 数据预处理 257
16.5 数据分析——创建外部表 261
16.6 建立模型 264
16.6.1 各年龄段用户消费总额 264
16.6.2 查询各品牌销售总额 265
16.6.3 查询各省份消费总额 266
16.6.4 使用Sqoop将数据导入MySQL数据库 266
16.7 数据可视化 268
16.8 小结 272
第17章 用户画像分析实战 273
17.1 项目背景 273
17.2 项目目标与项目开发过程 274
17.2.1 数据采集 274
17.2.2 数据预处理 275
17.2.3 模型构建 275
17.2.4 数据分析 276
17.3 核心代码解读 277
17.3.1 项目流程介绍 277
17.3.2 核心类的解读 278
17.3.3 core-site.xml配置文件 279
17.3.4 hdfs-site.xml配置文件 279
17.3.5 UserProfile.properties配置文件 280
17.3.6 LoadConfig.java：读取配置信息 280
17.3.7 ReadFile.java：读取文件 281
17.3.8 ReadFromHdfs.java：提取信息 281
17.3.9 UserProfile.java：创建用户画像 282
17.3.10 TextArrayWritable.java：字符串处理工具类 285
17.3.11 MapReduce任务1：UserProfileMapReduce.java 285
17.3.12 MapReduce任务2：UserProfileMapReduce2.java 289
17.3.13 UserProfilePutInHbaseMap.java：提取用户画像 291
17.3.14 UserProfilePutInHbaseReduce：存储用户画像 292
17.4 项目部署 293
17.5 小结 294
第18章 基于个性化的视频推荐系统实战 295
18.1 项目背景 295
18.2 项目目标与推荐系统简介 295
18.2.1 推荐系统的分类 295
18.2.2 推荐模型的构建流程 296
18.2.3 推荐系统核心算法 297
18.2.4 如何基于Mahout框架完成商品推荐 300
18.2.5 基于Mahout框架的商品推荐实例 300
18.3 推荐系统项目架构 302
18.4 推荐系统模型构建 303
18.5 核心代码 304
18.5.1 公共部分 305
18.5.2 离线部分 307
18.5.3 在线部分 311
18.6 小结 314
第19章 电信离网用户挽留实战 315
19.1 商业理解 315
19.2 数据理解 316
19.2.1 收集数据 316
19.2.2 了解数据 317
19.2.3 保证数据质量 318
19.3 数据整理 318
19.3.1 数据整合 318
19.3.2 数据过滤 319
19.4 数据清洗 319
19.4.1 噪声识别 320
19.4.2 离群值和极端值的定义 321
19.4.3 离群值处理方法 321
19.4.4 数据空值处理示例 323
19.5 数据转换 324
19.5.1 变量转换 324
19.5.2 压缩分类水平数 324
19.5.3 连续数据离散化 325
19.5.4 变换哑变量 326
19.5.5 数据标准化 326
19.5.6 数据压缩 326
19.6 建模 327
19.6.1 决策树算法概述 327
19.6.2 决策树的训练步骤 327
19.6.3 训练决策树 328
19.6.4 C4.5算法 329
19.6.5 决策树剪枝 332
19.7 评估 335
19.7.1 混淆矩阵 335
19.7.2 ROC曲线 336
19.8 部署 338
19.9 用户离网案例代码详解 339
19.9.1 数据准备 339
19.9.2 相关性分析 341
19.9.3 最终建模 342
19.9.4 模型评估 343
19.10 小结 346
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>从零开始学Hadoop大数据分析（视频教学版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop基础教程
第1章　绪论　　1
1.1 　大数据处理　　1
1.1.1 　数据的价值　　2
1.1.2 　受众较少　　2
1.1.3 　一种不同的方法　　4
1.1.4 　Hadoop　　7
1.2 　基于Amazon Web Services的云计算　　12
1.2.1 　云太多了　　12
1.2.2 　第三种方法　　12
1.2.3 　不同类型的成本　　12
1.2.4 　AWS：Amazon的弹性架构　　13
1.2.5 　本书内容　　14
1.3 　小结　　15
第2章　安装并运行Hadoop　　16
2.1 　基于本地Ubuntu主机的Hadoop系统　　16
2.2 　实践环节：检查是否已安装JDK　　17
2.3 　实践环节：下载Hadoop　　18
2.4 　实践环节：安装SSH　　19
2.5 　实践环节：使用Hadoop计算圆周率　　20
2.6 　实践环节：配置伪分布式模式　　22
2.7 　实践环节：修改HDFS的根目录　　24
2.8 　实践环节：格式化NameNode　　25
2.9 　实践环节：启动Hadoop　　26
2.10 　实践环节：使用HDFS　　27
2.11 　实践环节：MapReduce的经典入门程序——字数统计　　28
2.12 　使用弹性MapReduce　　33
2.13 　实践环节：使用管理控制台在EMR运行WordCount　　34
2.13.1 　使用EMR的其他方式　　41
2.13.2 　AWS生态系统　　42
2.14 　本地Hadoop与EMR Hadoop的对比　　42
2.15 　小结　　43
第3章　理解MapReduce　　44
3.1 　键值对　　44
3.1.1 　具体含义　　44
3.1.2 　为什么采用键/值数据　　45
3.1.3 　MapReduce作为一系列键/值变换　　46
3.2 　MapReduce的Hadoop Java API　　47
3.3 　编写MapReduce程序　　50
3.4 　实践环节：设置classpath　　50
3.5 　实践环节：实现WordCount　　51
3.6 　实践环节：构建JAR文件　　53
3.7 　实践环节：在本地Hadoop集群运行WordCount　　54
3.8 　实践环节：在EMR上运行WordCount　　54
3.8.1 　0.20之前版本的Java MapReduce API　　56
3.8.2 　Hadoop提供的mapper和reducer实现　　57
3.9 　实践环节：WordCount的简易方法　　58
3.10 　查看WordCount的运行全貌　　59
3.10.1 　启动　　59
3.10.2 　将输入分块　　59
3.10.3 　任务分配　　60
3.10.4 　任务启动　　60
3.10.5 　不断监视JobTracker　　60
3.10.6 　mapper的输入　　61
3.10.7 　mapper的执行　　61
3.10.8 　mapper的输出和reducer的输入　　61
3.10.9 　分块　　62
3.10.10　　可选分块函数　　62
3.10.11　　reducer类的输入　　62
3.10.12　　reducer类的执行　　63
3.10.13　　reducer类的输出　　63
3.10.14　　关机　　63
3.10.15　　这就是MapReduce的全部　　64
3.10.16　　也许缺了combiner　　64
3.11 　实践环节：使用combiner编写WordCount　　64
3.12 　实践环节：更正使用combiner的WordCount　　65
3.13 　Hadoop专有数据类型　　67
3.13.1　　Writable和Writable-Comparable接口　　67
3.13.2 　wrapper类介绍　　68
3.14 　实践环节：使用Writable包装类　　69
3.15 　输入/输出　　71
3.15.1　　文件、split和记录　　71
3.15.2　　InputFormat和RecordReader　　71
3.15.3　　Hadoop提供的InputFormat　　72
3.15.4　　Hadoop提供的RecordReader　　73
3.15.5　　OutputFormat和Record-Writer　　73
3.15.6　　Hadoop提供的OutputFormat　　73
3.15.7　　别忘了Sequence files　　74
3.16 　小结　　74
第4章　开发MapReduce程序　　75
4.1 　使用非Java语言操作Hadoop　　75
4.1.1 　Hadoop Streaming工作原理　　76
4.1.2　　使用Hadoop Streaming的原因　　76
4.2 　实践环节：使用Streaming实现Word-Count　　76
4.3 　分析大数据集　　79
4.3.1 　获取UFO目击事件数据集　　79
4.3.2 　了解数据集　　80
4.4 　实践环节：统计汇总UFO数据　　80
4.5 　实践环节：统计形状数据　　82
4.6 　实践环节：找出目击事件的持续时间与UFO形状的关系　　84
4.7 　实践环节：在命令行中执行形状/时间分析　　87
4.8 　实践环节：使用ChainMapper进行字段验证/分析　　88
4.9 　实践环节：使用Distributed Cache改进地点输出　　93
4.10 　计数器、状态和其他输出　　96
4.11 　实践环节：创建计数器、任务状态和写入日志　　96
4.12 　小结　　102
第5章　高级MapReduce技术　　103
5.1 　初级、高级还是中级　　103
5.2 　多数据源联结　　103
5.2.1 　不适合执行联结操作的情况　　104
5.2.2　　map端联结与reduce端联结的对比　　104
5.2.3 　匹配账户与销售信息　　105
5.3 　实践环节：使用MultipleInputs实现reduce端联结　　105
5.3.1 　实现map端联结　　109
5.3.2 　是否进行联结　　112
5.4 　图算法　　112
5.4.1 　Graph 101　　112
5.4.2 　图和MapReduce　　112
5.4.3 　图的表示方法　　113
5.5 　实践环节：图的表示　　114
5.6 　实践环节：创建源代码　　115
5.7 　实践环节：第一次运行作业　　119
5.8 　实践环节：第二次运行作业　　120
5.9 　实践环节：第三次运行作业　　121
5.10 　实践环节：第四次也是最后一次运行作业　　122
5.10.1 　运行多个作业　　124
5.10.2 　关于图的终极思考　　124
5.11 　使用语言无关的数据结构　　124
5.11.1 　候选技术　　124
5.11.2 　Avro简介　　125
5.12 　实践环节：获取并安装Avro　　125
5.13 　实践环节：定义模式　　126
5.14 　实践环节：使用Ruby创建Avro源数据　　127
5.15 　实践环节：使用Java语言编程操作Avro数据　　128
5.16 　实践环节：在MapReduce中统计UFO形状　　130
5.17 　实践环节：使用Ruby检查输出数据　　134
5.18 　实践环节：使用Java检查输出数据　　135
5.19 　小结　　137
第6章　故障处理　　138
6.1 　故障　　138
6.1.1 　拥抱故障　　138
6.1.2 　至少不怕出现故障　　139
6.1.3 　严禁模仿　　139
6.1.4 　故障类型　　139
6.1.5 　Hadoop节点故障　　139
6.2 　实践环节：杀死DataNode进程　　141
6.3 　实践环节：复制因子的作用　　144
6.4 　实践环节：故意造成数据块丢失　　146
6.5 　实践环节：杀死TaskTracker进程　　149
6.6 　实践环节：杀死JobTracker　　153
6.7 　实践环节：杀死NameNode进程　　154
6.8 　实践环节：引发任务故障　　160
6.9 　数据原因造成的任务故障　　163
6.10 　实践环节：使用skip模式处理异常数据　　164
6.11 　小结　　169
第7章　系统运行与维护　　170
7.1 　关于EMR的说明　　170
7.2 　Hadoop配置属性　　171
7.3 　实践环节：浏览默认属性　　171
7.3.1 　附加的属性元素　　172
7.3.2 　默认存储位置　　172
7.3.3 　设置Hadoop属性的几种方式　　173
7.4 　集群设置　　174
7.4.1 　为集群配备多少台主机　　174
7.4.2 　特殊节点的需求　　176
7.4.3 　不同类型的存储系统　　177
7.4.4 　Hadoop的网络配置　　178
7.5 　实践环节：查看默认的机柜配置　　180
7.6 　实践环节：报告每台主机所在机柜　　180
7.7 　集群访问控制　　183
7.8 　实践环节：展示Hadoop的默认安全机制　　183
7.9 　管理NameNode　　187
7.10 　实践环节：为fsimage文件新增一个存储路径　　188
7.11 　实践环节：迁移到新的NameNode主机　　190
7.12 　管理HDFS　　192
7.12.1 　数据写入位置　　192
7.12.2 　使用平衡器　　193
7.13 　MapReduce管理　　193
7.13.1 　通过命令行管理作业　　193
7.13.2 　作业优先级和作业调度　　194
7.14 　实践环节：修改作业优先级并结束作业运行　　194
7.15 　扩展集群规模　　197
7.15.1 　提升本地Hadoop集群的计算能力　　197
7.15.2 　提升EMR作业流的计算能力　　198
7.16 　小结　　198
第8章　Hive：数据的关系视图　　200
8.1 　Hive概述　　200
8.1.1 　为什么使用Hive　　200
8.1.2 　感谢Facebook　　201
8.2 　设置Hive　　201
8.2.1 　准备工作　　201
8.2.2 　下载Hive　　202
8.3 　实践环节：安装Hive　　202
8.4 　使用Hive　　203
8.5 　实践环节：创建UFO数据表　　204
8.6 　实践环节：在表中插入数据　　206
8.7 　实践环节：验证表　　208
8.8 　实践环节：用正确的列分隔符重定义表　　210
8.9 　实践环节：基于现有文件创建表　　212
8.10 　实践环节：执行联结操作　　214
8.11 　实践环节：使用视图　　216
8.12 　实践环节：导出查询结果　　219
8.13 　实践环节：制作UFO目击事件分区表　　221
8.13.1 　分桶、归并和排序　　224
8.13.2 　用户自定义函数　　225
8.14 　实践环节：新增用户自定义函数　　225
8.14.1 　是否进行预处理　　228
8.14.2 　Hive和Pig的对比　　229
8.14.3 　未提到的内容　　229
8.15 　基于Amazon Web Services的Hive　　230
8.16 　实践环节：在EMR上分析UFO数据　　230
8.16.1 　在开发过程中使用交互式作业流　　235
8.16.2 　与其他AWS产品的集成　　236
8.17 　小结　　236
第9章　与关系数据库协同工作　　238
9.1 　常见数据路径　　238
9.1.1 　Hadoop用于存储档案　　238
9.1.2 　使用Hadoop进行数据预处理　　239
9.1.3 　使用Hadoop作为数据输入工具　　239
9.1.4 　数据循环　　240
9.2 　配置MySQL　　240
9.3 　实践环节：安装并设置MySQL　　240
9.4 　实践环节：配置MySQL允许远程连接　　243
9.5 　实践环节：建立员工数据库　　245
9.6 　把数据导入Hadoop　　246
9.6.1 　使用MySQL工具手工导入　　246
9.6.2 　在mapper中访问数据库　　246
9.6.3 　更好的方法：使用Sqoop　　247
9.7 　实践环节：下载并配置Sqoop　　247
9.8 　实践环节：把MySQL的数据导入HDFS　　249
9.9 　实践环节：把MySQL数据导出到
Hive　　253
9.10 　实践环节：有选择性的导入数据　　255
9.11 　实践环节：使用数据类型映射　　257
9.12 　实践环节：通过原始查询导入数据　　258
9.13 　从Hadoop导出数据　　261
9.13.1 　在reducer中把数据写入关系数据库　　261
9.13.2 　利用reducer输出SQL数据文件　　262
9.13.3 　仍是最好的方法　　262
9.14 　实践环节：把Hadoop数据导入MySQL　　262
9.15 　实践环节：把Hive数据导入MySQL　　265
9.16 　实践环节：改进mapper并重新运行数据导出命令　　267
9.17 　在AWS上使用Sqoop　　269
9.18 　小结　　270
第10章　使用Flume收集数据　　271
10.1 　关于AWS的说明　　271
10.2 　无处不在的数据　　271
10.2.1 　数据类别　　272
10.2.2 　把网络流量导入Hadoop　　272
10.3 　实践环节：把网络服务器数据导入Hadoop　　272
10.3.1 　把文件导入Hadoop　　273
10.3.2 　潜在的问题　　273
10.4 　Apache Flume简介　　274
10.5 　实践环节：安装并配置Flume　　275
10.6 　实践环节：把网络流量存入日志文件　　277
10.7 　实践环节：把日志输出到控制台　　279
10.8 　实践环节：把命令的执行结果写入平面文件　　281
10.9 　实践环节：把远程文件数据写入本地平面文件　　283
10.9.1 　信源、信宿和信道　　284
10.9.2 　Flume配置文件　　286
10.9.3 　一切都以事件为核心　　287
10.10 　实践环节：把网络数据写入HDFS　　287
10.11 　实践环节：加入时间戳　　289
10.12 　实践环节：多层Flume网络　　292
10.13 　实践环节：把事件写入多个信宿　　294
10.13.1 　选择器的类型　　295
10.13.2 　信宿故障处理　　295
10.13.3 　使用简单元件搭建复杂系统　　296
10.14 　更高的视角　　297
10.14.1 　数据的生命周期　　297
10.14.2 　集结数据　　297
10.14.3 　调度　　297
10.15 　小结　　298
第11章　展望未来　　299
11.1 　全书回顾　　299
11.2 　即将到来的Hadoop变革　　300
11.3 　其他版本的Hadoop软件包　　300
11.4 　其他Apache项目　　303
11.4.1 　HBase　　303
11.4.2 　Oozie　　303
11.4.3 　Whir　　304
11.4.4 　Mahout　　304
11.4.5 　MRUnit　　305
11.5 　其他程序设计模式　　305
11.5.1 　Pig　　305
11.5.2 　Cascading　　305
11.6 　AWS资源　　306
11.6.1 　在EMR上使用HBase　　306
11.6.2 　SimpleDB　　306
11.6.3 　DynamoDB　　306
11.7 　获取信息的渠道　　307
11.7.1 　源代码　　307
11.7.2 　邮件列表和论坛　　307
11.7.3 　LinkedIn群组　　307
11.7.4 　Hadoop用户群　　307
11.7.5 　会议　　308
11.8 　小结　　308
随堂测验答案　　309
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop基础教程
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop核心技术
前　言
基　础　篇
第1章　认识Hadoop 2
1.1　缘于搜索的小象 2
1.1.1　Hadoop的身世 2
1.1.2　Hadoop简介 3
1.1.3　Hadoop发展简史 6
1.2　大数据、Hadoop和云计算 7
1.2.1　大数据 7
1.2.2　大数据、Hadoop和云计算的关系 8
1.3　设计思想与架构 9
1.3.1　数据存储与切分 9
1.3.2　MapReduce模型 11
1.3.3　MPI和MapReduce 13
1.4　国外Hadoop的应用现状 13
1.5　国内Hadoop的应用现状 17
1.6　Hadoop发行版 20
1.6.1　Apache Hadoop 20
1.6.2　Cloudera Hadoop 20
1.6.3　Hortonworks Hadoop发行版 21
1.6.4　MapR Hadoop发行版 22
1.6.5　IBM Hadoop发行版 24
1.6.6　Intel Hadoop发行版 24
1.6.7　华为Hadoop发行版 25
1.7　小结 26
第2章　Hadoop使用之初体验 27
2.1　搭建测试环境 27
2.1.1　软件与准备 27
2.1.2　安装与配置 28
2.1.3　启动与停止 29
2.2　算法分析与设计 31
2.2.1　Map设计 31
2.2.2　Reduce设计 32
2.3　实现接口 32
2.3.1　Java API实现 33
2.3.2　Streaming接口实现 36
2.3.3　Pipes接口实现 38
2.4　编译 40
2.4.1　基于Java API实现的编译 40
2.4.2　基于Streaming实现的编译 40
2.4.3　基于Pipes实现的编译 41
2.5　提交作业 41
2.5.1　基于Java API实现作业提交 41
2.5.2　基于Streaming实现作业提交 42
2.5.3　基于Pipes实现作业提交 43
2.6　小结 44
第3章　Hadoop存储系统 45
3.1　基本概念 46
3.1.1　NameNode 46
3.1.2　DateNode 46
3.1.3　客户端 47
3.1.4　块 47
3.2　HDFS的特性和目标 48
3.2.1　HDFS的特性 48
3.2.2　HDFS的目标 48
3.3　HDFS架构 49
3.3.1　Master/Slave架构 49
3.3.2　NameNode和Secondary NameNode通信模型 51
3.3.3　文件存取机制 52
3.4　HDFS核心设计 54
3.4.1　Block大小 54
3.4.2　数据复制 55
3.4.3　数据副本存放策略 56
3.4.4　数据组织 57
3.4.5　空间回收 57
3.4.6　通信协议 58
3.4.7　安全模式 58
3.4.8　机架感知 59
3.4.9　健壮性 59
3.4.10　负载均衡 60
3.4.11　升级和回滚机制 62
3.5　HDFS权限管理 64
3.5.1　用户身份 64
3.5.2　系统实现 65
3.5.3　超级用户 65
3.5.4　配置参数 65
3.6　HDFS配额管理 66
3.7　HDFS的缺点 67
3.8　小结 68
第4章　HDFS的使用 69
4.1　HDFS环境准备 69
4.1.1　HDFS安装配置 69
4.1.2　HDFS格式化与启动 70
4.1.3　HDFS运行检查 70
4.2　HDFS命令的使用 71
4.2.1　fs shell 71
4.2.2　archive 77
4.2.3　distcp 78
4.2.4　fsck 81
4.3　HDFS Java API的使用方法 82
4.3.1　Java API简介 82
4.3.2　读文件 82
4.3.3　写文件 86
4.3.4　删除文件或目录 90
4.4　C接口libhdfs 91
4.4.1　libhdfs介绍 91
4.4.2　编译与部署 91
4.4.3　libhdfs接口介绍 92
4.4.4　libhdfs使用举例 95
4.5　WebHDFS接口 97
4.5.1　WebHDFS REST API简介 97
4.5.2　WebHDFS配置 98
4.5.3　WebHDFS使用 98
4.5.4　WebHDFS错误响应和查询参数 101
4.6　小结 103
第5章　MapReduce计算框架 104
5.1　Hadoop MapReduce简介 104
5.2　MapReduce模型 105
5.2.1　MapReduce编程模型 105
5.2.2　MapReduce实现原理 106
5.3　计算流程与机制 108
5.3.1　作业提交和初始化 108
5.3.2　Mapper 110
5.3.3　Reducer 111
5.3.4　Reporter和OutputCollector 112
5.4　MapReduce的输入/输出格式 113
5.4.1　输入格式 113
5.4.2　输出格式 118
5.5　核心问题 124
5.5.1　Map和Reduce数量 124
5.5.2　作业配置 126
5.5.3　作业执行和环境 127
5.5.4　作业容错机制 129
5.5.5　作业调度 131
5.6　有用的MapReduce特性 132
5.6.1　计数器 132
5.6.2　DistributedCache 134
5.6.3　Tool 135
5.6.4　IsolationRunner 136
5.6.5　Prof?iling 136
5.6.6　MapReduce调试 136
5.6.7　数据压缩 137
5.6.8　优化 138
5.7　小结 138
第6章　Hadoop命令系统 139
6.1　Hadoop命令系统的组成 139
6.2　用户命令 141
6.3　管理员命令 144
6.4　测试命令 148
6.5　应用命令 156
6.6　Hadoop的streaming命令 163
6.6.1　streaming命令 163
6.6.2　参数使用分析 164
6.7　Hadoop的pipes命令 168
6.7.1　pipes命令 168
6.7.2　参数使用分析 169
6.8　小结 170
高　级　篇
第7章　MapReduce深度分析 172
7.1　MapReduce总结构分析 172
7.1.1　数据流向分析 172
7.1.2　处理流程分析 174
7.2　MapTask实现分析 176
7.2.1　总逻辑分析 176
7.2.2　Read阶段 178
7.2.3　Map阶段 178
7.2.4　Collector和Partitioner阶段 180
7.2.5　Spill阶段 181
7.2.6　Merge阶段 185
7.3　ReduceTask实现分析 186
7.3.1　总逻辑分析 186
7.3.2　Shuffle阶段 187
7.3.3　Merge阶段 189
7.3.4　Sort阶段 190
7.3.5　Reduce阶段 191
7.4　JobTracker分析 192
7.4.1　JobTracker服务分析 192
7.4.2　JobTracker启动分析 193
7.4.3　JobTracker核心子线程分析 195
7.5　TaskTracker分析 201
7.5.1　TaskTracker启动分析 201
7.5.2　TaskTracker核心子线程分析 205
7.6　心跳机制实现分析 207
7.6.1　心跳检测分析 207
7.6.2　TaskTracker.transmitHeart-Beat() 207
7.6.3　JobTracker.heartbeat() 209
7.6.4　JobTracker.processHeartbeat() 212
7.7　作业创建分析 213
7.7.1　初始化分析 214
7.7.2　作业提交分析 215
7.8　作业执行分析 217
7.8.1　JobTracker初始化 218
7.8.2　TaskTracker.startNewTask() 220
7.8.3　TaskTracker.localizeJob() 220
7.8.4　TaskRunner.run() 221
7.8.5　MapTask.run() 222
7.9　小结 223
第8章　Hadoop Streaming和Pipes原理与实现 224
8.1　Streaming原理浅析 224
8.2　Streaming实现架构 226
8.3　Streaming核心实现机制 227
8.3.1　主控框架实现 227
8.3.2　用户进程管理 228
8.3.3　框架和用户程序的交互 229
8.3.4　PipeMapper和PiperReducer 230
8.4　Pipes原理浅析 231
8.5　Pipes实现架构 233
8.6　Pipes核心实现机制 234
8.6.1　主控类实现 234
8.6.2　用户进程管理 235
8.6.3　PipesMapRunner 235
8.6.4　PipesReducer 238
8.6.5　C++端HadoopPipes 238
8.7　小结 239
第9章　Hadoop作业调度系统 240
9.1　作业调度概述 241
9.1.1　相关概念 241
9.1.2　作业调度流程 242
9.1.3　集群资源组织与管理 243
9.1.4　队列控制和权限管理 244
9.1.5　插件式调度框架 245
9.2　FIFO调度器 246
9.2.1　基本调度策略 246
9.2.2　FIFO实现分析 247
9.2.3　FIFO初始化与停止 248
9.2.4　作业监听控制 249
9.2.5　任务分配算法 250
9.2.6　配置与使用 254
9.3　公平调度器 254
9.3.1　产生背景 254
9.3.2　主要功能 255
9.3.3　基本调度策略 255
9.3.4　FairScheduler实现分析 257
9.3.5　FairScheduler启停分析 258
9.3.6　作业监听控制 260
9.3.7　资源池管理 260
9.3.8　作业更新策略 262
9.3.9　作业权重和资源量的计算 266
9.3.10　任务分配算法 267
9.3.11　FairScheduler配置参数 268
9.3.12　使用与管理 270
9.4　容量调度器 272
9.4.1　产生背景 272
9.4.2　主要功能 272
9.4.3　基本调度策略 274
9.4.4　CapacityScheduler实现分析 274
9.4.5　CapacityScheduler启停分析 275
9.4.6　作业监听控制 277
9.4.7　作业初始化分析 277
9.4.8　任务分配算法 278
9.4.9　内存匹配机制 279
9.4.10　配置与使用 280
9.5　调度器对比分析 283
9.5.1　调度策略对比 283
9.5.2　队列和优先级 283
9.5.3　资源分配保证 283
9.5.4　作业限制 284
9.5.5　配置管理 284
9.5.6　扩展性支持 284
9.5.7　资源抢占和延迟调度 284
9.5.8　优缺点分析 285
9.6　其他调度器 285
9.6.1　HOD调度器 285
9.6.2　LATE调度器 286
9.7　小结 288
实　战　篇
第10章　Hadoop集群搭建 290
10.1　Hadoop版本的选择 290
10.2　集群基础硬件需求 291
10.2.1　内存 291
10.2.2　CPU 292
10.2.3　磁盘 292
10.2.4　网卡 293
10.2.5　网络拓扑 293
10.3　集群基础软件需求 294
10.3.1　操作系统 294
10.3.2　JVM和SSH 295
10.4　虚拟化需求 295
10.5　事前准备 296
10.5.1　创建安装用户 296
10.5.2　安装Java 297
10.5.3　安装SSH并设置 297
10.5.4　防火墙端口设置 298
10.6　安装Hadoop 298
10.6.1　安装HDFS 299
10.6.2　安装MapReduce 299
10.7　集群配置 300
10.7.1　配置管理 300
10.7.2　环境变量配置 301
10.7.3　核心参数配置 302
10.7.4　HDFS参数配置 303
10.7.5　MapReduce参数配置 306
10.7.6　masters和slaves配置 313
10.7.7　客户端配置 313
10.8　启动和停止 314
10.8.1　启动/停止HDFS 314
10.8.2　启动/停止MapReduce 315
10.8.3　启动验证 315
10.9　集群基准测试 316
10.9.1　HDFS基准测试 316
10.9.2　MapReduce基准测试 317
10.9.3　综合性能测试 318
10.10　集群搭建实例 319
10.10.1　部署策略 319
10.10.2　软件和硬件环境 320
10.10.3　Hadoop安装 321
10.10.4　配置core-site.xml 321
10.10.5　配置hdfs-site.xml 322
10.10.6　配置mapred-site.xml 322
10.10.7　SecondaryNameNode和Slave 324
10.10.8　配置作业队列 324
10.10.9　配置第三方调度器 325
10.10.10　启动与验证 327
10.11　小结 327
第11章　Hadoop Streaming和Pipes编程实战 328
11.1　Streaming基础编程 328
11.1.1　Streaming编程入门 328
11.1.2　Map和Reduce数目 331
11.1.3　队列、优先级及权限 332
11.1.4　分发文件和压缩包 333
11.1.5　压缩参数的使用 336
11.1.6　本地作业的调试 338
11.2　Streaming高级应用 338
11.2.1　参数与环境变量传递 339
11.2.2　自定义分隔符 340
11.2.3　自定义Partitioner 343
11.2.4　自定义计数器 347
11.2.5　处理二进制数据 347
11.2.6　使用聚合函数 351
11.3　Pipes编程接口 352
11.3.1　TaskContext 352
11.3.2　Mapper 353
11.3.3　Reducer 354
11.3.4　Partitioner 354
11.3.5　RecordReader 355
11.3.6　RecordWriter 356
11.4　Pipes编程应用 357
11.5　小结 359
第12章　Hadoop MapReduce应用开发 360
12.1　开发环境准备 360
12.2　Eclipse集成环境开发 361
12.2.1　构建MapReduce Eclipse IDE 361
12.2.2　开发示例 363
12.3　MapReduce Java API编程 368
12.3.1　Mapper编程接口 369
12.3.2　Reducer编程接口 370
12.3.3　驱动类编写 372
12.3.4　编译运行 373
12.4　压缩功能使用 374
12.4.1　Hadoop数据压缩 374
12.4.2　压缩特征与性能 374
12.4.3　本地压缩库 375
12.4.4　使用压缩 376
12.5　排序应用 378
12.5.1　Hadoop排序问题 378
12.5.2　二次排序 378
12.5.3　比较器和组合排序 380
12.5.4　全局排序 381
12.6　多路输出 382
12.7　常见问题与处理方法 384
12.7.1　常见的开发问题 384
12.7.2　运行时错误问题 386
12.8　小结 387
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop核心技术
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析YARN架构设计与实现原理
前　言
第一部分　准备篇
第1章　环境准备 2
1.1　准备学习环境 2
1.1.1　基础软件下载 2
1.1.2　如何准备Linux环境 3
1.2　获取Hadoop源代码 5
1.3　搭建Hadoop源代码阅读环境 5
1.3.1　创建Hadoop工程 5
1.3.2　Hadoop源代码阅读技巧 8
1.4　Hadoop源代码组织结构 10
1.5　Hadoop初体验 12
1.5.1　搭建Hadoop环境 12
1.5.2　Hadoop Shell介绍 15
1.6　编译及调试Hadoop源代码 16
1.6.1　编译Hadoop源代码 17
1.6.2　调试Hadoop源代码 18
1.7　小结 20
第2章　YARN设计理念与基本架构 21
2.1　YARN产生背景 21
2.1.1　MRv1的局限性 21
2.1.2　轻量级弹性计算平台 22
2.2　Hadoop基础知识 23
2.2.1　术语解释 23
2.2.2　Hadoop版本变迁 25
2.3　YARN基本设计思想 29
2.3.1　基本框架对比 29
2.3.2　编程模型对比 30
2.4　YARN 基本架构 31
2.4.1　YARN基本组成结构 32
2.4.2　YARN通信协议 34
2.5　YARN工作流程 35
2.6　多角度理解YARN 36
2.6.1　并行编程 36
2.6.2　资源管理系统 36
2.6.3　云计算 37
2.7　本书涉及内容 38
2.8　小结 38
第二部分　YARN核心设计篇
第3章　YARN基础库 40
3.1　概述 40
3.2　第三方开源库 41
3.2.1　Protocol Buffers 41
3.2.2　Apache Avro 43
3.3　底层通信库 46
3.3.1　RPC通信模型 46
3.3.2　Hadoop RPC的特点概述 48
3.3.3　RPC总体架构 48
3.3.4　Hadoop RPC使用方法 49
3.3.5　Hadoop RPC类详解 51
3.3.6　Hadoop RPC参数调优 57
3.3.7　YARN RPC实现 57
3.3.8　YARN RPC应用实例 61
3.4　服务库与事件库 65
3.4.1　服务库 66
3.4.2　事件库 66
3.4.3　YARN服务库和事件库的使用方法 68
3.4.4　事件驱动带来的变化 70
3.5　状态机库 72
3.5.1　YARN状态转换方式 72
3.5.2　状态机类 73
3.5.3　状态机的使用方法 73
3.5.4　状态机可视化 76
3.6　源代码阅读引导 76
3.7　小结 77
3.8　问题讨论 77
第4章　YARN应用程序设计方法 78
4.1　概述 78
4.2　客户端设计 79
4.2.1　客户端编写流程 80
4.2.2　客户端编程库 84
4.3　ApplicationMaster设计 84
4.3.1　ApplicationMaster编写流程 84
4.3.2　ApplicationMaster编程库 92
4.4　YARN 应用程序实例 95
4.4.1　DistributedShell 95
4.4.2　Unmanaged AM 99
4.5　源代码阅读引导 100
4.6　小结 100
4.7　问题讨论 100
第5章　ResourceManager剖析 102
5.1　概述 102
5.1.1　ResourceManager基本职能 102
5.1.2　ResourceManager内部架构 103
5.1.3　ResourceManager事件与事件处理器 106
5.2　用户交互模块 108
5.2.1　ClientRMService 108
5.2.2　AdminService 109
5.3　ApplicationMaster管理 109
5.4　NodeManager管理 112
5.5　Application管理 113
5.6　状态机管理 114
5.6.1　RMApp状态机 115
5.6.2　RMAppAttempt状态机 119
5.6.3　RMContainer状态机 123
5.6.4　RMNode状态机 127
5.7　几个常见行为分析 129
5.7.1　启动ApplicationMaster 129
5.7.2　申请与分配Container 132
5.7.3　杀死Application 134
5.7.4　Container超时 135
5.7.5　ApplicationMaster超时 138
5.7.6　NodeManager超时 138
5.8　安全管理 139
5.8.1　术语介绍 139
5.8.2　Hadoop认证机制 139
5.8.3　Hadoop授权机制 142
5.9　容错机制 144
5.9.1　Hadoop HA基本框架 145
5.9.2　YARN HA实现 148
5.10　源代码阅读引导 149
5.11　小结 151
5.12　问题讨论 152
第6章　资源调度器 153
6.1　资源调度器背景 153
6.2　HOD调度器 154
6.2.1　Torque资源管理器 154
6.2.2　HOD作业调度 155
6.3　YARN资源调度器的基本架构 157
6.3.1　基本架构 157
6.3.2　资源表示模型 160
6.3.3　资源调度模型 161
6.3.4　资源抢占模型 164
6.4　YARN层级队列管理机制 169
6.4.1　层级队列管理机制 169
6.4.2　队列命名规则 171
6.5　Capacity Scheduler 172
6.5.1　Capacity Scheduler的功能 172
6.5.2　Capacity Scheduler实现 176
6.6　Fair Scheduler 179
6.6.1　Fair Scheduler功能介绍 180
6.6.2　Fair Scheduler实现 182
6.6.3　Fair Scheduler与Capacity Scheduler对比 183
6.7　其他资源调度器介绍 184
6.8　源代码阅读引导 185
6.9　小结 186
6.10　问题讨论 187
第7章　NodeManager剖析 188
7.1　概述 188
7.1.1　NodeManager基本职能 188
7.1.2　NodeManager内部架构 190
7.1.3　NodeManager事件与事件处理器 193
7.2　节点健康状况检测 194
7.2.1　自定义Shell脚本 194
7.2.2　检测磁盘损坏数目 196
7.3　分布式缓存机制 196
7.3.1　资源可见性与分类 198
7.3.2　分布式缓存实现 200
7.4　目录结构管理 203
7.4.1　数据目录管理 203
7.4.2　日志目录管理 203
7.5　状态机管理 206
7.5.1　Application状态机 207
7.5.2　Container状态机 210
7.5.3　LocalizedResource状态机 213
7.6　Container生命周期剖析 214
7.6.1　Container资源本地化 214
7.6.2　Container运行 218
7.6.3　Container资源清理 222
7.7　资源隔离 224
7.7.1　Cgroups介绍 224
7.7.2　内存资源隔离 228
7.7.3　CPU资源隔离 230
7.8　源代码阅读引导 234
7.9　小结 235
7.10　问题讨论 236
第三部分　计算框架篇
第8章　离线计算框架MapReduce 238
8.1　概述 238
8.1.1　基本构成 238
8.1.2　事件与事件处理器 240
8.2　MapReduce客户端 241
8.2.1　ApplicationClientProtocol协议 242
8.2.2　MRClientProtocol协议 243
8.3　MRAppMaster工作流程 243
8.4　MR作业生命周期及相关状态机 246
8.4.1　MR作业生命周期 246
8.4.2　Job状态机 249
8.4.3　Task状态机 253
8.4.4　TaskAttempt状态机 255
8.5　资源申请与再分配 259
8.5.1　资源申请 259
8.5.2　资源再分配 262
8.6　Container启动与释放 263
8.7　推测执行机制 264
8.7.1　算法介绍 265
8.7.2　推测执行相关类 266
8.8　作业恢复 267
8.9　数据处理引擎 269
8.10　历史作业管理器 271
8.11　MRv1与MRv2对比 273
8.11.1　MRv1 On YARN 273
8.11.2　MRv1与MRv2架构比较 274
8.11.3　MRv1与MRv2编程接口兼容性 274
8.12　源代码阅读引导 275
8.13　小结 277
8.14　问题讨论 277
第9章　DAG计算框架Tez 278
9.1　背景 278
9.2　Tez数据处理引擎 281
9.2.1　Tez编程模型 281
9.2.2　Tez数据处理引擎 282
9.3　DAG Master实现 284
9.3.1　DAG编程模型 284
9.3.2　MR到DAG转换 286
9.3.3　DAGAppMaster 288
9.4　优化机制 291
9.4.1　当前YARN框架存在的问题 291
9.4.2　Tez引入的优化技术 292
9.5　Tez应用场景 292
9.6　与其他系统比较 294
9.7　小结 295
第10章　实时/内存计算框架Storm/Spark 296
10.1　Hadoop MapReduce的短板 296
10.2　实时计算框架Storm 296
10.2.1　Storm编程模型 297
10.2.2　Storm基本架构 302
10.2.3　Storm On YARN 304
10.3　内存计算框架Spark 307
10.3.1　Spark编程模型 308
10.3.2　Spark基本架构 312
10.3.3　Spark On YARN 316
10.3.4　Spark/Storm On YARN比较 317
10.4　小结 317
第四部分　高级篇
第11章　Facebook Corona剖析 320
11.1　概述 320
11.1.1　Corona的基本架构 320
11.1.2　Corona的RPC协议与序列化框架 322
11.2　Corona设计特点 323
11.2.1　推式网络通信模型 323
11.2.2　基于Hadoop 0.20版本 324
11.2.3　使用Thrift 324
11.2.4　深度集成Fair Scheduler 324
11.3　工作流程介绍 324
11.3.1　作业提交 325
11.3.2　资源申请与任务启动 326
11.4　主要模块介绍 327
11.4.1　ClusterManager 327
11.4.2　CoronaJobTracker 330
11.4.3　CoronaTaskTracker 333
11.5　小结 335
第12章　Apache Mesos剖析 336
12.1　概述 336
12.2　底层网络通信库 337
12.2.1　libprocess基本架构 338
12.2.2　一个简单示例 338
12.3　Mesos服务 340
12.3.1　SchedulerProcess 341
12.3.2　Mesos Master 342
12.3.3　Mesos Slave 343
12.3.4　ExecutorProcess 343
12.4　Mesos工作流程 344
12.4.1　框架注册过程 344
12.4.2　Framework Executor注册过程 345
12.4.3　资源分配到任务运行过程 345
12.4.4　任务启动过程 347
12.4.5　任务状态更新过程 347
12.5　Mesos资源分配策略 348
12.5.1　Mesos资源分配框架 349
12.5.2　Mesos资源分配算法 349
12.6　Mesos容错机制 350
12.6.1　Mesos Master容错 350
12.6.2　Mesos Slave容错 351
12.7　Mesos应用实例 352
12.7.1　Hadoop On Mesos 352
12.7.2　Storm On Mesos 353
12.8　Mesos与YARN对比 354
12.9　小结 355
第13章　YARN总结与发展趋势 356
13.1　资源管理系统设计动机 356
13.2　资源管理系统架构演化 357
13.2.1　集中式架构 357
13.2.2　双层调度架构 358
13.2.3　共享状态架构 358
13.3　YARN发展趋势 359
13.3.1　YARN自身的完善 359
13.3.2　以YARN为核心的生态系统 361
13.3.3　YARN周边工具的完善 363
13.4　小结 363
附录A　YARN安装指南 364
附录B　YARN配置参数介绍 367
附录C　Hadoop Shell命令介绍 371
附录D　参考资料 374
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析YARN架构设计与实现原理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>高可用性的HDFS : Hadoop分布式文件系统深度实践
目录
第1章 HDFS HA及解决方案 1
1.1 HDFS系统架构 2
1.2 HA定义 3
1.3 HDFS HA原因分析及应对措施 4
1.3.1 可靠性 4
1.3.2 可维护性 5
1.4 现有HDFS HA解决方案 5
1.4.1 Hadoop的元数据备份方案 6
1.4.2 Hadoop的SecondaryNameNode方案 7
1.4.3 Hadoop的Checkpoint ode方案 7
1.4.4 Hadoop的BackupNode方案 8
1.4.5 DRDB方案 9
1.4.6 FaceBook的AvatarNode方案 10
1.5 方案优缺点比较 10
第2章 HDFS元数据解析 13
2.1 概述 14
2.2 内存元数据结构 14
2.2.1 INode 15
2.2.2 Block 16
2.2.3 BlockInfo和DatanodeDescriptor 17
2.2.4 小结 17
2.2.5 代码分析——元数据结构 18
2.3 磁盘元数据文件 24
2.4 Format情景分析 27
2.5 元数据应用场景分析 45
第3章 Hadoop的元数据备份方案 47
3.1 运行机制分析 48
3.1.1 NameNode启动加载元数据情景分析 50
3.1.2 元数据更新及日志写入情景分析 64
3.1.3 Checkpoint过程情景分析 73
3.1.4 元数据可靠性机制 109
3.1.5 元数据一致性机制 110
3.2 使用说明 110
第4章 Hadoop的Backup Node方案 113
4.1 Backup Node概述 114
4.1.1 系统架构 115
4.1.2 使用原则 115
4.1.3 优缺点 116
4.2 运行机制分析 116
4.2.1 启动流程 117
4.2.2 元数据操作情景分析 141
4.2.3 日志池（journal spool）机制 151
4.2.4 故障切换机制 156
4.3 实验方案说明 158
4.4 构建实验环境 158
4.4.1 网络拓扑 159
4.4.2 系统安装及配置 160
4.4.3 安装JDK 170
4.4.4 虚拟机集群架设 171
4.4.5 NameNode安装及配置 173
4.4.6 Backup Node安装及配置 173
4.4.7 Data Node安装及配置 174
4.4.8 Clients安装及配置 175
4.5 异常解决方案 175
4.5.1 异常情况分析 175
4.5.2 NameNode配置 175
4.5.3 Backup Node配置 182
4.5.4 Data Node配置 185
4.5.5 NameNode宕机切换实验 189
4.5.6 NameNode宕机读写测试 196
第5章 AvatarNode运行机制 205
5.1 方案说明 206
5.1.1 系统架构 206
5.1.2 思路分析 208
5.1.3 性能数据 209
5.2 元数据分析 209
5.2.1 类FSNamesystem 210
5.2.2 类FSDirectory 210
5.2.3 AvatarNode的磁盘元数据文件 211
5.3 AvatarNode Primary启动过程 211
5.4 AvatarNode Standby启动过程 217
5.4.1 AvatarNode的构造方法 217
5.4.2 Standby线程的run()方法 218
5.4.3 Ingest线程的run()方法 220
5.4.4 Ingest线程的ingestFSEdits ()方法 220
5.4.5 Standby线程的doCheckpoint()方法 221
5.5 用户操作情景分析 223
5.5.1 创建目录情景分析 223
5.5.2 创建文件情景分析 231
5.6 AvatarNode Standby故障切换过程 240
5.7 元数据一致性保证机制 242
5.7.1 元数据目录树信息 242
5.7.2 Data Node与Block数据块映射信息 243
5.8 Block更新同步问题 246
5.8.1 问题描述 246
5.8.2 结论 246
5.8.3 源码分析 246
第6章 AvatarNode使用 253
6.1 方案说明 254
6.1.1 网络拓扑 254
6.1.2 操作系统安装及配置 255
6.2 使用Avatar打补丁版本 255
6.2.1 Hadoop源码联机Build 256
6.2.2 Hadoop源码本地Build 262
6.2.3 NFS服务器构建 264
6.2.4 Avatar分发与部署 267
6.2.5 Primary（namenode0）节点配置 269
6.2.7 Data Node节点配置 276
6.2.8 Client节点配置 278
6.2.9 创建目录 279
6.2.10 挂载NFS 280
6.2.11 启动Ucarp 280
6.2.12 格式化 281
6.2.13 系统启动 281
6.2.14 检查 282
6.2.15 NameNode失效切换写文件实验 283
6.2.16 NameNode失效切换读文件实验 291
6.3 Avatar FaceBook版本的使用 294
6.3.1 Hadoop FaceBook版本安装 294
6.3.2 节点配置 295
6.3.3 启动HDFS 300
6.3.4 NameNode失效切换 302
第7章 AvatarNode异常解决方案 305
7.1 测试环境 306
7.2 Primary失效 306
7.2.1 解决方案 306
7.2.2 写操作实验步骤 307
7.2.3 改进写操作机制 313
7.2.4 读操作实验步骤 313
7.2.5 小结 317
7.3 Standby失效 317
7.4 NFS失效（数据未损坏） 317
7.4.1 解决方案 317
7.4.2 写操作实验步骤 318
7.4.3 读操作实验步骤 320
7.4.4 小结 322
7.5 NFS失效（数据已损坏） 323
7.5.1 解决方案 323
7.5.2 写操作实验步骤 324
7.5.3 读操作实验步骤 327
7.5.4 小结 330
7.6 Primary先失效，NFS后失效（数据未损坏） 331
7.6.1 解决方案 331
7.6.2 写操作实验步骤 331
7.6.3 读操作实验步骤 333
7.6.4 小结 334
7.7 Primary先失效（数据未损坏），NFS后失效（数据损坏） 335
7.7.1 解决方案 335
7.7.2 写操作实验步骤 335
7.7.3 读操作实验步骤 338
7.7.4 小结 339
7.8 NFS先失效（数据未损坏），Primary后失效 340
7.8.1 解决方案 340
7.8.2 写操作实验步骤 340
7.8.3 读操作实验步骤 342
7.8.4 小结 343
7.9 NFS先失效（数据损坏），Primary后失效（数据损坏） 344
7.9.1 解决方案 344
7.9.2 写操作实验步骤 344
7.9.3 读操作实验步骤 346
7.9.4 小结 348
7.10 实验结论 348
第8章 Cloudera HA NameNode使用 349
8.1 HA NameNode说明 350
8.2 CDH4B1版本HDFS集群配置 351
8.2.1 虚拟机安装 351
8.2.2 nn1配置 351
8.2.3 dn1~dn3配置 355
8.2.4 HDFS集群构建 358
8.3 HA NameNode配置 361
8.3.1 nn1配置 361
8.3.2 其他节点配置 365
8.4 HA NameNode使用 367
8.4.1 启动HA HDFS集群 367
8.4.2 第1次failover 368
8.4.3 模拟写操作 368
8.4.4 模拟Active Name Node失效，第2次failover 369
8.3.5 模拟新的Standby NameNode加入 370
8.5 小结 371
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>高可用性的HDFS : Hadoop分布式文件系统深度实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南 (第4版 英文影印版)
Foreword
Preface
Part Ⅰ.Hadoop Fundamentals
1.MeetHadoop
Data！
Data Storage and Analysis
Querying All Your Data
Beyond Batch
Comparison with Other Systems
Relational Database Management Systems
Grid Computing
Volunteer Computing
A Brief History of Apache Hadoop
What's in This Book？
2.MapReduce
A Weather Dataset
Data Format
Analyzing the Data with Unix Tools
Analyzing the Data with Hadoop
Map and Reduce
Java MapReduce
Scaling Out
Data Flow
Combiner Functions
Running a Distributed MapReduce Job
Hadoop Streaming
Ruby
Python
3.The Hadoop Distributed Filesystem
The Design of HDFS
HDFS Concepts
Blocks
Namenodes and Datanodes
Block Caching
HDFS Federation
HDFS High Availability
The Command—Line Interface
Basic Filesystem Operations
Hadoop Filesystems
Interfaces
The Java Interface
Reading Data from a Hadoop URL
Reading Data Using the FileSystem API
Writing Data
Directories
Querying the Filesystem
Deleting Data
Data Flow
Anatomy of a File Read
Anatomy of a File Write
Coherency Model
Parallel Copying with distcp
Keeping an HDFS Cluster Balanced
4.YARN
Anatomy of a YARN Application Run
Resource Requests
Application Lifespan
Building YARN Applications
YARN Compared to MapReduce 1
Scheduling in YARN
Scheduler Options
Capacity Scheduler Configuration
Fair Scheduler Configuration
Delay Scheduling
Dominant Resource Fairness
Further Reading
5.Hadoop I／O
Data Integrity
Data Integrity in HDFS
LocaIFileSystem
ChecksumFileSystem
Compression
Codecs
Compression and Input Splits
Using Compression in MapReduce
Serialization
The Writable Interface
Writable Classes
Implementing a Custom Writable
Serialization Frameworks
File—Based Data Structures
SequenceFile
MapFile
Other File Formats and Column—Oriented Formats
Part Ⅱ.MapReduce
6.Developing a MapReduce Application
The Conflguration API
Combining Resources
Variable Expansion
Setting Up the Development Environment
Managing Configuration
GenericOptionsParser， Tool， and ToolRunner
Writing a Unit Test with MRUnit
Mapper
Reducer
Running Locally on Test Data
Running a Job in a Local Job Runner
Testing the Driver
Running on a Cluster
Packaging a Job
Launching a Job
The MapReduce Web UI
Retrieving the Results
Debugging a Job
Hadoop Logs
Remote Debugging
Tuning a Job
Profiling Tasks
MapReduce Workflows
Decomposing a Problem into MapReduce Jobs
IobControl
Apache Oozie
7.How MapReduce Works
Anatomy ofa MapReduce Job Run
Job Submission
Job Initialization
Task Assignmenl
Task Execution
Progress and Status Updates
Job Completion
Failures
Task Failure
Application Master Failure
Node Manager Failure
Resource Manager Failure
Shuffle and Sort
The Map Side
The Reduce Side
Configuration Tuning
Task Execution
The Task Execution Environment
Speculative Execution
Output Committers
8.MapReduce Typesand Formats
MapReduce Types
The Default MapReduce Job
Input Formats
Input Splits and Records
Text Input
Binary Input
Multiple Inputs
Database Input （and Output）
Output Formats
Text Output
Binary Output
Multiple Outputs
Lazy Output
Database Output
……
9.MapReduce Features
Part Ⅲ.Hadoop Operations
10.Setting Up a Hadoop Cluster
11.Administering Hadoop
Part Ⅳ.RelatedProjects
12.Avro
13.Parquet
14.Flume
15.Sqoop
16.Pig
17.Hive
18.Crunch
19.Spark
20.HBase
21.ZooKeeper
Part Ⅴ.Case Studies
22.Composable Data at Cerner
23.Biological Data Saence： Saving Lives with Software
24.Cascading
A.Installing Apache Hadoop
B.Cloudera's Distribution Including Apache Hadoop
C.Preparing the NCDC Weather Data
D.The Old and New Java MapReduce APls
Index
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南 (第4版 英文影印版)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop云计算一体机实践指南
前言
理论部分 1
第1章 云计算理论 1
1.1 云计算的概念 1
1.2 云计算发展现状 1
1.3 网格计算与云计算 4
1.4 云计算的发展环境 5
1.5 各大IT厂商云计算平台特点概述 8
1.6 开源云计算系统概述 20
第2章 Hadoop理论 27
2.1 Hadoop简介 27
2.2 Hadoop架构 28
2.3 Hadoop分布式文件系统HDFS 29
2.4 分布式数据处理MapReduce 33
第3章 Linux命令操作 35
3.1 Linux操作系统介绍 35
3.2 Linux常用shell命令 36
基础实践部分 40
第4章 集群搭建 40
4.1 CentOS安装 40
4.1.1 实验目的 40
4.1.2 实验设备 40
4.1.3 实验内容 40
4.1.4 实验步骤 40
4.2 集群搭建 57
4.2.1 实验目的 57
4.2.2 实验设备 57
4.2.3 实验内容 57
4.2.4 实验步骤 57
4.3 获取Hadoop安装包 79
4.3.1 实验目的 79
4.3.2 实验设备 79
4.3.3 实验内容 79
4.3.4 实验步骤 79
4.4 启动和关闭hadoop集群 83
4.4.1 实验目的 83
4.4.2 实验设备 83
4.4.3 实验内容 83
4.4.4 实验步骤 83
第5章 熟悉Hadoop本地集群 87
5.1 熟悉hadoop一些常用命令 87
5.1.1 实验目的 87
5.1.2 实验设备 87
5.1.3 实验内容 87
5.1.4 实验步骤 87
5.2 使用distcp进行并行复制 91
5.3 Web浏览Hadoop集群 92
5.3.1 实验目的 92
5.3.2 实验设备 92
5.3.3 实验内容 92
5.3.4 实验步骤 92
5.4 使用hadoop命令归档文件 93
第6章 Hadoop管理应用 96
6.1 系统体检和报告 96
6.2 了解HDFS的平衡命令 98
6.3 权限设置 99
6.4 配额管理 99
6.5 启用回收站 99
第7章 HDFS实践 101
7.1 使用HDFS上传文件 101
7.1.1 实验目的 101
7.1.2 实验设备 101
7.1.3 实验内容 101
7.1.4 实验原理 101
7.1.5 实验步骤 102
7.2 使用HDFS浏览文件和目录 104
7.2.1 实验目的 104
7.2.2 实验设备 104
7.2.3 实验内容 104
7.2.4 实验原理 105
7.2.5 实验步骤 107
7.3 使用HDFS打开、下载和删除文件 108
7.3.1 实验目的 108
7.3.2 实验设备 108
7.3.3 实验内容 108
7.3.4 实验原理 108
7.3.5 实验步骤 110
第8章 MapReduce实践 113
8.1 数据去重实验 113
8.1.1 实验目的 113
8.1.2 实验设备 113
8.1.3 实验内容 113
8.1.4 实验原理 114
8.1.5 实验步骤 116
8.2 数据排序实验 119
8.2.1 实验目的 119
8.2.2 实验设备 119
8.2.3 实验内容 120
8.2.4 实验原理 121
8.2.5 实验步骤 123
8.3 平均成绩实验 126
8.3.1 实验目的 126
8.3.2 实验设备 126
8.3.3 实验内容 126
8.3.4 实验原理 126
8.3.5 实验步骤 129
8.4 单表关联实验 131
8.4.1 实验目的 131
8.4.2 实验设备 131
8.4.3 实验内容 131
8.4.4 实验原理 133
8.4.5 实验步骤 137
项目实训 142
第9章 个人存储私有云综合实训 142
9.1 实验目的 142
9.2 实验设备 142
9.3 实验内容 142
9.4 实验原理 142
9.5 实验步骤 146
第10章 气象数据分析云综合实训 155
10.1 实验目的 155
10.2 实验设备 155
10.3 实验内容 155
10.4 实验原理 155
10.5 实验步骤 161
第11章 微信人物关系综合实训 165
11.1 实验目的 165
11.2 实验设备 165
11.3 实验内容 165
11.4 实验原理 165
11.5 实验步骤 173
第12章 云图书馆实例综合实训 178
12.1 实验目的 178
12.2 实验设备 178
12.3 实验内容 178
12.4 实验原理 178
12.5 实验步骤 188
第13章 物联网与云计算（快递）实例 192
13.1 实验目的 192
13.2 实验设备 192
13.3 实验内容 192
13.4 实验原理 192
13.5 实验步骤 197
参考文献 203
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop云计算一体机实践指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据解决方案
第1章 Hadoop概述 1
1.1 商业分析与大数据 2
1.1.1 Hadoop的组件 3
1.1.2 Hadoop分布式文件系统(HDFS) 3
1.1.3 MapReduce是什么 4
1.1.4 YARN是什么 5
1.2 ZooKeeper是什么 6
1.3 Hive是什么 7
1.4 与其他系统集成 8
1.4.1 Hadoop生态系统 9
1.4.2 数据集成与Hadoop 11
1.5 小结 16
第2章 存储 19
2.1 Hadoop HDFS的基础知识 20
2.1.1 概念 21
2.1.2 架构 25
2.1.3 接口 29
2.2 在分布式模式下设置HDFS群集 35
2.3 HDFS的高级特性 40
2.3.1 快照 41
2.3.2 离线查看器 44
2.3.3 分层存储 52
2.3.4 纠删码 55
2.4 文件格式 59
2.5 云存储 63
2.6 小结 64
第3章 计算 65
3.1 Hadoop MapReduce的基础 66
3.1.1 概念 66
3.1.2 架构 69
3.2 如何启动MapReduce作业 76
3.2.1 编写Map任务 77
3.2.2 编写reduce任务 79
3.2.3 编写MapReduce作业 80
3.2.4 配置 83
3.3 MapReduce的高级特性 85
3.3.1 分布式缓存 85
3.3.2 计数器 87
3.3.3 作业历史服务器 89
3.4 与Spark作业的区别 91
3.5 小结 92
第4章 用户体验 93
4.1 Apache Hive 94
4.1.1 安装Hive 96
4.1.2 HiveQL 97
4.1.3 UDF/SerDe 103
4.1.4 Hive调优 105
4.2 Apache Pig 106
4.2.1 安装Pig 107
4.2.2 Pig Latin 108
4.3 UDF 110
4.4 Hue 111
4.5 Apache Oozie 114
4.5.1 安装Oozie 115
4.5.2 Oozie的工作原理 118
4.5.3 工作流/协调器 119
4.5.4 Oozie CLI 124
4.6 小结 124
第5章 与其他系统集成 125
5.1 Apache Sqoop 126
5.2 Apache Flume 130
5.3 Apache Kafka 136
5.3.1 工作原理 138
5.3.2 Kafka Connect 141
5.3.3 流处理 143
5.4 Apache Storm 144
5.4.1 工作原理 145
5.4.2 Trident 148
5.4.3 Kafka集成 149
5.5 小结 152
第6章 Hadoop安全 153
6.1 提升Hadoop群集安全性 154
6.1.1 边界安全 154
6.1.2 Kerberos认证 156
6.1.3 Hadoop中的服务级授权 162
6.1.4 用户模拟 167
6.1.5 提升HTTP信道的安全性 170
6.2 提升数据安全性 174
6.2.1 数据分类 175
6.2.2 将数据传到群集 176
6.2.3 保护群集中的数据 182
6.3 增强应用程序安全性 189
6.3.1 YARN架构 189
6.3.2 YARN中的应用提交 190
6.4 小结 195
第7章 自由的生态圈：Hadoop与Apache BigTop 197
7.1 基础概念 198
7.1.1 软件栈 199
7.1.2 测试栈 200
7.1.3 在我的笔记本电脑上工作 201
7.2 开发定制的软件栈 201
7.2.1 Apache Bigtop：历史 201
7.2.2 Apache Bigtop：概念和哲学思想 202
7.2.3 项目结构 204
7.2.4 谈谈构建系统 205
7.2.5 工具链和开发环境 206
7.2.6 BOM定义 207
7.3 部署 208
7.3.1 Bigtop Provisioner 208
7.3.2 群集的无主节点Puppet部署 209
7.3.3 使用Puppet进行配置管理 213
7.4 集成验证 215
7.4.1 iTests和验证应用程序 216
7.4.2 栈集成测试开发 217
7.4.3 栈的验证 220
7.4.4 群集故障测试 221
7.4.5 栈的冒烟测试 222
7.5 将所有工作组合在一起 223
7.6 小结 224
第8章 Hadoop软件栈的In-Memory计算 227
8.1 In-Memory计算简介 229
8.2 Apache Ignite：内存优先 231
8.2.1 Apache Ignite的系统体系架构 232
8.2.2 数据网格 233
8.2.3 高可用性讨论 236
8.2.4 计算网格 237
8.2.5 服务网格 238
8.2.6 内存管理 238
8.2.7 持久化存储 240
8.3 使用Ignite加速旧式Hadoop 240
8.3.1 In-Memory存储的好处 241
8.3.2 内存文件系统：HDFS缓存 242
8.3.3 In-Memory MapReduce 243
8.4 Apache Ignite的高级用法 247
8.4.1 Spark和Ignite 247
8.4.2 共享状态 249
8.4.3 Hadoop上的In-Memory SQL 251
8.4.4 使用Ignite的SQL 252
8.4.5 使用Apache Ignite进行流处理 255
8.5 小结 256
术语表 259
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据解决方案
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南(第3版)
" 目录
第1章 初识Hadoop 1
1.1 数据！数据！ 1
1.2 数据的存储与分析 3
1.3 相较于其他系统的优势 4
1.3.1 关系型数据库管理系统 5
1.3.2 网格计算 7
1.3.3 志愿计算 9
1.4 Hadoop发展简史 10
1.5 Apache Hadoop和Hadoop生态系统 14
1.6 Hadoop的发行版本 15
1.6.1 本书包含的内容 16
1.6.2 兼容性 17
第2章 关于MapReduce 19
2.1 气象数据集 19
2.2 使用Unix工具来分析数据 21
2.3 使用Hadoop来分析数据 23
2.3.1 map和reduce 23
2.3.2 Java MapReduce 24
2.4 横向扩展 33
2.4.1 数据流 34
2.4.2 combiner函数 37
2.4.3 运行分布式的MapReduce作业 39
2.5 Hadoop Streaming 40
2.5.1 Ruby版本 40
2.5.2 Python版本 43
2.6 Hadoop Pipes 44
第3章 Hadoop分布式文件系统 49
3.1 HDFS的设计 49
3.2 HDFS的概念 51
3.2.1 数据块 51
3.2.2 namenode和datanode 52
3.2.3 联邦HDFS 53
3.2.4 HDFS的高可用性 54
3.3 命令行接口 56
3.4 Hadoop文件系统 58
3.5 Java接口 62
3.5.1 从Hadoop URL读取数据 63
3.5.2 通过FileSystem API读取数据 64
3.5.3 写入数据 68
3.5.4 目录 70
3.5.5 查询文件系统 70
3.5.6 删除数据 75
3.6 数据流 75
3.6.1 剖析文件读取 75
3.6.2 剖析文件写入 78
3.6.3 一致模型 81
3.7 通过Flume和Sqoop导入数据 83
3.8 通过distcp并行复制 84
3.9 Hadoop存档 86
3.9.1 使用Hadoop存档工具 86
3.9.2 不足 88
第4章 Hadoop的I/O操作 89
4.1 数据完整性 89
4.1.1 HDFS的数据完整性 89
4.1.2 LocalFileSystem 91
4.1.3 ChecksumFileSystem 91
4.2 压缩 92
4.2.1 codec 93
4.2.2 压缩和输入分片 98
4.2.3 在MapReduce中使用压缩 99
4.3 序列化 102
4.3.1 Writable接口 103
4.3.2 Writable类 105
4.3.3 实现定制的Writable集合 114
4.3 序列化框架 118
4.4 Avro 121
4.4.1 Avro数据类型和模式 122
4.4.2 内存中的序列化和反序列化 126
4.4.3 Avro数据文件 129
4.4.4 互操作性 130
4.4.5 模式的解析 133
4.4.6 排列顺序 135
4.4.7 关于Avro MapReduce 137
4.4.8 使用Avro MapReduce进行排序 141
4.4.9 其他语言的Avro MapReduce 143
4.5 基于文件的数据结构 143
4.5.1 关于SequenceFile 143
4.5.2 关于MapFile 151
第5章 MapReduce应用开发 157
5.1 用于配置的API 157
5.1.1 资源合并 159
5.1.2 可变的扩展 160
5.2 配置开发环境 160
5.2.1 管理配置 162
5.2.2 辅助类GenericOptionsParser，Tool和ToolRunner 165
5.3 用MRUnit来写单元测试 168
5.3.1 关于Mapper 168
5.3.2 关于Reducer 170
5.4 本地运行测试数据 171
5.4.1 在本地作业运行器上运行作业 171
5.4.2 测试驱动程序 175
5.5 在集群上运行 176
5.5.1 打包作业 177
5.5.2 启动作业 179
5.5.3 MapReduce的Web界面 181
5.5.4 获取结果 184
5.5.5 作业调试 185
5.5.6 Hadoop日志 190
5.5.7 远程调试 192
5.6 作业调优 193
5.7 MapReduce的工作流 196
5.7.1 将问题分解成MapReduce作业 197
5.7.2 关于JobControl 198
5.7.3 关于Apache Oozie 199
第6章 MapReduce的工作机制 205
6.1 剖析MapReduce作业运行机制 205
6.1.1 经典的MapReduce (MapReduce 1) 206
6.1.2 YARN (MapReduce 2) 213
6.2 失败 219
6.2.1 经典MapReduce中的失败 219
6.2.2 YARN中的失败 222
6.3 作业的调度 224
6.3.1 公平调度器 225
6.3.2 容量调度器 225
6.4 shuffle和排序 226
6.4.1 map端 226
6.4.2 reduce端 228
6.4.3 配置调优 230
6.5 任务的执行 232
6.5.1 任务执行环境 232
6.5.2 推测执行 233
6.5.3 关于OutputCommitters 235
6.5.4 任务JVM重用 237
6.5.5 跳过坏记录 238
第7章 MapReduce的类型与格式 241
7.1 MapReduce的类型 241
7.1.1 默认的MapReduce作业 245
7.1.2 默认的Streaming作业 249
7.2 输入格式 252
7.2.1 输入分片与记录 252
7.2.2 文本输入 264
7.2.3 二进制输入 268
7.2.4 多个输入 269
7.2.5 数据库输入(和输出) 270
7.3 输出格式 271
7.3.1 文本输出 271
7.3.2 二进制输出 272
7.3.3 多个输出 272
7.3.4 延迟输出 277
7.3.5 数据库输出 277
第8章 MapReduce的特性 279
8.1 计数器 279
8.1.1 内置计数器 279
8.1.2 用户定义的Java计数器 284
8.1.3 用户定义的Streaming计数器 289
8.2 排序 289
8.2.1 准备 290
8.2.2 部分排序 291
8.2.3 全排序 295
8.2.4 辅助排序 299
8.3 连接 305
8.3.1 map端连接 307
8.3.2 reduce端连接 307
8.4 边数据分布 311
8.4.1 利用JobConf来配置作业 311
8.4.2 分布式缓存 311
8.5 MapReduce库类 318
第9章 构建Hadoop集群 321
9.1 集群规范 321
9.2 集群的构建和安装 325
9.2.1 安装Java 326
9.2.2 创建Hadoop用户 326
9.2.3 安装Hadoop 326
9.2.4 测试安装 327
9.3 SSH配置 327
9.4 Hadoop配置 328
9.4.1 配置管理 329
9.4.2 环境设置 332
9.4.3 Hadoop守护进程的关键属性 336
9.4.4 Hadoop守护进程的地址和端口 341
9.4.5 Hadoop的其他属性 343
9.4.6 创建用户帐号 346
9.5 YARN配置 346
9.5.1 YARN守护进程的重要属性 347
9.5.2 YARN守护进程的地址和端口 350
9.6 安全性 352
9.6.1 Kerberos和Hadoop 353
9.6.2 委托令牌 355
9.6.3 其他安全性改进 356
9.7 利用基准评测程序测试Hadoop集群 358
9.7.1 Hadoop基准评测程序 358
9.7.2 用户作业 361
9.8 云端的Hadoop 361
第10章 管理Hadoop 367
10.1 HDFS 367
10.1.1 永久性数据结构 367
10.1.2 安全模式 373
10.1.3 日志审计 375
10.1.4 工具 375
10.2 监控 380
10.2.1 日志 381
10.2.2 度量 382
10.2.3 Java管理扩展(JMX) 385
10.3 维护 387
10.3.1 日常管理过程 387
10.3.2 委任和解除节点 389
10.3.3 升级 392
第11章 关于Pig 397
11.1 安装与运行Pig 398
11.1.1 执行类型 399
11.1.2 运行Pig程序 400
11.1.3 Grunt 401
11.1.4 Pig Latin编辑器 401
11.2 示例 402
11.3 与数据库进行比较 405
11.4 Pig Latin 406
11.4.1 结构 407
11.4.2 语句 408
11.4.3 表达式 413
11.4.4 类型 414
11.4.5 模式 415
11.4.6 函数 420
11.4.7 宏 422
11.5 用户自定义函数 423
11.5.1 过滤UDF 423
11.5.2 计算UDF 427
11.5.3 加载UDF 429
11.6 数据处理操作 432
11.6.1 数据的加载和存储 432
11.6.2 数据的过滤 433
11.6.3 数据的分组与连接 436
11.6.4 数据的排序 441
11.6.5 数据的组合和切分 442
11.7 Pig实战 443
11.7.1 并行处理 443
11.7.2 参数代换 444
第12章 关于Hive 447
12.1 安装Hive 448
12.2 示例 450
12.3 运行Hive 451
12.3.1 配置Hive 452
12.3.2 Hive服务 454
12.3.3 Metastore 456
12.4 Hive与传统数据库相比 458
12.4.1 读时模式vs.写时模式 458
12.4.2 更新、事务和索引 459
12.5 HiveQL 460
12.5.1 数据类型 461
12.5.2 操作与函数 463
12.6 表 464
12.6.1 托管表和外部表 465
12.6.2 分区和桶 466
12.6.3 存储格式 471
12.6.4 导入数据 477
12.6.5 表的修改 479
12.6.6 表的丢弃 480
12.7 查询数据 480
12.7.1 排序和聚集 480
12.7.2 MapReduce脚本 481
12.7.3 连接 482
12.7.4 子查询 486
12.7.5 视图 486
12.8 用户定义函数 488
12.8.1 写UDF 489
12.8.2 写UDAF 491
第13章 关于HBase 497
13.1 HBase基础 497
13.2 概念 498
13.3.1 数据模型的“旋风之旅” 498
13.3.2 实现 500
13.3 安装 503
13.4 客户端 506
13.4.1 Java 506
13.4.2 Avro、REST和Thrift 510
13.5 示例 511
13.5.1 模式 511
13.5.2 加载数据 512
13.5.3 Web查询 516
13.6 HBase和RDBMS的比较 519
13.6.1 成功的服务 520
13.6.2 HBase 521
13.6.3 实例：HBase在Streamy.com的使用 522
13.7 Praxis 524
13.7.1 版本 524
13.7.2 HDFS 525
13.7.3 用户界面 526
13.7.4 度量 526
13.7.5 模式的设计 526
13.7.6 计数器 527
13.7.7 批量加载 528
第14章 关于ZooKeeper 529
14.1 安装和运行ZooKeeper 530
14.2 示例 532
14.2.1 ZooKeeper中的组成员关系 533
14.2.2 创建组 534
14.2.3 加入组 536
14.2.4 列出组成员 537
14.2.5 删除组 539
14.3 ZooKeeper服务 540
14.3.1 数据模型 540
14.3.2 操作 543
14.3.3 实现 548
14.3.4 一致性 549
14.3.5 会话 552
14.3.6 状态 554
14.4 使用ZooKeeper来构建应用 555
14.4.1 配置服务 555
14.4.2 可复原的ZooKeeper应用 559
14.4.3 锁服务 563
14.4.4 更多分布式数据结构和协议 565
14.5 生产环境中的ZooKeeper 567
14.5.1 可恢复性和性能 567
14.5.2 配置 568
第15章 关于Sqoop 571
15.1 获取Sqoop 571
15.2 Sqoop连接器 573
15.3 一个导入的例子 573
15.4 生成代码 577
15.5 深入了解数据库导入 578
15.5.1 导入控制 580
15.5.2 导入和一致性 581
15.5.3 直接模式导入 581
15.6 使用导入的数据 581
15.7 导入大对象 585
15.8 执行导出 587
15.9 深入了解导出功能 589
15.9.1 导出与事务 590
15.9.2 导出和SequenceFile 591
第16章 实例学习 593
16.1 Hadoop 在Last.fm的应用 593
16.1.1 Last.fm：社会音乐史上的革命 593
16.1.2 Hadoop在Last.fm中的应用 593
16.1.3 用Hadoop制作图表 594
16.1.4 Track Statistics程序 595
16.1.5 总结 602
16.2 Hadoop和Hive在Facebook的应用 603
16.2.1 Hadoop在Facebook的使用 603
16.2.2 虚构的使用样例 606
16.2.3 Hive 609
16.2.4 存在的问题与未来工作计划 613
16.3 Nutch搜索引擎 615
16.3.1 背景介绍 615
16.3.2 数据结构 616
16.3.3 Nutch系统利用Hadoop进行数据处理的精选实例 619
16.3.4 总结 630
16.4 Rackspace的日志处理 631
16.4.1 要求/问题 631
16.4.2 简史 632
16.4.3 选择Hadoop 632
16.4.4 收集和存储 632
16.4.5 对日志的MapReduce处理 634
16.5 关于Cascading 640
16.5.1 字段、元组和管道 641
16.5.2 操作 644
16.5.3 Tap、Scheme和Flow 645
16.5.4 Cascading实战 646
16.5.5 灵活性 650
16.5.6 Hadoop和Cascading在ShareThis的应用 650
16.5.7 总结 655
16.6 Apache Hadoop上万亿数量级排序 655
16.7 用Pig和Wukong探索10亿数量级边的网络图 659
16.7.1 社区判断 661
16.7.2 每个人都在和我说话：Twitter回复关系图 661
16.7.3 对称链接 664
16.7.4 社区提取 666
附录A 安装Apache Hadoop 669
附录B 关于CDH 675
附录C 准备NCDC气象数据 677
"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南(第3版)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop集群与安全
目　录
译者序
作者简介
审校者简介
前言
第1章　构建Hadoop集群1
1.1　选择Hadoop集群硬件2
1.1.1　选择DataNode硬件3
1.1.2　低存储密度集群4
1.1.3　高存储密度集群5
1.1.4　NameNode和JobTracker硬件配置6
1.1.5　网关和其他辅助服务8
1.1.6　网络配置8
1.1.7　Hadoop硬件总结9
1.2　Hadoop发行版10
1.2.1　Hadoop版本10
1.2.2　选择Hadoop发行版11
1.2.3　Cloudera Hadoop 发行版11
1.2.4　Hortonworks Hadoop发行版12
1.2.5　MapR12
1.3　为Hadoop集群选择操作系统13
1.4　小结14
第2章　安装和配置Hadoop15
2.1　在Hadoop集群中配置操作系统15
2.1.1　选择和设置文件系统15
2.1.2　设置Java开发包16
2.1.3　其他操作系统设定17
2.1.4　设置CDH存储库18
2.2　设置NameNode18
2.2.1　JournalNode节点、ZooKeeper以及故障转移控制器22
2.2.2Hadoop配置文件23
2.2.3　NameNode高可用方案配置25
2.2.4　JobTracker配置31
2.2.5DataNode配置36
2.3　小结47
第3章　配置Hadoop生态系统48
3.1托管Hadoop生态项目48
3.2　Sqoop49
3.2.1安装和配置Sqoop49
3.2.2　Sqoop导入示例50
3.2.3　Sqoop导出示例52
3.3　Hive52
3.3.1Hive架构53
3.3.2安装Hive Metastore54
3.3.3　安装Hive客户端　56
3.3.4　安装Hive Server57
3.4Impala59
3.4.1　Impala架构59
3.4.2　安装Impala state store60
3.4.3　安装Impala server60
3.5　小结63
第4章　Hadoop安全64
4.1　Hadoop安全概述64
4.2　Hadoop分布式文件系统安全65
4.3　MapReduce安全66
4.4　Hadoop服务级别验证 68
4.5　Hadoop和Kerberos69
4.5.1　Kerberos概述70
4.5.2　Hadoop中的Kerberos71
4.6　小结76
第5章　监控Hadoop集群77
5.1　监控策略介绍77
5.2　Hadoop参数78
5.2.1　JMX参数79
5.2.2　使用Nagios监控Hadoop80
5.2.3　监控Hadoop分布式文件系统81
5.2.4　NameNode校验81
5.2.5　JournalNode检查83
5.2.6　ZooKeeper检查83
5.3　监控MapReduce84
5.4　使用Ganglia监控Hadoop85
5.5　小结86
第6章　在云端使用Hadoop87
6.1　Amazon Elastic MapReduce87
6.1.1　安装EMR命令行接口88
6.1.2　选择Hadoop版本89
6.1.3　启动EMR集群89
6.2　使用Whirr93
6.3　小结94
第7章　Hadoop平台安全概述95
7.1　为什么需要保障Hadoop生态系统的安全96
7.2　确保Hadoop生态系统安全面临的挑战96
7.3　关键安全因素97
7.4　小结99
第8章　Hadoop安全体系设计100
8.1　什么是Kerberos100
8.1.1　Kerberos关键术语101
8.1.2　Kerberos如何工作102
8.1.3　Kerberos 的优点103
8.2　不采用Kerberos的Hadoop默认安全模型103
8.3　Hadoop Kerberos 安全模型实现105
8.3.1　用户层次的访问控制105
8.3.2　服务层次的访问控制105
8.3.3　用户和服务认证106
8.3.4　授权令牌106
8.3.5　作业令牌106
8.3.6　数据块访问令牌107
8.4　小结108
第9章　配置一个安全Hadoop集群109
9.1　前提条件109
9.2　设置Kerberos110
9.3　配置Hadoop使用Kerberos认证117
9.3.1　在所有Hadoop节点设置Kerberos客户端117
9.3.2　配置Hadoop服务标识118
9.4　Hadoop用户设置124
9.5　安全Hadoop自动部署124
9.6　小结125
第10章　Hadoop生态系统安全保障126
10.1　为Hadoop生态系统组件配置Kerberos127
10.1.1　Hive安全设置127
10.1.2　Oozie安全设置130
10.1.3　Flume安全设置131
10.1.4　HBase安全设置134
10.1.5　Sqoop安全设置137
10.1.6　Pig安全设置138
10.2　Hadoop生态系统组件安全保障最佳实践138
10.3　小结139
第11章　集成Hadoop与企业安全系统140
11.1　集成EIM系统141
11.1.1　配置EIM与Hadoop集成142
11.1.2　集成基于Active Directory的EIM系统与Hadoop生态系统143
11.2　从企业网络访问安全Hadoop集群144
11.2.1　HttpFS145
11.2.2　HUE145
11.2.3　Knox Gateway Server146
11.3　小结147
第12章　Hadoop中敏感数据安全保护148
12.1　Hadoop中敏感数据及保护方法148
12.2　小结154
第13章　安全事件与审计日志155
13.1　Hadoop集群安全事故和事件监控155
13.2　Hadoop集群审计日志设置158
13.3　小结160
附录　Hadoop安全机制解决方案161
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop集群与安全
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析 : 基于Storm、Spark等Hadoop替代技术的实时应用
目录
前言
致谢
关于作者
1 引言：为什么要超越 Hadoop Map-Reduce 1
Hadoop的适用范围 3
大数据分析之机器学习实现的革命 10
第一代机器学习工具 /范式 11
第二代机器学习工具 /范式 11
第三代机器学习工具 /范式 14
小结 18
参考文献 19
2 何为伯克利数据分析栈（BDAS） 23
实现 BDAS的动机 24
Spark：动机 25
Shark：动机 26
Mesos：动机 28
BDAS的设计及架构 29
Spark：高效的集群数据处理的范式 34
Spark的弹性分布式数据集 36
Spark的实现 40
Spark VS. 分布式共享内存系统 42
RDD的表达性 44
类似 Spark的系统 45
Shark：分布式系统上的 SQL接口 46
Spark为 Shark提供的扩展 47
列内存存储 49
分布式数据加载 50
完全分区智能连接 50
分区修剪 50
机器学习的支持 51
Mesos：集群调度及管理系统 51
Mesos组件 52
资源分配 54
隔离 55
容错性 57
小结 58
参考文献 59
3 使用 Spark实现机器学习算法 66
机器学习基础知识 66
机器学习：随机森林示例 68
逻辑回归：概述 72
二元形式的逻辑回归 73
逻辑回归估计 75
多元逻辑回归 76
Spark中的逻辑回归算法 77
支持向量机 80
复杂决策面 81
支持向量机背后的数学原理 82
Spark中的支持向量机 84
Spark对 PMML的支持 85
PMML结构 87
PMML的生产者及消费者 92
Spark对朴素贝叶斯的 PMML支持 94
Spark对线性回归的 PMML支持 95
在 Spark中使用 MLbase进行机器学习 97
参考文献 99
4 实现实时的机器学习算法 101
Storm简介 101
数据流 103
拓扑 104
Storm集群 105
简单的实时计算例子 106
数据流组 108
Storm的消息处理担保 109
基于 Storm的设计模式 111
分布式远程过程调用 111
Trident：基于 Storm的实时聚合 115
实现基于 Storm的逻辑回归算法 116
实现基于 Storm的支持向量机算法 120
Storm对朴素贝叶斯 PMML的支持 122
实时分析的应用 126
工业日志分类 126
互联网流量过滤器 130
Storm的替代品 131
Spark流 133
D-Streams的动机 133
参考文献 135
5 图处理范式 138
Pregel：基于 BSP的图处理框架 139
类似的做法 141
开源的 Pregel实现 143
Giraph 143
GoldenORB 145
Phoebus 145
Apache Hama 146
Stanford GPS 146
GraphLab 147
GraphLab：多核版本 148
分布式的 GraphLab 150
PowerGraph 152
通过 GraphLab实现网页排名算法 156
顶点程序 158
基于 GraphLab实现随机梯度下降算法 163
参考文献 167
6 结论：超越Hadoop Map-Reduce的大数据分析 171
Hadoop YARN概览 172
Hadoop YARN的动机 172
作为资源调度器的 YARN 174
YARN上的其他框架 175
大数据分析的未来是怎样的 177
参考文献 180
附录A 代码笔记 182
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析 : 基于Storm、Spark等Hadoop替代技术的实时应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop云计算实战
第1章 云计算概论 1
1.1 云计算概述 1
1.1.1 云计算的定义 1
1.1.2 云计算产生的背景 2
1.1.3 云时代谁是主角 3
1.1.4 云计算的特征 4
1.1.5 云计算的发展史 5
1.1.6 云计算的服务层次 7
1.1.7 云计算的服务形式 7
1.1.8 云计算的实现机制 9
1.1.9 云计算研究方向 11
1.1.10 云计算发展趋势 12
1.2 云计算关键技术研究 14
1.2.1 虚拟化技术 14
1.2.2 数据存储技术 15
1.2.3 资源管理技术 17
1.2.4 能耗管理技术 18
1.2.5 云监测技术 19
1.3 云计算应用研究 22
1.3.1 语义分析应用 22
1.3.2 IT企业应用 22
1.3.3 生物学应用 23
1.3.4 电信企业应用 24
1.3.5 数据库的应用 27
1.3.6 地理信息应用 28
1.3.7 医学应用 29
1.4 云安全 30
1.4.1 云安全发展趋势 31
1.4.2 云安全与网络安全的差别 31
1.4.3 云安全研究的方向 31
1.4.4 云安全难点问题 32
1.4.5 云安全新增及增强功能 32
1.5 云计算生命周期 33
1.6 云计算存在的问题 34
1.7 云计算的优缺点 35
第2章 Hadoop相关项目介绍 37
2.1 Hadoop简介 37
2.1.1 Hadoop的基本架构 37
2.1.2 Hadoop文件系统结构 40
2.1.3 Hadoop文件读操作 41
2.1.4 Hadoop文件写操作 42
2.2 Hadoop系统性质 42
2.2.1 可靠存储性 43
2.2.2 数据均衡 43
2.3 比较SQL数据库与Hadoop 44
2.4 MapReduce概述 45
2.4.1 MapReduce实现机制 45
2.4.2 MapReduce执行流程 46
2.4.3 MapReduce映射和化简 47
2.4.4 MapReduce输入格式 47
2.4.5 MapReduce输出格式 48
2.4.6 MapReduce运行速度 48
2.5 HBase概述 48
2.5.1 HBase的系统框架 49
2.5.2 HBase访问接口 51
2.5.3 HBase的存储格式 52
2.5.4 HBase的读写流程 52
2.5.5 Hbase的优缺点 53
2.6 ZooKeeper概述 53
2.6.1 为什么需要ZooKeeper 54
2.6.2 ZooKeeper设计目标 54
2.6.3 ZooKeeper数据模型 54
2.6.4 ZooKeeper工作原理 55
2.6.5 ZooKeeper实现机制 56
2.6.6 ZooKeeper的特性 57
2.7 Hive概述 58
2.7.1 Hive的组成 59
2.7.2 Hive结构解析 59
2.8 Pig概述 63
2.9 Cassandra概述 64
2.9.1 Cassandra主要功能 64
2.9.2 Cassandra的体系结构 65
2.9.3 Cassandra存储机制 65
2.9.4 Cassandra的写过程 66
2.9.5 Cassandra的读过程 67
2.9.6 Cassandra的删除 68
2.10 Chukwa概述 68
2.10.1 使用Chukwa的原因 68
2.10.2 Chukwa的不是 69
2.10.3 Chukwa的定义 69
2.10.4 Chukwa架构与设计 70
第3章 Hadoop配置与实战 74
3.1 Hadoop的安装 74
3.1.1 在Linux下安装Hadoop 74
3.1.2 运行模式 75
3.1.3 在Windows下安装Hadoop 80
3.2 运行Hadoop 86
3.3 Hadoop的Avatar机制 87
3.3.1 系统架构 88
3.3.2 元数据同步机制 89
3.3.3 切换故障过程 91
3.3.4 运行流程 92
3.3.5 切换故障流程 96
3.4 Hadoop实战 99
3.4.1 使用Hadoop运行wordcount实例 99
3.4.2 使用Eclipse编写Hadoop程序 101
第4章 Hadoop的分布式数据HDFS 102
4.1 HDFS的操作 102
4.1.1 文件操作 102
4.1.2 管理与更新 103
4.2 FS Shell使用指南 104
4.3 API使用 111
4.3.1 文件系统的常见操作 111
4.3.2 API的Java操作实例 113
第5章 Hadoop编程模型MapReduce 118
5.1 MapReduce基础 118
5.1.1 MapReduce编程模型 118
5.1.2 MapReduce实现机制 119
5.1.3 Java MapReduce 121
5.2 MapReduce的容错性 124
5.3 MapReduce实例分析 125
5.4 不带map()、reduce()的MapReduce 131
5.5 Shuffle过程 133
5.6 新增Hadoop API 136
5.7 Hadoop的Streaming 138
5.7.1 通过UNIX命令使用Streaming 138
5.7.2 通过Ruby版本使用Streaming 139
5.7.3 通过Python版本使用Streaming 141
5.8 MapReduce实战 142
5.8.1 MapReduce排序 142
5.8.2 MapReduce二次排序 145
5.9 MapReduce作业分析 153
5.10 定制MapReduce数据类型 156
5.10.1 内置的数据输入格式和RecordReader 156
5.10.2 定制输入数据格式与RecordReader 157
5.10.3 定制数据输出格式实现多集合文件输出 160
5.11 链接MapReduce作业 162
5.11.1 顺序链接MapReduce作业 162
5.11.2 复杂的MapReduce链接 163
5.11.3 前后处理的链接 163
5.11.4 链接不同的数据 166
5.12 Hadoop的Pipes 172
5.13 创建Bloom filter 174
5.13.1 Bloom filter作用 175
5.13.2 Bloom filter实现 175
第6章 Hadoop的数据库HBase 182
6.1 HBase数据模型 182
6.1.1 数据模型 182
6.1.2 概念视图 183
6.1.3 物理视图 184
6.2 HBase与RDBMS对比 185
6.3 Bigtable的应用实例 188
6.4 HBase的安装与配置 189
6.5 Java API 196
6.6 HBase实例分析 204
6.6.1 RowLock 204
6.6.2 HBase的HFileOutputFormat 207
6.6.3 HBase的TableOutputFormat 210
6.6.4 在HBase中使用MapReduce 213
6.6.5 HBase分布式模式 215
第7章 Hadoop的数据仓库Hive 220
7.1 Hive的安装 220
7.1.1 准备的软件包 220
7.1.2 内嵌模式安装 220
7.1.3 安装独立模式 221
7.1.4 远程模式安装 222
7.1.5 查看数据信息 222
7.2 Hive的入口 223
7.2.1 类CliDriver 225
7.2.2 类CliSessionState 229
7.2.3 类CommandProcessor 230
7.3 Hive QL详解 232
7.3.1 Hive的数据类型 232
7.3.2 Hive与数据库比较 233
7.3.3 DDL操作 234
7.3.4 join查询 241
7.3.5 DML操作 243
7.3.6 SQL操作 245
7.3.7 Hive QL的应用实例 248
7.4 Hive的服务 250
7.4.1 JDBC/ODBC服务 250
7.4.2 Thrift服务 253
7.4.3 Web接口 255
7.5 Hive SQL的优化 256
7.5.1 Hive SQL优化选项 256
7.5.2 Hive SQL优化应用实例 258
7.6 Hive的扩展性 261
7.6.1 SerDe 262
7.6.2 Map/Reduce脚本 263
7.6.3 UDF 263
7.6.4 UDAF 264
7.7 Hive实战 266
第8章 Hadoop的大规模数据平台Pig 274
8.1 Pig的安装与运行 274
8.1.1 Pig的安装 274
8.1.2 Pig的运行 274
8.2 Pig实现 278
8.3 Pig Latin语言 279
8.3.1 Pig Latin语言概述 280
8.3.2 Pig Latin数据类型 282
8.3.3 Pig Latin运算符 284
8.3.4 Pig Latin关键字 287
8.3.5 Pig内置函数 288
8.4 自定义函数 291
8.4.1 UDF的编写 292
8.4.2 UDFS的使用 293
8.5 Jaql和Pig查询语言的比较 293
8.5.1 Pig和Jaql运行环境和执行形式的比较 294
8.5.2 Pig和Jaql支持数据类型的比较 294
8.5.3 Pig和Jaql操作符和内建函数以及自定义函数的比较 295
8.5.4 其他 299
8.6 Pig实战 300
第9章 Hadoop的非关系型数据Cassandra 308
9.1 Cassandra的安装 308
9.1.1 在Windows 7中安装 308
9.1.2 在Linux中安装 310
9.2 Cassandra的数据模型 311
9.2.1 Column 311
9.2.2 SuperColumn 312
9.2.3 ColumnFamily 312
9.2.4 Row 313
9.2.5 排序 313
9.3 Cassandra的实例分析 315
9.3.1 Cassandra的数据存储结构 315
9.3.2 跟踪客户端代码 319
9.4 Cassandra常用的编程语言 324
9.4.1 Java使用Cassandra 324
9.4.2 PHP使用Cassandra 325
9.4.3 Python使用Cassandra 326
9.4.4 C#使用Cassandra 327
9.4.5 Ruby使用Cassandra 328
9.5 Cassandra与MapReduce结合 328
9.5.1 需求分析 329
9.5.2 代码分析 330
9.5.3 MapReduce代码 330
9.6 Cassandra实战 331
9.6.1 BuyerDao功能验证 331
9.6.2 SellerDao功能验证 332
9.6.3 ProductDao功能验证 333
9.6.4 新建Schema在线功能 336
9.6.5 功能验证 337
第10章 Hadoop的收集数据Chukwa 339
10.1 Chukwa的安装与配置 339
10.1.1 配置要求 339
10.1.2 Chukwa的安装 340
10.1.3 基本命令 341
10.2 Chukwa数据流处理 344
10.2.1 支持数据类型 344
10.2.2 数据处理 345
10.2.3 自定义数据模块 351
10.3 Chukwa源代码分析 352
10.3.1 Chukwa适配器 352
10.3.2 Chukwa连接器 357
10.3.3 Chukwa收集器 362
10.4 Chukwa实例分析 366
10.4.1 生成数据 366
10.4.2 收集数据 367
10.4.3 处理数据 367
10.4.4 析取数据 368
10.4.5 稀释数据 368
第11章 Hadoop的分布式系统ZooKeeper 369
11.1 ZooKeeper的安装与配置 369
11.1.1 ZooKeeper的安装 369
11.1.2 ZooKeeper的配置 371
11.1.3 ZooKeeper数据模型 373
11.1.4 ZooKeeper的API接口 373
11.1.5 ZooKeeper编程实现 375
11.2 ZooKeeper的Leader流程 378
11.3 ZooKeeper锁服务 379
11.3.1 ZooKeeper中的锁机制 379
11.3.2 ZooKeeper的写锁实现 380
11.3.3 ZooKeeper锁服务实现例子 381
11.4 创建ZooKeeper应用程序 383
11.5 ZooKeeper的应用开发 387
11.6 ZooKeeper的典型应用 395
11.6.1 统一命名服务 396
11.6.2 配置管理 396
11.6.3 集群管理 397
11.6.4 共享锁 398
11.6.5 队列管理 399
11.7 实现NameNode自动切换 402
网上参考资源 410
参考文献 412
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop云计算实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop应用架构
版权声明
O'Reilly Media, Inc. 介绍
译者序
序
前言
第一部分　考虑 Hadoop 应用的架构设计
第 1 章　Hadoop 数据建模
第 2 章　Hadoop 数据移动
第 3 章　Hadoop 数据处理
第 4 章　Hadoop 数据处理通用范式
第 5 章　Hadoop 图处理
第 6 章　协调调度
第 7 章　Hadoop 近实时处理
第二部分　案例研究
第 8 章　点击流分析
第 9 章　欺诈检测
第 10 章　数据仓库
附录 A　Impala 中的关联
作者简介
封面介绍
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop应用架构
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据开发案例教程与项目实战
基础篇
第1章　Hadoop概述　1
1．1　Hadoop简介　1
1．2　Hadoop相关项目　2
1．3　Hadoop来源　3
1．4　Hadoop的发展史　4
1．5　Hadoop特点　5
1．6　Hadoop体系架构　6
1．6．1　HDFS体系结构　7
1．6．2　MapReduce体系结构　7
本章小结　8
习题　8
第2章　Hadoop基础环境配置　9
2．1　准备Linux环境　9
2．1．1　安装VMware12虚拟机　9
2．1．2　部署CentOS 64位操作系统　11
2．2　Linux配置　16
2．2．1　什么是Linux　16
2．2．2　Linux发行版　16
2．2．3　配置网络　16
2．2．4　Linux终端　17
2．3　Hadoop环境搭建　21
2．3．1　JDK安装和测试　21
2．3．2　Hadoop安装和配置　25
2．3．3　SSH免密码配置　31
本章小结　33
习题　34
第3章　分布式存储HDFS　35
3．1　HDFS概念　35
3．1．1　HDFS简介　35
3．1．2　HDFS设计思路和理念　35
3．2　HDFS体系结构　36
3．3　HDFS文件存储机制　36
3．4　HDFS Shell介绍　39
3．4．1　命令格式　39
3．4．2　HDFS用户命令　40
3．4．3　HDFS管理员命令　40
3．5　Hadoop项目创建　47
3．6　RPC通信原理　53
3．6．1　什么是Hadoop的RPC　53
3．6．2　RPC采用的模式　53
3．7　分布式文件系统操作类　59
本章小结　69
习题　69
第4章　计算系统MapReduce　70
4．1　MapReduce概念　70
4．1．1　MapReduce简介　70
4．1．2　MapReduce 数据类型与格式　71
4．1．3　数据类型Writable接口　71
4．1．4　Hadoop序列化机制　72
4．2　MapReduce架构　72
4．2．1　数据分片　72
4．2．2　MapReduce执行过程　73
4．2．3　Mapper执行过程　73
4．2．4　Reducer执行过程　74
4．2．5　Shuffle过程　75
4．3　第一个MapReduce案例　75
4．4　MapReduce接口类　79
4．4．1　MapReduce输入的处理类　79
4．4．2　MapReduce输出的处理类　80
本章小结　87
习题　87
第5章　计算模型Yarn　88
5．1　Yarn概述　88
5．1．1　Yarn简介　88
5．1．2　Yarn的组成　89
5．2　Yarn的执行过程　89
5．3　新旧MapReduce的对比　90
本章小结　101
习题　101
第6章　数据云盘　102
6．1　项目概述　102
6．2　功能需求　102
6．3　软件开发需求　102
6．4　效果展示　103
6．5　系统开发　104
本章小结　125
习题　125
提高篇
第7章　协调系统Zookeeper　126
7．1　Zookeeper概述　126
7．1．1　Zookeeper简介　126
7．1．2　Zookeeper数据模型　127
7．1．3　Zookeeper特征　127
7．1．4　Zookeeper工作原理　128
7．2　Zookeeper术语　129
7．2．1　节点　129
7．2．2　角色　129
7．2．3　顺序号　129
7．2．4　观察　129
7．2．5　Leader选举　129
7．3　事件　130
7．4　Zookeeper Shell操作　130
7．4．1　Zookeeper服务命令　130
7．4．2　Zookeeper客户端命令　134
7．5　Zookeeper API操作　137
本章小结　156
习题　156
第8章　Hadoop数据库Hbase　157
8．1　Hbase概述　157
8．1．1　Hbase简介　157
8．1．2　Hbase优势和特点　158
8．1．3　Hbase专业术语　158
8．2　Hbase架构　158
8．2．1　角色　159
8．2．2　Hbase物理存储和逻辑视图　160
8．3　Hbase Shell操作　163
8．4　Hbase API操作　168
8．5　Hbase 过滤器　182
8．5．1　过滤器的含义　182
8．5．2　过滤器的比较操作符　182
8．5．3　过滤器的比较器　183
本章小结　193
习题　193
第9章　Hadoop数据仓库Hive　194
9．1　Hive概述　194
9．1．1　Hive简介　194
9．1．2　Hive数据类型　194
9．1．3　Hive Metastore　195
9．1．4　Hive存储和压缩　195
9．1．5　Hive与传统数据库对比　195
9．2　Hive的系统架构　196
9．3　Hive的数据模型　200
9．3．1　内部表　200
9．3．2　外部表　200
9．3．3　分区表　201
9．3．4　桶表　201
9．4　Hive Shell操作　201
9．5　Hive API操作　208
9．6　Hive内置函数和UDF　215
9．6．1　内置函数　215
9．6．2　UDF函数　215
本章小结　222
习题　222
第10章　Hadoop数据采集Flume　223
10．1　Flume概述　223
10．1．1　Flume简介　223
10．1．2　Flume核心概念　223
10．1．3　Flume 系统要求　224
10．2　Flume架构　224
10．3　Flume常见操作命令　225
10．4　Flume环境搭建　226
10．4．1　设置一个Agent　226
10．4．2　启动Agent　226
本章小结　231
习题　231
第11章　OTA离线数据分析平台　232
11．1　项目概述　232
11．2　功能需求　233
11．3　软件开发关键技术　233
11．4　效果展示　233
11．5　平台搭建与测试　233
11．5．1　配置ssh免密码登录　233
11．5．2　配置JDK　234
11．5．3　配置Hadoop　236
11．5．4　配置Hive　242
11．6　数据收集　247
11．6．1　解压Flume　247
11．6．2　修改配置文件　248
11．6．3　启动Flume　248
11．6．4　校验数据　248
11．7　数据分析　249
11．7．1　数据清洗　249
11．7．2　ETL编程　256
11．7．3　业务分析　261
11．7．4　配置Sqoop　264
11．7．5　从HDFS导出数据至MySQL　267
11．8　数据展示　268
11．8．1　搭建Web开发环境　268
11．8．2　添加代码　272
11．8．3　项目结构　282
11．8．4　启动Tomcat　283
11．8．5　访问Web页面　283
本章小结　283
习题　284
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据开发案例教程与项目实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>精通Hadoop
第1章　Hadoop 2.X　　1
1.1　Hadoop的起源　　1
1.2　Hadoop的演进　　2
1.3　Hadoop 2.X　　6
1.3.1　Yet Another Resource Negotiator（YARN）　　7
1.3.2　存储层的增强　　8
1.3.3　支持增强　　11
1.4　Hadoop的发行版　　11
1.4.1　选哪个Hadoop发行版　　12
1.4.2　可用的发行版　　14
1.5　小结　　16
第2章　MapReduce进阶　　17
2.1　MapReduce输入　　18
2.1.1　InputFormat类　　18
2.1.2　InputSplit类　　18
2.1.3　RecordReader类　　19
2.1.4　Hadoop的“小文件”问题　　20
2.1.5　输入过滤　　24
2.2　Map任务　　27
2.2.1　dfs.blocksize属性　　28
2.2.2　中间输出结果的排序与溢出　　28
2.2.3　本地reducer和Combiner　　31
2.2.4　获取中间输出结果——Map 侧　　31
2.3　Reduce任务　　32
2.3.1　获取中间输出结果——Reduce侧　　32
2.3.2　中间输出结果的合并与溢出　　33
2.4　MapReduce的输出　　34
2.5　MapReduce作业的计数器　　34
2.6　数据连接的处理　　36
2.6.1　Reduce侧的连接　　36
2.6.2　Map侧的连接　　42
2.7　小结　　45
第3章　Pig进阶　　47
3.1　Pig对比SQL　　48
3.2　不同的执行模式　　48
3.3　Pig的复合数据类型　　49
3.4　编译Pig脚本　　50
3.4.1　逻辑计划　　50
3.4.2　物理计划　　51
3.4.3　MapReduce计划　　52
3.5　开发和调试助手　　52
3.5.1　DESCRIBE命令　　52
3.5.2　EXPLAIN命令　　53
3.5.3　ILLUSTRATE命令　　53
3.6　Pig 操作符的高级特性　　54
3.6.1　FOREACH操作符进阶　　54
3.6.2　Pig的特殊连接　　58
3.7　用户定义函数　　61
3.7.1　运算函数　　61
3.7.2　加载函数　　66
3.7.3　存储函数　　68
3.8　Pig的性能优化　　69
3.8.1　优化规则　　69
3.8.2　Pig脚本性能的测量　　71
3.8.3　Pig的Combiner　　72
3.8.4　Bag数据类型的内存　　72
3.8.5　Pig的reducer数量　　72
3.8.6　Pig的multiquery模式　　73
3.9　最佳实践　　73
3.9.1　明确地使用类型　　74
3.9.2　更早更频繁地使用投影　　74
3.9.3　更早更频繁地使用过滤　　74
3.9.4　使用LIMIT操作符　　74
3.9.5　使用DISTINCT操作符　　74
3.9.6　减少操作　　74
3.9.7　使用Algebraic UDF　　75
3.9.8　使用Accumulator UDF　　75
3.9.9　剔除数据中的空记录　　75
3.9.10　使用特殊连接　　75
3.9.11　压缩中间结果　　75
3.9.12　合并小文件　　76
3.10　小结　　76
第4章　Hive进阶　　77
4.1　Hive架构　　77
4.1.1　Hive元存储　　78
4.1.2　Hive编译器　　78
4.1.3　Hive执行引擎　　78
4.1.4　Hive的支持组件　　79
4.2　数据类型　　79
4.3　文件格式　　80
4.3.1　压缩文件　　80
4.3.2　ORC文件　　81
4.3.3　Parquet文件　　81
4.4　数据模型　　82
4.4.1　动态分区　　84
4.4.2　Hive表索引　　85
4.5　Hive查询优化器　　87
4.6　DML进阶　　88
4.6.1　GROUP BY操作　　88
4.6.2　ORDER BY与SORT BY　　88
4.6.3　JOIN类型　　88
4.6.4　高级聚合　　89
4.6.5　其他高级语句　　90
4.7　UDF、UDAF和UDTF　　90
4.8　小结　　93
第5章　序列化和Hadoop I/O　　95
5.1　Hadoop数据序列化　　95
5.1.1　Writable与WritableComparable　　96
5.1.2　Hadoop与Java序列化的区别　　 98
5.2　Avro序列化　　100
5.2.1　Avro与MapReduce　　102
5.2.2　Avro与Pig　　105
5.2.3　Avro与Hive　　106
5.2.4　比较Avro与Protocol Buffers/Thrift　　107
5.3　文件格式　　108
5.3.1　Sequence文件格式　　108
5.3.2　MapFile格式　　111
5.3.3　其他数据结构　　113
5.4　压缩　　113
5.4.1　分片与压缩　　114
5.4.2　压缩范围　　115
5.5　小结　　115
第6章　YARN——其他应用模式进入Hadoop的引路人　　116
6.1　YARN的架构　　117
6.1.1　资源管理器　　117
6.1.2　Application Master　　118
6.1.3　节点管理器　　119
6.1.4　YARN客户端　　120
6.2　开发YARN的应用程序　　120
6.2.1　实现YARN客户端　　120
6.2.2　实现AM实例　　125
6.3　YARN的监控　　129
6.4　YARN中的作业调度　　134
6.4.1　容量调度器　　134
6.4.2　公平调度器　　137
6.5　YARN命令行　　139
6.5.1　用户命令　　140
6.5.2　管理员命令　　140
6.6　小结　　141
第7章　基于YARN的Storm——Hadoop中的低延时处理　　142
7.1　批处理对比流式处理　　142
7.2　Apache Storm　　144
7.2.1　Apache Storm的集群架构　　144
7.2.2　Apache Storm的计算和数据模型　　145
7.2.3　Apache Storm用例　　146
7.2.4　Apache Storm的开发　　147
7.2.5　Apache Storm 0.9.1　　153
7.3　基于YARN的Storm　　154
7.3.1　在YARN上安装Apache Storm　　154
7.3.2　安装过程　　154
7.4　小结　　161
第8章　云上的Hadoop　　162
8.1　云计算的特点　　162
8.2　云上的Hadoop　　163
8.3　亚马逊Elastic MapReduce　　164
8.4　小结　　175
第9章　HDFS替代品　　176
9.1　HDFS的优缺点　　176
9.2　亚马逊AWS S3　　177
9.3　在Hadoop中实现文件系统　　179
9.4　在Hadoop中实现S3原生文件系统　　179
9.5　小结　　189
第10章　HDFS联合　　190
10.1　旧版HDFS架构的限制　　190
10.2　HDFS联合的架构　　192
10.2.1　HDFS联合的好处　　193
10.2.2　部署联合NameNode　　193
10.3　HDFS高可用性　　195
10.3.1　从NameNode、检查节点和备份节点　　195
10.3.2　高可用性——共享edits　　196
10.3.3　HDFS实用工具　　197
10.3.4　三层与四层网络拓扑　　197
10.4　HDFS块放置策略　　198
10.5　小结　　200
第11章　Hadoop安全　　201
11.1　安全的核心　　201
11.2　Hadoop中的认证　　202
11.2.1　Kerberos认证　　202
11.2.2　Kerberos的架构和工作流　　203
11.2.3　Kerberos认证和Hadoop　　204
11.2.4　HTTP接口的认证　　204
11.3　Hadoop中的授权　　205
11.3.1　HDFS的授权　　205
11.3.2　限制HDFS的使用量　　208
11.3.3　Hadoop中的服务级授权　　209
11.4　Hadoop中的数据保密性　　211
11.5　Hadoop中的日志审计　　216
11.6　小结　　217
第12章　使用Hadoop进行数据分析　　 218
12.1　数据分析工作流　　218
12.2　机器学习　　220
12.3　Apache Mahout　　222
12.4　使用Hadoop和Mahout进行文档分析　　223
12.4.1　词频　　223
12.4.2　文频　　224
12.4.3　词频－逆向文频　　224
12.4.4　Pig中的Tf-idf　　225
12.4.5　余弦相似度距离度量　　228
12.4.6　使用k-means 的聚类　　228
12.4.7　使用Apache Mahout进行k-means聚类　　229
12.5　RHadoop　　233
12.6　小结　　233
附录 微软Windows中的Hadoop　　235
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>精通Hadoop
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战
第一部分 Hadoop——一种分布式编程框架
第1 章 Hadoop简介　　2
1.1 为什么写《Hadoop 实战》　　3
1.2 什么是Hadoop 　　3
1.3 了解分布式系统和Hadoop 　　4
1.4 比较SQL 数据库和Hadoop　　5
1.5 理解MapReduce　　6
1.5.1 动手扩展一个简单程序　　7
1.5.2 相同程序在MapReduce中的扩展　　9
1.6 用Hadoop统计单词——运行第一个程序　　11
1.7 Hadoop历史　　15
1.8 小结　　16
1.9 资源　　16
第2 章 初识Hadoop 　　17
2.1 Hadoop 的构造模块　　17
2.1.1 NameNode 　　17
2.1.2 DataNode 　　18
2.1.3 Secondary NameNode 　　19
2.1.4 JobTracker　　19
2.1.5 TaskTracker　　19
2.2 为Hadoop 集群安装SSH　　21
2.2.1 定义一个公共账号　　21
2.2.2 验证SSH安装　　21
2.2.3 生成SSH密钥对　　21
2.2.4 将公钥分布并登录验证　　22
2.3 运行Hadoop 　　22
2.3.1 本地（单机）模式　　23
2.3.2 伪分布模式　　24
2.3.3 全分布模式　　25
2.4 基于Web 的集群用户界面　　28
2.5 小结　　30
第3 章 Hadoop组件　　31
3.1 HDFS 文件操作　　31
3.1.1 基本文件命令　　32
3.1.2 编程读写HDFS　　35
3.2 剖析MapReduce 程序　　37
3.2.1 Hadoop数据类型　　39
3.2.2 Mapper　　40
3.2.3 Reducer　　41
3.2.4 Partitioner：重定向Mapper输出　　41
3.2.5 Combiner：本地reduce 　　43
3.2.6 预定义mapper和Reducer类的单词计数　　43
3.3 读和写　　43
3.3.1 InputFormat 　　44
3.3.2 OutputFormat　　49
3.4 小结　　50
第二部分 实战
第4 章 编写MapReduce基础程序　　52
4.1 获得专利数据集　　52
4.1.1 专利引用数据　　53
4.1.2 专利描述数据　　54
4.2 构建MapReduce 程序的基础模板　　55
4.3 计数　　60
4.4 适应Hadoop API 的改变　　64
4.5 Hadoop 的Streaming 　　 67
4.5.1 通过Unix命令使用Streaming 　　 68
4.5.2 通过脚本使用Streaming　　 69
4.5.3 用Streaming处理键/值对　　 72
4.5.4 通过Aggregate包使用Streaming　　75
4.6 使用combiner 提升性能　　 80
4.7 温故知新　　83
4.8 小结　　84
4.9 更多资源　　84
第5 章 高阶MapReduce 　　 85
5.1 链接MapReduce 作业　　 85
5.1.1 顺序链接MapReduce作业　　 85
5.1.2 具有复杂依赖的MapReduce链接　　86
5.1.3 预处理和后处理阶段的链接　　86
5.2 联结不同来源的数据　　 89
5.2.1 Reduce侧的联结　　 90
5.2.2 基于DistributedCache的复制联结　　 98
5.2.3 半联结：map侧过滤后在reduce侧联结　　101
5.3 创建一个Bloom filter 　　102
5.3.1 Bloom filter做了什么　　102
5.3.2 实现一个Bloom filter 　　104
5.3.3 Hadoop 0.20 以上版本的Bloom filter 　　 110
5.4 温故知新　　　110
5.5 小结　　 111
5.6 更多资源　　 112
第6 章 编程实践　　 113
6.1 开发MapReduce 程序　　 113
6.1.1 本地模式　　 114
6.1.2 伪分布模式　　 118
6.2 生产集群上的监视和调试　　123
6.2.1 计数器　　123
6.2.2 跳过坏记录　　125
6.2.3 用IsolationRunner重新运行出错的任务　　128
6.3 性能调优　　 129
6.3.1 通过combiner来减少网络流量　　129
6.3.2 减少输入数据量　　129
6.3.3 使用压缩　　129
6.3.4 重用JVM 　　132
6.3.5 根据猜测执行来运行　　132
6.3.6 代码重构与算法重写　　133
6.4 小结　　134
第7 章 细则手册　　135
7.1 向任务传递作业定制的参数　　 135
7.2 探查任务特定信息　　137
7.3 划分为多个输出文件　　138
7.4 以数据库作为输入输出　　143
7.5 保持输出的顺序　　145
7.6 小结　　 146
第8 章 管理Hadoop　　147
8.1 为实际应用设置特定参数值　　 147
8.2 系统体检　　149
8.3 权限设置　　151
8.4 配额管理　　151
8.5 启用回收站　　152
8.6 删减DataNode 　　152
8.7 增加DataNode 　　153
8.8 管理NameNode 和SNN　　 153
8.9 恢复失效的NameNode 　　155
8.10 感知网络布局和机架的设计　　156
8.11 多用户作业的调度　　157
8.11.1 多个JobTracker 　　 158
8.11.2 公平调度器　　158
8.12 小结　　 160
第三部分 Hadoop也疯狂
第9 章 在云上运行Hadoop 　　 162
9.1 Amazon Web Services 简介　　162
9.2 安装AWS　　163
9.2.1 获得AWS身份认证凭据　　164
9.2.2 获得命令行工具　　166
9.2.3 准备SSH密钥对　　168
9.3 在EC2 上安装Hadoop　　169
9.3.1 配置安全参数　　169
9.3.2 配置集群类型　　169
9.4 在EC2 上运行MapReduce 程序　　171
9.4.1 将代码转移到Hadoop集群上　　171
9.4.2 访问Hadoop集群上的数据　　172
9.5 清空和关闭EC2 实例　　175
9.6 Amazon Elastic MapReduce 和其他AWS 服务　　176
9.6.1 Amazon Elastic MapReduce 　　176
9.6.2 AWS导入/导出　　177
9.7 小结　　177
第10 章 用Pig编程　　178
10.1 像Pig 一样思考　　178
10.1.1 数据流语言　　179
10.1.2 数据类型　　179
10.1.3 用户定义函数　　179
10.2 安装Pig 　　179
10.3 运行Pig 　　180
10.4 通过Grunt 学习Pig Latin　　182
10.5 谈谈Pig Latin 　　186
10.5.1 数据类型和schema　　186
10.5.2 表达式和函数　　187
10.5.3 关系型运算符　　189
10.5.4 执行优化　　196
10.6 用户定义函数　　196
10.6.1 使用UDF 　　196
10.6.2 编写UDF 　　197
10.7 脚本　　199
10.7.1 注释　　199
10.7.2 参数替换　　200
10.7.3 多查询执行　　201
10.8 Pig 实战——计算相似专利的例子　　201
10.9 小结　　206
第11 章 Hive及Hadoop群　　207
11.1 Hive 　　207
11.1.1 安装与配置Hive 　　208
11.1.2 查询的示例　　210
11.1.3 深入HiveQL 　　213
11.1.4 Hive小结　　221
11.2 其他Hadoop 相关的部分　　221
11.2.1 HBase 　　221
11.2.2 ZooKeeper 　　221
11.2.3 Cascading 　　221
11.2.4 Cloudera 　　222
11.2.5 Katta 　　222
11.2.6 CloudBase 　　222
11.2.7 Aster Data和Greenplum 　　222
11.2.8 Hama和Mahout 　　223
11.2.9 search-hadoop.com 　　223
11.3 小结　　223
第12 章 案例研究　　224
12.1 转换《纽约时报》1100 万个库存图片文档　　224
12.2 挖掘中国移动的数据　　225
12.3 在StumbleUpon 推荐最佳网站　　229
12.3.1 分布式StumbleUpon 的开端　　230
12.3.2 HBase 和StumbleUpon 　　230
12.3.3 StumbleUpon 上的更多Hadoop 应用　　236
12.4 搭建面向企业查询的分析系统——IBM的ES2 项目　　238
12.4.1 ES2 系统结构　　240
12.4.2 ES2 爬虫　　241
12.4.3 ES2 分析　　242
12.4.4 小结　　249
12.4.5 参考文献　　250
附录A HDFS文件命令　　251
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop生态系统
前言
第1章关键技术
1.1 Hadoop分布式文件系统（HDFS）
1.2 MapReduce
1.3 YARN
1.4 Spark
第2章数据库及数据管理
2.1 Cassandra
2.2 HBase
2.3 Accumulo
2.4 Memcached
2.5 Blur
2.6 Solr
2.7 MongoDB
2.8 Hive
2.9 Spark SQL（前身是Shark）
2.10 Giraph
第3章序列化
3.1 Avro
3.2 JSON
3.3 Protocol Buffers（protobuf）
3.4 Parquet
第4章管理与监控
4.1 Ambari
4.2 Hcatalog
4.3 Nagios
4.4 Puppet
4.5 Chef
4.6 ZooKeeper
4.7 Oozie
4.8 Ganglia
第5章分析辅助
5.1 MapReduce接口
5.2分析库
5.3 Pig
5.4 Hadoop Streaming
5.5 Mahout
5.6 MLLib
5.7 Hadoop图像处理接口（HIPI）
5.8 Spatial Hadoop
第6章数据传输
6.1 Sqoop
6.2 Flume
6.3 DistCp
6.4 Storm
第7章安全、访问控制和审计
7.1 Sentry
7.2 Kerberos
7.3 Knox
第8章云计算和虚拟化
8.1 Serengeti
8.2 Docker
8.3 Whirr
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop生态系统
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop MapReduce性能优化
目录
第1章 了解Hadoop MapReduce 1
1.1 MapReduce模型 1
1.2 Hadoop MapReduce概述 3
1.3 Hadoop MapReduce的工作原理 4
1.4 影响MapReduce性能的因素 5
1.5 小结 8
第2章 Hadoop参数概述 9
2.1 研究Hadoop参数 9
2.1.1 配置文件mapred-site.xml 10
2.1.2 配置文件hdfs-site.xml 15
2.1.3 配置文件core-site.xml 18
2.2 Hadoop MapReduce性能指标 19
2.3 性能监测工具 20
2.3.1 用Chukwa监测Hadoop 21
2.3.2 使用Ganglia监测Hadoop 21
2.3.3 使用Nagios监测Hadoop 21
2.4 用Apache Ambari监测Hadoop 22
2.5 小结 23
第3章 检测系统瓶颈 25
3.1 性能调优 25
3.2 创建性能基线 27
3.3 识别资源瓶颈 30
3.3.1 识别内存瓶颈 30
3.3.2 识别CPU瓶颈 31
3.3.3 识别存储瓶颈 32
3.3.4 识别网络带宽瓶颈 33
3.4 小结 34
第4章 识别资源薄弱环节 35
4.1 识别集群薄弱环节 35
4.1.1 检查Hadoop集群节点的健康状况 36
4.1.2 检查输入数据大小 37
4.1.3 检查海量I/O和网络阻塞 38
4.1.4 检查并发任务不足 39
4.1.5 检查CPU过饱和 40
4.2 量化Hadoop集群 41
4.3 正确配置集群 44
4.4 小结 47
第5章 强化map和reduce任务 49
5.1 强化map任务 49
5.1.1 输入数据和块大小的影响 51
5.1.2 处置小文件和不可拆分文件 51
5.1.3 在Map阶段压缩溢写记录 53
5.1.4 计算map任务的吞吐量 55
5.2 强化reduce任务 57
5.2.1 计算reduce任务的吞吐量 58
5.2.2 改善Reduce执行阶段 59
5.3 调优map和reduce参数 60
5.4 小结 64
第6章 优化MapReduce任务 65
6.1 使用Combiner 65
6.2 使用压缩技术 68
6.3 使用正确Writable类型 72
6.4 明智地复用类型 74
6.5 优化mapper和reducer的代码 76
6.6 小结 78
第7章 最佳实践与建议 81
7.1 硬件调优与操作系统推荐 81
7.1.1 Hadoop集群检查表 81
7.1.2 Bios调优检查表 82
7.1.3 OS配置建议 82
7.2 Hadoop最佳实践与建议 83
7.2.1 部署Hadoop 83
7.2.2 Hadoop调优建议 84
7.2.3 使用MapReduce模板类代码 86
7.3 小结 90
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop MapReduce性能优化
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop硬实战
前言 ...............................................................................................................XV
致谢 ............................................................................................................XVII
关于本书 ..................................................................................................... XIX
第1 部分　背景和基本原理...............................................1
1　跳跃中的Hadoop....................................................................................... 3
1.1　什么是Hadoop ................................................................................................. 4
1.1.1　Hadoop 的核心组件 ............................................................................ 5
1.1.2　Hadoop 生态圈 .................................................................................... 9
1.1.3　物理架构 ............................................................................................ 10
1.1.4　谁在使用Hadoop .............................................................................. 12
1.1.5　Hadoop 的局限性 .............................................................................. 13
1.2　运行Hadoop ................................................................................................... 14
1.2.1　下载并安装Hadoop .......................................................................... 14
1.2.2　Hadoop 的配置 .................................................................................. 15
1.2.3　CLI 基本命令 ..................................................................................... 17
1.2.4　运行MapReduce 作业 ....................................................................... 18
1.3　本章小结 ........................................................................................................ 24
第2 部分　数据逻辑....................................................... 25
2　将数据导入导出Hadoop.........................................................27
2.1　导入导出的关键要素 .................................................................................... 29
2.2　将数据导入Hadoop ....................................................................................... 30
2.2.1　将日志文件导入Hadoop .................................................................. 31
技术点1　使用Flume 将系统日志文件导入HDFS ............................. 33
2.2.2　导入导出半结构化和二进制文件 .................................................... 42
技术点2　自动复制文件到HDFS 的机制 ............................................ 43
技术点3　使用Oozie 定期执行数据导入活动 ..................................... 48
2.2.3　从数据库中拉数据 ............................................................................ 52
技术点4　使用MapReduce 将数据导入数据库 ................................... 53
技术点5　使用Sqoop 从MySQL 导入数据 ......................................... 58
2.2.4　HBase ................................................................................................. 68
技术点6　HBase 导入HDFS ................................................................. 68
技术点7　将HBase 作为MapReduce 的数据源 .................................. 70
2.3　将数据导出Hadoop ....................................................................................... 73
2.3.1　将数据导入本地文件系统 ................................................................ 73
技术点8　自动复制HDFS 中的文件 .................................................... 73
2.3.2　数据库 ................................................................................................ 74
技术点9　使用Sqoop 将数据导入MySQL .......................................... 75
2.3.3　Hbase .................................................................................................. 78
技术点10　将数据从HDFS 导入HBase .............................................. 78
技术点11　使用HBase 作为MapReduce 的数据接收器 .................... 79
2.4　本章小结 ........................................................................................................ 81
3　数据序列化——处理文本文件及其他格式的文件........................83
3.1　了解MapReduce 中的输入和输出 ............................................................... 84
3.1.1　数据输入 ............................................................................................ 85
3.1.2　数据输出 ............................................................................................ 89
3.2　处理常见的序列化格式 ................................................................................ 91
3.2.1　XML ................................................................................................... 91
技术点12　MapReduce 和XML ............................................................ 91
3.2.2　JSON ................................................................................................... 95
技术点13　MapReduce 和JSON ........................................................... 95
3.3　大数据的序列化格式 .................................................................................... 99
3.3.1　比较SequenceFiles、Protocol Buffers、Thrift 和 Avro .................. 99
3.3.2　Sequence File .................................................................................... 101
技术点14　处理SequenceFile .............................................................. 103
3.3.3　Protocol Buffers ................................................................................ 109
技术点15　整合Protocol Buffers 和MapReduce ............................... 110
3.3.4　Thrift ................................................................................................. 117
技术点16　使用Thrift .......................................................................... 117
3.3.5　Avro .................................................................................................. 119
技术点17　MapReduce 的下一代数据序列化技术 ............................ 120
3.4　自定义文件格式 .......................................................................................... 127
3.4.1　输入输出格式 .................................................................................. 127
技术点18　输入和输出格式为CSV 的文件 ...................................... 128
3.4.2　output committing 的重要性 ........................................................... 136
3.5　本章小结 ...................................................................................................... 136
第3 部分　大数据模式..................................................137
4　处理大数据的MapReduce 模式............................................. 139
4.1　Join ................................................................................................................ 140
4.1.1　Repartition Join ................................................................................ 141
技术点19　优化repartition join ........................................................... 142
4.1.2　Replicated Join ................................................................................. 146
4.1.3　Semi-join .......................................................................................... 147
技术点20　实现semi-join .................................................................... 148
4.1.4　为你的数据挑选最优的合并策略 .................................................. 154
4.2　排序 .............................................................................................................. 155
4.2.1　二次排序 .......................................................................................... 156
技术点21　二次排序的实现 ................................................................ 157
4.2.2　整体并行排序 .................................................................................. 162
技术点22　通过多个reducer 对key 进行排序 .................................. 162
4.3　抽样 .............................................................................................................. 165
技术点23　蓄水池抽样（reservoir 抽样） ........................................... 165
4.4　本章小结 ...................................................................................................... 168
5　优化HDFS 处理大数据的技术............................................... 169
5.1　处理小文件 .................................................................................................. 170
技术点24　使用Avro 存储大量小文件 .............................................. 170
5.2　通过压缩提高数据存储效率 ...................................................................... 178
技术点25　选择合适的压缩解码器 .................................................... 178
技术点26　在HDFS、MapReduce、Pig 和Hive 中使用数据压缩 .. 182
技术点27　在MapReduce、Hive 和Pig 中处理可分割的LZOP ..... 187
5.3　本章小结 ...................................................................................................... 193
6　诊断和优化性能问题............................................................. 194
6.1　衡量MapReduce 和你的环境 ..................................................................... 195
6.1.1　提取作业统计信息的工具 .............................................................. 195
6.1.2　监控 .................................................................................................. 196
6.2　确定性能问题的原因 .................................................................................. 198
6.2.1　了解哪些因素会影响MapReduce 作业的性能 ............................. 198
6.2.2　map 端异常 ...................................................................................... 200
技术点28　发现输入数据中的坑 ........................................................ 200
技术点29　确定map 端数据倾斜问题 ............................................... 201
技术点30　判定map 任务吞吐量 ....................................................... 203
技术点31　小文件 ................................................................................ 204
技术点32　不可切割的文件 ................................................................ 206
6.2.3　reduce 端问题 .................................................................................. 207
技术点33　reducer 任务数过大或过小 ............................................... 208
技术点34　定位reduce 端数据倾斜问题 ............................................ 209
技术点35　确定reduce 任务是否存在整体吞吐量过低 .................... 211
技术点36　缓慢的洗牌（shuffle）和排序 ......................................... 213
6.2.4　任务的一般性能问题 ...................................................................... 213
技术点37　作业竞争和调度器限制 .................................................... 215
技术点38　使用堆转储来查找未优化的用户代码 ............................ 216
6.2.5　硬件性能问题 .................................................................................. 218
技术点39　查找硬件的失效 ................................................................ 218
技术点40　CPU 竞争 ........................................................................... 219
技术点41　内存交换 ............................................................................ 220
技术点42　磁盘健康 ............................................................................ 222
技术点43　网络 .................................................................................... 224
6.3　可视化 .......................................................................................................... 226
技术点44　提取并可视化任务执行时间 ............................................ 227
6.4　优化 ............................................................................................................. 229
6.4.1　剖析MapReduce 的用户代码 ......................................................... 230
技术点45　剖析map 和reduce 任务 ................................................... 230
6.4.2　参数配置 .......................................................................................... 232
6.4.3　优化 shuffle 和 sort 阶段 ................................................................. 234
技术点46　避免reducer ....................................................................... 234
技术点47　过滤和投影 ........................................................................ 235
技术点48　使用 combiner .................................................................... 236
技术点49　超炫的使用比较器的快速排序 ........................................ 237
6.4.4　减轻倾斜 .......................................................................................... 241
技术点50　收集倾斜数据 .................................................................... 242
技术点51　减轻reducer 阶段倾斜 ...................................................... 243
6.4.5　在MapReduce 中优化用户的Java 代码 ........................................ 244
6.4.6　数据序列化 ...................................................................................... 248
6.5　本章小结 ...................................................................................................... 249
第4 部分 数据科学.......................................................251
7　数据结构和算法的运用.......................................................... 253
7.1　使用图进行数据建模和解决问题 .............................................................. 254
7.1.1　模拟图 .............................................................................................. 255
7.1.2　最短路径算法 .................................................................................. 255
技术点52　找出两个用户间的最短距离 ............................................ 256
7.1.3　friends-of-friends（FoF） ................................................................. 263
技术点53　计算FoF ............................................................................. 263
7.1.4　PageRank .......................................................................................... 269
技术点54　通过Web 图计算PageRank .............................................. 269
7.2　Bloom filter ................................................................................................... 275
技术点55　在MapReduce 中并行创建Bloom filter ......................... 277
技术点56　通过MapReduce 对Bloom filter 进行semi-join ............. 281
7.3　本章小结 ...................................................................................................... 284
8　结合R 和Hadoop 进行数据统计............................................ 285
8.1　比较R 和MapReduce 集成的几种方法 .................................................... 286
8.2　R 基础知识 ................................................................................................... 288
8.3　R 和Streaming ............................................................................................. 290
8.3.1　Streaming 和map-only R ................................................................. 290
技术点57　计算股票日平均值 ............................................................ 290
8.3.2　Streaming、R 和完整的MapReduce .............................................. 293
技术点58　计算股票的累积均值 ........................................................ 293
8.4　Rhipe——将客户端R 和Hadoop 进行集成 ............................................. 297
技术点59　使用Rhipe 计算CMA ....................................................... 297
8.5　 RHadoop——更简单地在客户端集成R 和Hadoop 的技术 .................... 301
技术点60　使用RHadoop 计算CMA ................................................. 302
8.6　本章小结 ...................................................................................................... 304
9　使用Mahout 进行预测分析................................................... 305
9.1　使用recommender 提供产品建议 .............................................................. 306
9.1.1　相似性度量的可视化 ...................................................................... 307
9.1.2　GroupLens 数据集 ........................................................................... 308
9.1.3　基于用户的recommender ............................................................... 310
9.1.4　基于物品的recommender ............................................................... 310
技术点61　使用基于物品的recommender 进行电影评级 ................ 311
9.2　classification ................................................................................................. 314
9.2.1　编写一个手动naïve Bayesian 分类器 ............................................ 315
9.2.2　可扩展的垃圾邮件侦测分类系统 .................................................. 321
技术点62　使用Mahout 训练和测试垃圾邮件分类器 ...................... 321
9.2.3　其他分类算法 .................................................................................. 325
9.3　K-means clustering ....................................................................................... 325
9.3.1　简单介绍 .......................................................................................... 326
9.3.2　并行执行K-means ........................................................................... 327
技术点63　K-means 处理合成的二维数据集 ..................................... 327
9.3.3　K-means 和文本 ............................................................................... 331
9.3.4　其他Mahout clustering 算法 ........................................................... 332
9.4　本章小结 ...................................................................................................... 332
第5 部分　驯服大象......................................................333
10　深入解析 Hive.................................................................. 335
10.1　Hive 基础 ................................................................................................ 336
10.1.1　安装 .......................................................................................... 336
10.1.2　元存储 ...................................................................................... 336
10.1.3　数据库、表、分区和存储 ...................................................... 336
10.1.4　数据模型 .................................................................................. 337
10.1.5　查询语言 .................................................................................. 337
10.1.6　交互式和非交互式Hive ......................................................... 337
10.2　使用Hive 进行数据分析 ....................................................................... 338
10.2.1　序列化和反序列化 .................................................................. 338
技术点64　载入日志文件 .............................................................. 338
10.2.2　UDF、分区、分桶和压缩 ...................................................... 344
技术点65　编写UDF 和压缩分区表 ............................................ 344
10.2.3　数据合并 .................................................................................. 350
技术点66　优化Hive 合并 ............................................................ 350
10.2.4　分组、排序和explain ............................................................. 355
10.3　本章小结 ................................................................................................ 358
11　Pig 流管道......................................................................... 359
11.1　Pig 基础 .................................................................................................. 360
11.1.1　安装 .......................................................................................... 360
11.1.2　架构 .......................................................................................... 360
11.1.3　PigLatin..................................................................................... 360
11.1.4　数据类型 .................................................................................. 361
11.1.5　操作符和函数 .......................................................................... 361
11.1.6　交互式和非交互式的Pig ........................................................ 362
11.2　使用Pig 在日志数据中发现恶意行为者 ............................................. 362
11.2.1　加载数据 .................................................................................. 363
技术点67　加载Apache 日志文件 ................................................ 363
11.2.2　过滤和投影 .............................................................................. 368
技术点68　通过过滤和投影减少数据处理量 .............................. 368
11.2.3　分组和聚合UDF ..................................................................... 370
技术点69　IP 地址的分组和计数 ................................................. 370
11.2.4　使用UDF 进行定位 ................................................................ 374
技术点70　使用分布式缓存进行IP 地理定位 ............................ 375
11.2.5　流 .............................................................................................. 378
技术点71　使用你的脚本合并Pig ............................................... 378
11.2.6　合并 .......................................................................................... 379
技术点72　在Pig 中合并数据 ...................................................... 380
11.2.7　排序 .......................................................................................... 381
技术点73　元组排序 ...................................................................... 381
11.2.8　存储数据 .................................................................................. 382
技术点74　在SequenceFiles 中存储数据 ..................................... 382
11.3　使用Pig 优化用户的工作流程 ............................................................. 385
技术点75　通过4 步快速处理大数据 .......................................... 385
11.4　性能 ......................................................................................................... 390
技术点76　Pig 优化 ....................................................................... 390
11.5　本章小结 ................................................................................................. 393
12　Crunch 及相关技术............................................................ 394
12.1　什么是Crunch ........................................................................................ 395
12.1.1　背景和概念 .............................................................................. 395
12.1.2　基本原理 .................................................................................. 395
12.1.3　简单示例 .................................................................................. 398
12.2　发现日志中最热门的URL .................................................................... 401
技术点77　使用Crunch 进行日志解析和基本分析 .................... 402
12.3　合并 ........................................................................................................ 405
技术点78　Crunch 的repartition join ............................................ 405
12.4　Cascading ................................................................................................ 407
12.5　本章小结 ................................................................................................ 409
13　测试和调试....................................................................... 410
13.1　测试 ........................................................................................................ 410
13.1.1　有效的单元测试的基本要素 .................................................. 411
13.1.2　MRUnit ..................................................................................... 413
技术点79　MapReduce 函数、作业和管道的单元测试 ............. 413
13.1.3　LocalJobRunner ........................................................................ 420
技术点80　用LocalJobRunner 进行重量级的作业测试 ............. 421
13.1.4　集成和QA 测试 ...................................................................... 423
13.2　调试用户空间的问题 ............................................................................ 424
13.2.1　访问任务日志 .......................................................................... 424
技术点81　检查任务日志 .............................................................. 424
13.2.2　调试不可预期的输入 .............................................................. 429
技术点82　定位input split 问题 .................................................... 429
13.2.3　调试JVM 配置 ........................................................................ 432
技术点83　解决任务的JVM 启动参数 ........................................ 433
13.2.4　高效调试的编码准则 .............................................................. 433
技术点84　调试和错误处理 .......................................................... 433
13.3　MapReduce 陷阱 .................................................................................... 437
技术点85　MapReduce 反模式 ..................................................... 438
13.4　本章小结 ................................................................................................ 441
附录A　相关技术..................................................................... 443
附录B　Hadoop 内置的数据导入导出工具.................................. 471
附录C　HDFS 解剖................................................................. 486
附录D　优化MapReduce 合并框架............................................ 493
索引.......................................................................................... 503
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop硬实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解Hadoop（原书第2版）
目录
译者序
作者简介
前言
第1章为什么会有大数据1
1.1什么是大数据1
1.2大数据技术背后的核心思想2
1.2.1把数据分发到多个节点2
1.2.2把计算逻辑移动到数据附近3
1.2.3计算节点进行本地数据处理3
1.2.4优选顺序读，次之随机读4
1.2.5一个例子4
1.3大数据的编程模型5
1.3.1大规模并行处理数据库系统5
1.3.2内存数据库系统6
1.3.3MapReduce系统6
1.3.4整体同步并行系统8
1.4大数据和事务性系统8
1.5我们能处理多大的数据量9
1.5.1一个计算密集型的例子10
1.5.2Amdhal定律10
1.6大数据商业用例11
1.7本章小结12
第2章Hadoop中的概念13
2.1Hadoop简介13
2.2MapReduce编程模型简介15
2.3Hadoop系统的组成19
2.3.1Hadoop 分布式文件系统20
2.3.2辅助名称节点25
2.3.3任务跟踪器26
2.3.4作业跟踪器26
2.4Hadoop 2.027
2.4.1容器29
2.4.2节点管理器29
2.4.3资源管理器30
2.4.4应用程序管理器30
2.4.5分步详解YARN请求31
2.5HDFS 的高可用性33
2.6本章小结33
第3章初识Hadoop框架34
3.1安装类型34
3.1.1单机模式35
3.1.2伪分布式集群模式35
3.1.3多节点集群安装模式35
3.1.4基于Amazon EMR预安装模式35
3.2使用Cloudera虚拟机搭建开发环境36
3.3一个MapReduce程序的组成37
3.4第一个Hadoop程序38
3.4.1以本地模式运行程序的必要条件39
3.4.2使用旧API编写的单词计数程序39
3.4.3构建程序42
3.4.4在集群模式下运行单词计数程序42
3.4.5使用新API编写的单词计数程序43
3.4.6构建程序44
3.4.7在集群模式下运行单词计数程序45
3.5Hadoop作业中的第三方函数库45
3.6本章小结50
第4章Hadoop系统管理51
4.1Hadoop的配置文件51
4.2配置Hadoop守护进程52
4.3Hadoop配置文件的优先级53
4.4深入探究Hadoop配置文件54
4.4.1core—site.xml54
4.4.2hdfs—*.xml55
4.4.3mapred—site.xml56
4.4.4yarn—site.xml58
4.4.5YARN中的内存分配60
4.5调度器61
4.5.1计算能力调度器62
4.5.2公平调度器65
4.5.3公平调度器配置65
4.5.4 yarn—site.xml 配置66
4.5.5策略文件的格式和配置67
4.5.6按照drf策略来确定优势资源的分配68
4.6从属文件69
4.7机架感知69
4.8 集群管理工具71
4.8.1检查HDFS71
4.8.2 HDFS管理命令行73
4.8.3 均衡HDFS上的数据分布75
4.8.4从HDFS中复制海量数据76
4.9本章小结76
第5章MapReduce开发基础78
5.1 Hadoop和数据处理78
5.2 航空公司数据集介绍79
5.2.1 准备开发环境80
5.2.2 准备Hadoop系统81
5.3 MapReduce编程模式81
5.3.1 只有Map阶段的作业（SELECT和WHERE查询）82
5.3.2 问题定义—SELECT子句82
5.3.3 问题定义—WHERE子句90
5.3.4 Map和Reduce作业（聚合查询）93
5.3.5 问题定义—GROUP BY和SUM子句93
5.3.6 应用Combiner提高Aggregation性能99
5.3.7 问题定义—优化后的Aggregators99
5.3.8 Partitioner的作用104
5.3.9 问题定义—按月分离航空数据105
5.4 综合分析108
5.5 本章小结110
第6章MapReduce开发进阶111
6.1 MapReduce编程模式111
6.2 Hadoop I／O 介绍111
6.3 问题定义—排序114
6.3.1 主要挑战：全排序115
6.3.2 在Cluster中运行Sorting作业125
6.3.3 仅根据Writable键排序125
6.3.4 根据排序回顾Hadoop的关键特性128
6.4 问题定义—分析连续的记录128
6.4.1 支持二次排序的重要组件129
6.4.2 在没有Grouping Comparator的情况下实现Secondary Sort136
6.4.3 在Cluster中运行SecondarySort作业137
6.4.4 利用Secondary Sort回顾Hadoop的关键特性137
6.5 问题定义—使用MapReducer进行连接138
6.5.1 处理多输入：Multiple—Inputs 类138
6.5.2 具备多个输入的Mapper类139
6.5.3 自定义 Partitioner： Carrier—CodeBasedPartioner141
6.5.4 在Reducer中实现连接141
6.5.5 在集群中运行MapReduce连接作业143
6.5.6 探讨与MapReduce相关的Hadoop主要特性144
6.6 问题定义—使用Map—Only 作业进行连接144
6.6.1 基于DistributeCache的解决方案145
6.6.2 在集群中运行Map—Only的连接作业147
6.6.3 总结探讨Map—Only连接时的Hadoop关键特性149
6.7 在MR作业中保存结果到多输出文件149
6.8 使用计数器收集统计数据151
6.9 本章小结153
第7章 Hadoop输入／输出155
7.1 压缩方式155
7.1.1 压缩内容的选择156
7.1.2 各种压缩方式157
7.1.3 配置压缩方式158
7.2 Hadoop的I／O处理过程内部159
7.2.1 Inputformat159
7.2.2 OutputFormat161
7.2.3 自定义OutputFormat：将文本转换成XML161
7.2.4 自定义 InputFormat：使用自定义的XML文件165
7.3 Hadoop文件173
7.3.1 SequenceFile173
7.3.2 MapFiles178
7.3.3 Avro Files180
7.4 本章小结185
第8章 测试Hadoop程序186
8.1 回顾一下单词统计的程序186
8.2 MRUnit概述188
8.2.1 安装MRUnit188
8.2.2 MRUnit 核心类188
8.2.3 编写一个MRUnit测试用例189
8.2.4 测试计数器191
8.2.5 MRUnit的特性194
8.2.6 MRUnit的局限性194
8.3 用LocalJobRunner测试195
8.3.1 setUp（ ）方法196
8.3.2 LocalJobRunner的局限性197
8.4 用MiniMRCluster测试198
8.4.1 配置开发环境198
8.4.2 MiniMRCluster例子199
8.4.3 MiniMRCluster的局限性201
8.5 对访问网络资源的MR作业进行测试202
8.6 本章小结202
第9章Hadoop的监控203
9.1 在Hadoop MapReduce Jobs中写日志消息203
9.2 在Hadoop MapReduce Jobs中查看日志消息206
9.3 在Hadoop 2.x中使用日志管理208
9.3.1 Hadoop 2.x中的日志存储208
9.3.2 日志管理提升210
9.3.3 使用基于Web的界面查看日志210
9.3.4 命令行界面211
9.3.5 日志的保存211
9.4 Hadoop集群性能监控211
9.5 使用YARN REST API212
9.6 使用供应商工具管理Hadoop集群213
9.7 本章小结214
第10章使用Hadoop构建数据仓库215
10.1 Apache Hive215
10.1.1 安装Hive216
10.1.2 Hive的架构217
10.1.3 元数据存储217
10.1.4 HiveQL编译基础217
10.1.5 Hive使用的概念218
10.1.6 HiveQL编译细节222
10.1.7 数据定义语言226
10.1.8 数据操作语言226
10.1.9 扩展接口227
10.1.10 Hive脚本229
10.1.11 性能表现229
10.1.12 整合MapReduce230
10.1.13 创建分区230
10.1.14 用户定义函数232
10.2 Impala234
10.2.1 Impala架构234
10.2.2 Impala特性235
10.2.3 Impala的局限235
10.3 Shark235
10.4 本章小结237
第11章使用Pig进行数据处理238
11.1 Pig简介238
11.2 运行Pig240
11.2.1 在Grunt Shell中执行241
11.2.2 执行Pig脚本241
11.2.3 嵌入式Java程序242
11.3 Pig Latin243
11.3.1 Pig脚本中的注释243
11.3.2 Pig语句的执行243
11.3.3 Pig命令244
11.4 UDF249
11.4.1 Mapper中的Eval函数调用249
11.4.2 Reducer中的Eval函数调用250
11.4.3 编写并使用自定义Filter—Func256
11.5 Pig与Hive对比258
11.6 Crunch API259
11.6.1 Crunch与Pig的区别259
11.6.2 Crunch管道的例子260
11.7 本章小结265
第12章HCatalog和企业级Hadoop266
12.1 HCataolg和企业级数据仓库用户266
12.2 HCatalog技术背景简介 267
12.2.1 HCatalog命令行接口269
12.2.2 WebHCat269
12.2.3 HCatalog的MapReduce接口270
12.2.4 HCatalog的Pig接口273
12.2.5 HCatalog通知接口274
12.3 HCatalog的安全和认证机制274
12.4 完整的解决方案275
12.5 本章小结275
第13章使用Hadoop分析日志277
13.1 日志文件分析应用277
13.1.1 网络分析277
13.1.2 安全规范与法务278
13.1.3 监控和报警279
13.1.4 物联网279
13.2 分析步骤280
13.2.1 载入280
13.2.2 提取280
13.2.3 可视化281
13.3 Apache Flume281
13.4 Netflix Suro283
13.5 云解决方案285
13.6 本章小结285
第14章使用HBase构建实时系统286
14.1 HBase是什么286
14.2 典型的HBase用例场景287
14.3 HBase数据模型288
14.3.1 HBase逻辑视图和客户端视图288
14.3.2 HBase与RDBMS的区别289
14.3.3 HBase表290
14.3.4 HBase单元格290
14.3.5 HBase列簇290
14.4 HBase命令和API291
14.4.1 获取命令列表：帮助命令291
14.4.2 创建表：create命令292
14.4.3 向表中加入行：put命令293
14.4.4 从表中检索行：get命令293
14.4.5 读取多行：scan命令293
14.4.6 统计表中的行数：count命令293
14.4.7 删除行：delete命令294
14.4.8 清空表：truncate命令294
14.4.9 删除表：drop命令294
14.4.10 更换表：alter命令294
14.5 HBase架构295
14.5.1 HBase组件295
14.5.2 HBase中的压缩与分区302
14.5.3 压缩303
14.6 HBase配置概览304
14.7 HBase应用程序设计305
14.7.1 长表vs宽表vs窄表305
14.7.2 行键设计306
14.8 使用Java API操作HBase307
14.8.1 一切都是字节307
14.8.2 创建HBase表307
14.8.3 使用HBaseAdmin类管理HBase308
14.8.4 使用Java API访问数据308
14.9 HBase与MapReduce集成312
14.9.1 使用MapReduce任务读取HBase表312
14.9.2 HBase和MapReduce集群315
14.10 本章小结316
第15章Hadoop与数据科学317
15.1 Hadoop中的数据科学方法318
15.2 Apache Hama318
15.2.1 整体同步并行计算模型318
15.2.2 Hama Hello World！319
15.2.3 蒙特卡洛方法321
15.2.4 K—Means聚类324
15.3 Apache Spark327
15.3.1 弹性分布式数据集（RDD）327
15.3.2 Spark与蒙特卡洛算法328
15.3.3 Spark与KMeans聚类330
15.4 RHadoop332
15.5 本章小结333
第16章Hadoop与云计算334
16.1 经济性334
16.1.1 自有集群335
16.1.2 基于云平台的集群335
16.1.3 弹性336
16.1.4 按需付费336
16.1.5 竞价336
16.1.6 混合集群336
16.2 后勤保障337
16.2.1 导入／导出337
16.2.2 数据保存337
16.3 安全性337
16.4 云端应用模型338
16.5 云服务商339
16.5.1 亚马逊网络服务（AWS）339
16.5.2 谷歌云平台341
16.5.3 微软Azure342
16.5.4 选择云服务商342
16.6 案例学习： AWS342
16.6.1 EMR343
16.6.2 EC2345
16.7 本章小结348
第17章构建YARN应用程序349
17.1 YARN：通用分布式系统349
17.2 YARN：快速浏览351
17.3 创建YARN应用程序353
17.4 DownloadService.java类354
17.5 Client.java类356
17.5.1 从客户端启动应用管理器的步骤356
17.5.2 创建YarnClient357
17.5.3 配置应用程序357
17.5.4 启动应用管理器360
17.5.5 监控应用360
17.6 ApplicationMaster.java362
17.6.1 启动工作任务的步骤363
17.6.2 初始化应用管理器协议和容器管理协议364
17.6.3 在资源管理器中注册应用管理器364
17.6.4 配置容器参数364
17.6.5 向资源管理器请求容器364
17.6.6 在任务节点上启动容器364
17.6.7 等待容器结束工作任务365
17.6.8 在资源管理器中注销应用管理器365
17.7 运行应用管理器367
17.7.1 在非托管模式中启动应用管理器367
17.7.2 在托管模式中启动应用管理器367
17.8 本章小结367
附录Ａ安装Hadoop
附录B使用Maven和Eclipse
附录CApache Ambari
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解Hadoop（原书第2版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析MapReduce架构设计与实现原理
前　言
第一部分　基础篇
第1章　阅读源代码前的准备/ 2
1.1　准备源代码学习环境/ 2
1.1.1　基础软件下载/ 2
1.1.2　如何准备Windows环境/ 3
1.1.3　如何准备Linux环境/ 6
1.2　获取Hadoop源代码/ 7
1.3　搭建Hadoop源代码阅读环境/ 8
1.3.1　创建Hadoop工程/ 8
1.3.2　Hadoop源代码阅读技巧/ 9
1.4　Hadoop源代码组织结构/ 10
1.5　Hadoop初体验/ 13
1.5.1　启动Hadoop/ 13
1.5.2　Hadoop Shell介绍/ 15
1.5.3　Hadoop Eclipse插件介绍/ 15
1.6　编译及调试Hadoop源代码/ 19
1.6.1　编译Hadoop源代码/ 19
1.6.2　调试Hadoop源代码/ 20
1.7　小结/ 23
第2章　MapReduce设计理念与基本架构/ 24
2.1　Hadoop发展史/ 24
2.1.1　Hadoop产生背景/ 24
2.1.2　Apache Hadoop新版本的特性/ 25
2.1.3　Hadoop版本变迁/ 26
2.2　Hadoop MapReduce设计目标/ 28
2.3　MapReduce编程模型概述/ 29
2.3.1　MapReduce编程模型简介/ 29
2.3.2　MapReduce编程实例/ 31
2.4　Hadoop基本架构/ 32
2.4.1　HDFS架构/ 33
2.4.2　Hadoop MapReduce架构/ 34
2.5　Hadoop MapReduce作业的生命周期/ 36
2.6　小结/ 38
第二部分　MapReduce编程模型篇
第3章　MapReduce编程模型/ 40
3.1　MapReduce编程模型概述/ 40
3.1.1　MapReduce编程接口体系结构/ 40
3.1.2　新旧MapReduce API比较/ 41
3.2　MapReduce API基本概念/ 42
3.2.1　序列化/ 42
3.2.2　Reporter参数/ 43
3.2.3　回调机制/ 43
3.3　Java API解析/ 44
3.3.1　作业配置与提交/ 44
3.3.2　InputFormat接口的设计与实现/ 48
3.3.3　OutputFormat接口的设计与实现/ 53
3.3.4　Mapper与Reducer解析/ 55
3.3.5　Partitioner接口的设计与实现/ 59
3.4　非Java API解析/ 61
3.4.1　Hadoop Streaming的实现原理/ 61
3.4.2　Hadoop Pipes的实现原理/ 64
3.5　Hadoop工作流/ 67
3.5.1　JobControl的实现原理/ 67
3.5.2　ChainMapper/ChainReducer的实现原理/ 69
3.5.3　Hadoop工作流引擎/ 71
3.6　小结/ 73
第三部分　MapReduce核心设计篇
第4章　Hadoop RPC框架解析/ 76
4.1　Hadoop RPC框架概述/ 76
4.2　Java基础知识/ 77
4.2.1　Java反射机制与动态代理/ 78
4.2.2　Java网络编程/ 80
4.2.3　Java NIO/ 82
4.3　Hadoop RPC基本框架分析/ 89
4.3.1　RPC基本概念/ 89
4.3.2　Hadoop RPC基本框架/ 91
4.3.3　集成其他开源RPC框架/ 98
4.4　MapReduce通信协议分析/ 100
4.4.1　MapReduce 通信协议概述/ 100
4.4.2　JobSubmissionProtocol通信协议/ 102
4.4.3　InterTrackerProtocol通信协议/ 102
4.4.4　TaskUmbilicalProtocol通信协议/ 103
4.4.5　其他通信协议/ 104
4.5　小结/ 106
第5章　作业提交与初始化过程分析/ 107
5.1　作业提交与初始化概述/ 107
5.2　作业提交过程详解/ 108
5.2.1　执行Shell命令/ 108
5.2.2　作业文件上传/ 109
5.2.3　产生InputSplit文件/ 111
5.2.4　作业提交到JobTracker/ 113
5.3　作业初始化过程详解/ 115
5.4　Hadoop DistributedCache原理分析/ 117
5.4.1　使用方法介绍/ 118
5.4.2　工作原理分析/ 120
5.5　小结/ 122
第6章　JobTracker内部实现剖析/ 123
6.1　JobTracker概述/ 123
6.2　JobTracker启动过程分析/ 125
6.2.1　JobTracker启动过程概述/ 125
6.2.2　重要对象初始化/ 125
6.2.3　各种线程功能/ 128
6.2.4　作业恢复/ 129
6.3　心跳接收与应答/ 129
6.3.1　更新状态/ 131
6.3.2　下达命令/ 131
6.4　Job和Task运行时信息维护/ 134
6.4.1　作业描述模型/ 134
6.4.2　JobInProgress/ 136
6.4.3　TaskInProgress/ 137
6.4.4　作业和任务状态转换图/ 139
6.5　容错机制/ 141
6.5.1　JobTracker容错/ 141
6.5.2　TaskTracker容错/ 142
6.5.3　Job/Task容错/ 145
6.5.4　Record容错/ 147
6.5.5　磁盘容错/ 151
6.6　任务推测执行原理/ 152
6.6.1　计算模型假设/ 153
6.6.2　1.0.0版本的算法/ 153
6.6.3　0.21.0版本的算法/ 154
6.6.4　2.0版本的算法/ 156
6.7　Hadoop资源管理/ 157
6.7.1　任务调度框架分析/ 159
6.7.2　任务选择策略分析/ 162
6.7.3　FIFO调度器分析/ 164
6.7.4　Hadoop资源管理优化/ 165
6.8　小结/ 168
第7章　TaskTracker内部实现剖析/ 169
7.1　TaskTracker概述/ 169
7.2　TaskTracker启动过程分析/ 170
7.2.1　重要变量初始化/ 171
7.2.2　重要对象初始化/ 171
7.2.3　连接JobTracker/ 172
7.3　心跳机制/ 172
7.3.1　单次心跳发送/ 172
7.3.2　状态发送/ 175
7.3.3　命令执行/ 178
7.4　TaskTracker行为分析/ 179
7.4.1　启动新任务/ 179
7.4.2　提交任务/ 179
7.4.3　杀死任务/ 181
7.4.4　杀死作业/ 182
7.4.5　重新初始化/ 184
7.5　作业目录管理/ 184
7.6　启动新任务/ 186
7.6.1　任务启动过程分析/ 186
7.6.2　资源隔离机制/ 193
7.7　小结/ 195
第8章　Task运行过程分析/ 196
8.1　Task运行过程概述/ 196
8.2　基本数据结构和算法/ 197
8.2.1　IFile存储格式/ 197
8.2.2　排序/ 198
8.2.3　Reporter/ 201
8.3　Map Task内部实现/ 204
8.3.1　Map Task整体流程/ 204
8.3.2　Collect过程分析/ 205
8.3.3　Spill过程分析/ 213
8.3.4　Combine过程分析/ 214
8.4　Reduce Task内部实现/ 214
8.4.1　Reduce Task整体流程/ 215
8.4.2　Shuffle和Merge阶段分析/ 215
8.4.3　Sort和Reduce阶段分析/ 218
8.5　Map/Reduce Task优化/ 219
8.5.1　参数调优/ 219
8.5.2　系统优化/ 220
8.6　小结/ 224
第四部分　MapReduce高级篇
第9章　Hadoop性能调优/ 228
9.1　概述/ 228
9.2　从管理员角度进行调优/ 229
9.2.1　硬件选择/ 229
9.2.2　操作系统参数调优/ 229
9.2.3　JVM参数调优/ 230
9.2.4　Hadoop参数调优/ 230
9.3　从用户角度进行调优/ 235
9.3.1　应用程序编写规范/ 235
9.3.2　作业级别参数调优/ 235
9.3.3　任务级别参数调优/ 239
9.4　小结/ 240
第10章　Hadoop多用户作业调度器/ 241
10.1　多用户调度器产生背景/ 241
10.2　HOD/ 242
10.2.1　Torque资源管理器/ 242
10.2.2　HOD作业调度/ 243
10.3　Hadoop队列管理机制/ 245
10.4　Capacity Scheduler实现/ 246
10.4.1　Capacity Scheduler功能介绍/ 247
10.4.2　Capacity Scheduler实现/ 249
10.4.3　多层队列调度/ 254
10.5　Fair Scheduler实现/ 255
10.5.1　Fair Scheduler功能介绍/ 255
10.5.2　Fair Scheduler实现/ 258
10.5.3　Fair Scheduler与Capacity Scheduler对比/ 263
10.6　其他Hadoop调度器介绍/ 264
10.7　小结/ 265
第11章　Hadoop安全机制/ 266
11.1　Hadoop安全机制概述/ 266
11.1.1　Hadoop面临的安全问题/ 266
11.1.2　Hadoop对安全方面的需求/ 267
11.1.3　Hadoop安全设计基本原则/ 267
11.2　基础知识/ 268
11.2.1　安全认证机制/ 268
11.2.2　Kerberos介绍/ 270
11.3　Hadoop安全机制实现/ 273
11.3.1　RPC/ 273
11.3.2　HDFS/ 276
11.3.3　MapReduce/ 278
11.3.4　上层服务/ 280
11.4　应用场景总结/ 281
11.4.1　文件存取/ 281
11.4.2　作业提交与运行/ 282
11.4.3　上层中间件访问Hadoop/ 282
11.5　小结/ 283
第12章　下一代MapReduce框架/ 284
12.1　第一代MapReduce框架的局限性/ 284
12.2　下一代MapReduce框架概述/ 284
12.2.1　基本设计思想/ 284
12.2.2　资源统一管理平台/ 286
12.3　Apache YARN/ 287
12.3.1　Apache YARN基本框架/ 287
12.3.2　Apache YARN工作流程/ 290
12.3.3　Apache YARN设计细节/ 291
12.3.4　MapReduce与YARN结合/ 294
12.4　Facebook Corona / 298
12.4.1　Facebook Corona基本框架/ 298
12.4.2　Facebook Corona工作流程/ 300
12.4.3　YARN与Corona对比/ 303
12.5　Apache Mesos/ 304
12.5.1　Apache Mesos基本框架/ 304
12.5.2　Apache Mesos资源分配/ 305
12.5.3　MapReduce与Mesos结合/ 307
12.6　小结/ 309
附录A　安装Hadoop过程中可能存在的问题及解决方案/ 310
附录B　Hadoop默认HTTP端口号以及HTTP地址/ 312
参考资料/ 313
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop技术内幕 : 深入解析MapReduce架构设计与实现原理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python+Spark 2.0+Hadoop机器学习与大数据实战
目 录
第1章 Python Spark机器学习与Hadoop大数据 1
1.1 机器学习的介绍 2
1.2 Spark的介绍 5
1.3 Spark数据处理 RDD、DataFrame、Spark SQL 7
1.4 使用Python开发 Spark机器学习与大数据应用 8
1.5 Python Spark 机器学习 9
1.6 Spark ML Pipeline机器学习流程介绍 10
1.7 Spark 2.0的介绍 12
1.8 大数据定义 13
1.9 Hadoop 简介 14
1.10 Hadoop HDFS分布式文件系统 14
1.11 Hadoop MapReduce的介绍 17
1.12 结论 18
第2章 VirtualBox虚拟机软件的安装 19
2.1 VirtualBox的下载和安装 20
2.2 设置VirtualBox存储文件夹 23
2.3 在VirtualBox创建虚拟机 25
2.4 结论 29
第3章 Ubuntu Linux 操作系统的安装 30
3.1 Ubuntu Linux 操作系统的安装 31
3.2 在Virtual设置Ubuntu虚拟光盘文件 33
3.3 开始安装Ubuntu 35
3.4 启动Ubuntu 40
3.5 安装增强功能 41
3.6 设置默认输入法 45
3.7 设置“终端”程序 48
3.8 设置“终端”程序为白底黑字 49
3.9 设置共享剪贴板 50
3.10 设置最佳下载服务器 52
3.11 结论 56
第4章 Hadoop Single Node Cluster的安装 57
4.1 安装JDK 58
4.2 设置SSH无密码登录 61
4.3 下载安装Hadoop 64
4.4 设置Hadoop环境变量 67
4.5 修改Hadoop配置设置文件 69
4.6 创建并格式化HDFS目录 73
4.7 启动Hadoop 74
4.8 打开HadoopResource-Manager Web界面 76
4.9 NameNode HDFS Web界面 78
4.10 结论 79
第5章 Hadoop Multi Node Cluster的安装 80
5.1 把Single NodeCluster复制到data1 83
5.2 设置VirtualBox网卡 84
5.3 设置data1服务器 87
5.4 复制data1服务器到data2、data3、master 94
5.5 设置data2服务器 97
5.6 设置data3服务器 100
5.7 设置master服务器 102
5.8 master连接到data1、data2、data3 创建HDFS目录 107
5.9 创建并格式化NameNodeHDFS目录 110
5.10 启动Hadoop Multi Node Cluster 112
5.11 打开Hadoop ResourceManager Web界面 114
5.12 打开NameNode Web界面 115
5.13 停止Hadoop Multi Node Cluster 116
5.14 结论 116
第 6 章 Hadoop HDFS命令 117
6.1 启动HadoopMulti-Node Cluster 118
6.2 创建与查看HDFS目录 120
6.3 从本地计算机复制文件到HDFS 122
6.4 将HDFS上的文件复制到本地计算机 127
6.5 复制与删除HDFS文件 129
6.6 在Hadoop HDFSWeb用户界面浏览HDFS 131
6.7 结论 134
第7章 Hadoop MapReduce 135
7.1 简单介绍WordCount.java 136
7.2 编辑WordCount.java 137
7.3 编译WordCount.java 141
7.4 创建测试文本文件 143
7.5 运行WordCount.java 145
7.6 查看运行结果 146
7.7 结论 147
第8章 Python Spark的介绍与安装 148
8.1 Scala的介绍与安装 150
8.2 安装Spark 153
8.3 启动pyspark交互式界面 156
8.4 设置pyspark显示信息 157
8.5 创建测试用的文本文件 159
8.6 本地运行pyspark程序 161
8.7 在Hadoop YARN运行pyspark 163
8.8 构建SparkStandalone Cluster运行环境 165
8.9 在SparkStandalone运行pyspark 171
8.10 Spark Web UI界面 173
8.11 结论 175
第9章 在 IPythonNotebook 运行 Python Spark 程序 176
9.1 安装Anaconda 177
9.2 在IPythonNotebook使用Spark 180
9.3 打开IPythonNotebook笔记本 184
9.4 插入程序单元格 185
9.5 加入注释与设置程序代码说明标题 186
9.6 关闭IPythonNotebook 188
9.7 使用IPythonNotebook在Hadoop YARN-client模式运行 189
9.8 使用IPythonNotebook在Spark Stand Alone模式运行 192
9.9 整理在不同的模式运行IPythonNotebook的命令 194
9.9.1 在 Local 启动 IPython Notebook 195
9.9.2 在Hadoop YARN-client 模式启动 IPython Notebook 195
9.9.3 在Spark Stand Alone 模式启动 IPython Notebook 195
9.10 结论 196
第10章 Python Spark RDD 197
10.1 RDD的特性 198
10.2 开启IPython Notebook 199
10.3 基本RDD“转换”运算 201
10.4 多个RDD“转换”运算 206
10.5 基本“动作”运算 208
10.6 RDD Key-Value 基本“转换”运算 209
10.7 多个RDD Key-Value“转换”运算 212
10.8 Key-Value“动作”运算 215
10.9 Broadcast 广播变量 217
10.10 accumulator累加器 220
10.11 RDD Persistence持久化 221
10.12 使用Spark创建WordCount 223
10.13 Spark WordCount详细解说 226
10.14 结论 228
第11章 Python Spark的集成开发环境 229
11.1 下载与安装eclipse Scala IDE 232
11.2 安装PyDev 235
11.3 设置字符串替代变量 240
11.4 PyDev 设置 Python 链接库 243
11.5 PyDev设置anaconda2链接库路径 245
11.6 PyDev设置Spark Python链接库 247
11.7 PyDev设置环境变量 248
11.8 新建PyDev项目 251
11.9 加入WordCount.py程序 253
11.10 输入WordCount.py程序 254
11.11 创建测试文件并上传至HDFS目录 257
11.12 使用spark-submit执行WordCount程序 259
11.13 在Hadoop YARN-client上运行WordCount程序 261
11.14 在Spark Standalone Cluster上运行WordCount程序 264
11.15 在eclipse外部工具运行Python Spark程序 267
11.16 在eclipse运行spark-submit YARN-client 273
11.17 在eclipse运行spark-submit Standalone 277
11.18 结论 280
第12章 Python Spark创建推荐引擎 281
12.1 推荐算法介绍 282
12.2 “推荐引擎”大数据分析使用场景 282
12.3 ALS推荐算法的介绍 283
12.4 如何搜索数据 285
12.5 启动IPython Notebook 289
12.6 如何准备数据 290
12.7 如何训练模型 294
12.8 如何使用模型进行推荐 295
12.9 显示推荐的电影名称 297
12.10 创建Recommend项目 299
12.11 运行RecommendTrain.py 推荐程序代码 302
12.12 创建Recommend.py推荐程序代码 304
12.13 在eclipse运行Recommend.py 307
12.14 结论 310
第13章 Python Spark MLlib决策树二元分类 311
13.1 决策树介绍 312
13.2 “StumbleUpon Evergreen”大数据问题 313
13.2.1 Kaggle网站介绍 313
13.2.2 “StumbleUpon Evergreen”大数据问题场景分析 313
13.3 决策树二元分类机器学习 314
13.4 如何搜集数据 315
13.4.1 StumbleUpon数据内容 315
13.4.2 下载 StumbleUpon 数据 316
13.4.3 用LibreOffice Calc 电子表格查看train.tsv 319
13.4.4 复制到项目目录 322
13.5 使用IPython Notebook示范 323
13.6 如何进行数据准备 324
13.6.1 导入并转换数据 324
13.6.2 提取 feature 特征字段 327
13.6.3 提取分类特征字段 328
13.6.4 提取数值特征字段 331
13.6.5 返回特征字段 331
13.6.6 提取 label 标签字段 331
13.6.7 建立训练评估所需的数据 332
13.6.8 以随机方式将数据分为 3 部分并返回 333
13.6.9 编写 PrepareData(sc) 函数 333
13.7 如何训练模型 334
13.8 如何使用模型进行预测 335
13.9 如何评估模型的准确率 338
13.9.1 使用 AUC 评估二元分类模型 338
13.9.2 计算 AUC 339
13.10 模型的训练参数如何影响准确率 341
13.10.1 建立 trainEvaluateModel 341
13.10.2 评估impurity参数 343
13.10.3 训练评估的结果以图表显示 344
13.10.4 编写 evalParameter 347
13.10.5 使用 evalParameter 评估 maxDepth 参数 347
13.10.6 使用 evalParameter 评估 maxBins 参数 348
13.11 如何找出准确率最高的参数组合 349
13.12 如何确认是否过度训练 352
13.13 编写RunDecisionTreeBinary.py程序 352
13.14 开始输入RunDecisionTreeBinary.py程序 353
13.15 运行RunDecisionTreeBinary.py 355
13.15.1 执行参数评估 355
13.15.2 所有参数训练评估找出最好的参数组合 355
13.15.3 运行 RunDecisionTreeBinary.py 不要输入参数 357
13.16 查看DecisionTree的分类规则 358
13.17 结论 360
第14章 Python Spark MLlib 逻辑回归二元分类 361
14.1 逻辑回归分析介绍 362
14.2 RunLogisticRegression WithSGDBinary.py程序说明 363
14.3 运行RunLogisticRegression WithSGDBinary.py进行参数评估 367
14.4 找出最佳参数组合 370
14.5 修改程序使用参数进行预测 370
14.6 结论 372
第15章 Python Spark MLlib支持向量机SVM二元分类 373
15.1 支持向量机SVM算法的基本概念 374
15.2 运行SVMWithSGD.py进行参数评估 376
15.3 运行SVMWithSGD.py 训练评估参数并找出最佳参数组合 378
15.4 运行SVMWithSGD.py 使用最佳参数进行预测 379
15.5 结论 381
第16章 Python Spark MLlib朴素贝叶斯二元分类 382
16.1 朴素贝叶斯分析原理的介绍 383
16.2 RunNaiveBayesBinary.py程序说明 384
16.3 运行NaiveBayes.py进行参数评估 386
16.4 运行训练评估并找出最好的参数组合 387
16.5 修改RunNaiveBayesBinary.py 直接使用最佳参数进行预测 388
16.6 结论 390
第17章 Python Spark MLlib决策树多元分类 391
17.1 “森林覆盖植被”大数据问题分析场景 392
17.2 UCI Covertype数据集介绍 393
17.3 下载与查看数据 394
17.4 修改PrepareData() 数据准备 396
17.5 修改trainModel 训练模型程序 398
17.6 使用训练完成的模型预测数据 399
17.7 运行RunDecisionTreeMulti.py 进行参数评估 401
17.8 运行RunDecisionTreeMulti.py 训练评估参数并找出最好的参数组合 403
17.9 运行RunDecisionTreeMulti.py 不进行训练评估 404
17.10 结论 406
第18章 Python Spark MLlib决策树回归分析 407
18.1 Bike Sharing大数据问题分析 408
18.2 Bike Sharing数据集 409
18.3 下载与查看数据 409
18.4 修改 PrepareData() 数据准备 412
18.5 修改DecisionTree.trainRegressor训练模型 415
18.6 以 RMSE 评估模型准确率 416
18.7 训练评估找出最好的参数组合 417
18.8 使用训练完成的模型预测数据 417
18.9 运行RunDecisionTreeMulti.py进行参数评估 419
18.10 运行RunDecisionTreeMulti.py训练评估参数并找出最好的参数组合 421
18.11 运行RunDecisionTreeMulti.py 不进行训练评估 422
18.12 结论 424
第19章 Python Spark SQL、DataFrame、RDD数据统计与可视化 425
19.1 RDD、DataFrame、Spark SQL 比较 426
19.2 创建RDD、DataFrame与Spark SQL 427
19.2.1 在 local 模式运行 IPython Notebook 427
19.2.2 创建RDD 427
19.2.3 创建DataFrame 428
19.2.4 设置 IPython Notebook 字体 430
19.2.5 为DataFrame 创建别名 431
19.2.6 开始使用 Spark SQL 431
19.3 SELECT显示部分字段 434
19.3.1 使用 RDD 选取显示部分字段 434
19.3.2 使用 DataFrames 选取显示字段 434
19.3.3 使用 Spark SQL 选取显示字段 435
19.4 增加计算字段 436
19.4.1 使用 RDD 增加计算字段 436
19.4.2 使用 DataFrames 增加计算字段 436
19.4.3 使用 Spark SQL 增加计算字段 437
19.5 筛选数据 438
19.5.1 使用 RDD 筛选数据 438
19.5.2 使用 DataFrames 筛选数据 438
19.5.3 使用 Spark SQL 筛选数据 439
19.6 按单个字段给数据排序 439
19.6.1 RDD 按单个字段给数据排序 439
19.6.2 使用 Spark SQL排序 440
19.6.3 使用 DataFrames按升序给数据排序 441
19.6.4 使用 DataFrames按降序给数据排序 442
19.7 按多个字段给数据排序 442
19.7.1 RDD 按多个字段给数据排序 442
19.7.2 Spark SQL 按多个字段给数据排序 443
19.7.3 DataFrames 按多个字段给数据排序 443
19.8 显示不重复的数据 444
19.8.1 RDD 显示不重复的数据 444
19.8.2 Spark SQL 显示不重复的数据 445
19.8.3 Dataframes显示不重复的数据 445
19.9 分组统计数据 446
19.9.1 RDD 分组统计数据 446
19.9.2 Spark SQL分组统计数据 447
19.9.3 Dataframes分组统计数据 448
19.10 Join 联接数据 450
19.10.1 创建 ZipCode 450
19.10.2 创建 zipcode_tab 452
19.10.3 Spark SQL 联接 zipcode_table 数据表 454
19.10.4 DataFrame user_df 联接 zipcode_df 455
19.11 使用 Pandas DataFrames 绘图 457
19.11.1 按照不同的州统计并以直方图显示 457
19.11.2 按照不同的职业统计人数并以圆饼图显示 459
19.12 结论 461
第20章 Spark ML Pipeline 机器学习流程二元分类 462
20.1 数据准备 464
20.1.1 在 local 模式执行 IPython Notebook 464
20.1.2 编写 DataFrames UDF 用户自定义函数 466
20.1.3 将数据分成 train_df 与 test_df 468
20.2 机器学习pipeline流程的组件 468
20.2.1 StringIndexer 468
20.2.2 OneHotEncoder 470
20.2.3 VectorAssembler 472
20.2.4 使用 DecisionTreeClassi?er 二元分类 474
20.3 建立机器学习pipeline流程 475
20.4 使用pipeline进行数据处理与训练 476
20.5 使用pipelineModel 进行预测 477
20.6 评估模型的准确率 478
20.7 使用TrainValidation进行训练验证找出最佳模型 479
20.8 使用crossValidation交叉验证找出最佳模型 481
20.9 使用随机森林 RandomForestClassi?er分类器 483
20.10 结论 485
第21章 Spark ML Pipeline 机器学习流程多元分类 486
21.1 数据准备 487
21.1.1 读取文本文件 488
21.1.2 创建 DataFrame 489
21.1.3 转换为 double 490
21.2 建立机器学习pipeline流程 492
21.3 使用dt_pipeline进行数据处理与训练 493
21.4 使用pipelineModel 进行预测 493
21.5 评估模型的准确率 495
21.4 使用TrainValidation进行训练验证找出最佳模型 496
21.7 结论 498
第22章 Spark ML Pipeline 机器学习流程回归分析 499
22.1 数据准备 501
22.1.1 在local 模式执行 IPython Notebook 501
22.1.2 将数据分成 train_df 与 test_df 504
22.2 建立机器学习pipeline流程 504
22.3 使用dt_pipeline进行数据处理与训练 506
22.4 使用pipelineModel 进行预测 506
22.5 评估模型的准确率 507
22.6 使用TrainValidation进行训练验证找出最佳模型 508
22.7 使用crossValidation进行交叉验证找出最佳模型 510
22.8 使用GBT Regression 511
22.9 结论 513
附录A 本书范例程序下载与安装说明 514
A.1 下载范例程序 515
A.2 打开本书IPythonNotebook范例程序 516
A.3 打开 eclipsePythonProject 范例程序 518
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python+Spark 2.0+Hadoop机器学习与大数据实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop金融大数据分析
第1 章　大数据回顾 1
大数据是什么 1
数据量 2
数据速度 2
数据类型 3
大数据技术的演进 3
过去 3
现在 4
未来 5
大数据愿景 5
存储 6
NoSQL 6
NoSQL 数据库类型 7
资源管理 7
数据治理 8
批量计算 8
实时计算 8
数据整合工具 9
机器学习 9
商务智能和可视化 9
大数据相关的职业 10
Hadoop 架构 11
HDFS 集群 12
MapReduce V1 14
MapReduce V2——YARN 15
Hadoop 生态圈简介 18
驯服大数据 18
Hadoop——英雄 19
HDFS——Hadoop 分布式系统 19
Hadoop 版本 23
发行版——本地部署 25
发行版——云端 27
总结 28
第2 章　金融服务中的大数据 29
各个行业的大数据使用情况 29
卫生保健 30
人类科学 30
电信 31
在线零售商 31
为什么金融部门需要大数据 31
金融部门的大数据应用案例 34
HDFS 上的数据归档 34
监管 35
欺诈检测 35
交易数据 36
风险管理 36
客户行为预测 36
情感分析——非结构化 36
其他应用案例 37
金融大数据的演进过程 37
应该如何学习金融大数据 41
把你的数据上传到HDFS 上 41
从HDFS 上查询数据 42
在Hadoop 上的SQL 43
实时 44
数据治理和运营 44
ETL 工具 45
数据分析和商业智能 45
金融大数据的实现 46
关键挑战 46
克服挑战 47
总结 50
第3 章　在云端使用Hadoop. 51
大数据云的故事 51
原因 52
时机 53
收获 54
项目细节——在云中进行风险模拟 54
解决方案 55
现实世界 55
目标世界 57
数据转换 60
数据分析 62
总结 63
第4 章　使用Hadoop 进行数据迁移. 65
项目细节——归档你的交易数据 65
解决方案 67
项目第一阶段——分裂交易数据到数据仓库和Hadoop 68
项目第二阶段——完成数据从关系型数据仓库到Hadoop 的迁移 77
总结 83
第5 章　入门 85
项目详细信息——风险和监管报告 86
解决方案 87
现实世界 87
目标世界 88
数据收集 89
数据转换 97
数据分析 112
总结 116
第6 章　变得有经验. 117
实时大数据 117
项目细节——识别欺诈交易 119
解决方案 120
现实世界 120
目标世界 120
马尔科夫链模型执行——批处理模式 121
数据收集 126
数据转换 128
总结 132
第7 章　深入扩展Hadoop 的企业级应用. 133
扩展开来——实际上的水平 134
更多的大数据使用案例 135
使用案例——再谈欺诈问题 136
解决方案 136
使用案例——用户投诉 137
解决方案 137
使用案例——算法交易 137
解决方案 138
使用案例——外汇交易 138
解决方案 138
使用案例——基于社交媒体的交易数据 139
解决方案 139
使用案例——非大数据 140
解决方案 140
数据湖 140
Lambda 架构 143
大数据管理 144
Apache Falcon 概览 146
安全性 147
总结 149
第8 章　Hadoop 的快速增长 151
Hadoop 发行版的升级周期 151
最佳实践和标准 154
环境 154
与BI 和ETL 工具的集成 155
提示 155
新的趋势 157
总结 158
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop金融大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南（第2版） : Hadoop权威指南
第1章 初识Hadoop
数据！数据！
数据存储与分析
与其他系统相比
关系型数据库管理系统
网格计算
志愿计算
1.3.4 Hadoop 发展简史
Apache Hadoop和Hadoop生态圈
第2章 关于MapReduce
一个气象数据集
数据的格式
使用Unix工具进行数据分析
使用Hadoop分析数据
map阶段和reduce阶段
横向扩展
合并函数
运行一个分布式的MapReduce作业
Hadoop的Streaming
Ruby版本
Python版本
Hadoop Pipes
编译运行
第3章 Hadoop分布式文件系统
HDFS的设计
HDFS的概念
数据块
namenode和datanode
命令行接口
基本文件系统操作
Hadoop文件系统
接口
Java接口
从Hadoop URL中读取数据
通过FileSystem API读取数据
写入数据
目录
查询文件系统
删除数据
数据流
文件读取剖析
文件写入剖析
一致模型
通过 distcp并行拷贝
保持 HDFS 集群的均衡
Hadoop的归档文件
使用Hadoop归档文件
不足
第4章 Hadoop I/O
数据完整性
HDFS的数据完整性
LocalFileSystem
ChecksumFileSystem
压缩
codec
压缩和输入切分
在MapReduce中使用压缩
序列化
Writable接口
Writable类
实现定制的Writable类型
序列化框架
Avro
依据文件的数据结构
写入SequenceFile
MapFile
第5章 MapReduce应用开发
配置API
合并多个源文件
可变的扩展
配置开发环境
配置管理
辅助类GenericOptionsParser，Tool和ToolRunner
编写单元测试
mapper
reducer
本地运行测试数据
在本地作业运行器上运行作业
测试驱动程序
在集群上运行
打包
启动作业
MapReduce的Web界面
获取结果
作业调试
使用远程调试器
作业调优
分析任务
MapReduce的工作流
将问题分解成MapReduce作业
运行独立的作业
第6章 MapReduce的工作机制
剖析MapReduce作业运行机制
作业的提交
作业的初始化
任务的分配
任务的执行
进度和状态的更新
作业的完成
失败
任务失败
tasktracker失败
jobtracker失败
作业的调度
Fair Scheduler
Capacity Scheduler
shuffle和排序
map端
reduce端
配置的调优
任务的执行
推测式执行
重用JVM
跳过坏记录
任务执行环境
第7章 MapReduce的类型与格式
MapReduce的类型
默认的MapReduce作业
输入格式
输入分片与记录
文本输入
二进制输入
多种输入
数据库输入（和输出）
输出格式
文本输出
二进制输出
多个输出
延迟输出
数据库输出
第8章 MapReduce的特性
计数器
内置计数器
用户定义的Java计数器
用户定义的Streaming计数器
排序
准备
部分排序
总排序
二次排序
联接
map端联接
reduce端联接
边数据分布
利用JobConf来配置作业
分布式缓存
MapReduce库类
第9章 构建Hadoop集群
集群规范
网络拓扑
集群的构建和安装
安装Java
创建Hadoop用户
安装Hadoop
测试安装
SSH配置
Hadoop配置
配置管理
环境设置
Hadoop守护进程的关键属性
Hadoop守护进程的地址和端口
Hadoop的其他属性
创建用户帐号
安全性
Kerberos和Hadoop
委托令牌
其他安全性改进
利用基准测试程序测试Hadoop集群
Hadoop基准测试程序
用户的作业
云上的Hadoop
Amazon EC2上的Hadoop
第10章 管理Hadoop
HDFS
永久性数据结构
安全模式
日志审计
工具
监控
日志
度量
Java管理扩展（JMX）
维护
日常管理过程
委任节点和解除节点
升级
第11章 Pig简介
安装与运行Pig
执行类型
运行Pig程序
Grunt
Pig Latin编辑器
示例
生成示例
与数据库比较
PigLatin
结构
语句
表达式
1.4.4 类型
模式
函数
用户自定义函数
过滤UDF
计算UDF
加载UDF
数据处理操作
加载和存储数据
过滤数据
分组与连接数据
对数据进行排序
组合和分割数据
Pig实战
并行处理
参数代换
第12章 Hive
1.1 安装Hive
1.1.1 Hive外壳环境
1.2 示例
1.3 运行Hive
1.3.1 配置Hive
1.3.2 Hive服务
1.3.3 Metastore
1.4 和传统数据库进行比较
1.4.1 读时模式（Schema on Read）vs.写时模式（Schema on Write）
1.4.2 更新、事务和索引
1.5 HiveQL
1.5.1 数据类型
1.5.2 操作和函数
1.6 表
1.6.1 托管表（Managed Tables）和外部表（External Tables）
1.6.2 分区（Partitions）和桶（Buckets）
1.6.3 存储格式
1.6.4 导入数据
1.6.5 表的修改
1.6.6 表的丢弃
1.7 查询数据
1.7.1 排序（Sorting）和聚集（Aggregating）
1.7.2 MapReduce脚本
1.7.3 连接
1.7.4 子查询
1.7.5 视图（view）
1.8 用户定义函数（User-Defined Functions）
1.8.1 编写UDF
1.8.2 编写UDAF
第13章 HBase
2.1 HBasics
2.1.1 背景
2.2 概念
2.2.1 数据模型的“旋风之旅”
2.2.2 实现
2.3 安装
2.3.1 测试驱动
2.4 客户机
2.4.1 Java
2.4.2 Avro，REST，以及Thrift
2.5 示例
2.5.1 模式
2.5.2 加载数据
2.5.3 Web查询
2.6 HBase和RDBMS的比较
2.6.1 成功的服务
2.6.2 HBase
2.6.3 实例：HBase在Streamy.com的使用
2.7 Praxis
2.7.1 版本
2.7.2 HDFS
2.7.3 用户接口（UI）
2.7.4 度量（metrics）
2.7.5 模式设计
2.7.6 计数器
2.7.7 批量加载（bulkloading）
第14章 ZooKeeper
安装和运行ZooKeeper
示例
ZooKeeper中的组成员关系
创建组
加入组
列出组成员
ZooKeeper服务
数据模型
操作
实现
一致性
会话
状态
使用ZooKeeper来构建应用
配置服务
具有可恢复性的ZooKeeper应用
锁服务
生产环境中的ZooKeeper
可恢复性和性能
配置
第15章 开源工具Sqoop
获取Sqoop
一个导入的例子
生成代码
其他序列化系统
深入了解数据库导入
导入控制
导入和一致性
直接模式导入
使用导入的数据
导入的数据与Hive
导入大对象
执行导出
深入了解导出
导出与事务
导出和SequenceFile
第16章 实例分析
Hadoop 在Last.fm的应用
Last.fm：社会音乐史上的革命
Hadoop a Last.fm
用Hadoop产生图表
Track Statistics程序
总结
Hadoop和Hive在Facebook的应用
概要介绍
Hadoop a Facebook
假想的使用情况案例
Hive
问题与未来工作计划
Nutch 搜索引擎
背景介绍
数据结构
Nutch系统利用Hadoop进行数据处理的精选实例
总结
Rackspace的日志处理
简史
选择Hadoop
收集和存储
日志的MapReduce模型
关于Cascading
字段、元组和管道
操作
Tap类，Scheme对象和Flow对象
Cascading实战
灵活性
Hadoop和Cascading在ShareThis的应用
总结
在Apache Hadoop上的TB字节数量级排序
使用Pig和Wukong来探索10亿数量级边的 网络图
测量社区
每个人都在和我说话：Twitter回复关系图
（度）degree
对称链接
社区提取
附录A 安装Apache Hadoop
附录B Cloudera’s Distribution for Hadoop
附录C 准备NCDC天气数据
索引
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop权威指南（第2版） : Hadoop权威指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战手册
《hadoop实战手册》
第1章　hadoop分布式文件系统——导入和导出数据　1
1.1　介绍　1
1.2　使用hadoop shell命令导入和导出数据到hdfs　2
1.3　使用distcp实现集群间数据复制　7
1.4　使用sqoop从mysql数据库导入数据到hdfs　9
1.5　使用sqoop从hdfs导出数据到mysql　12
1.6　配置sqoop以支持sql server　15
1.7　从hdfs导出数据到mongodb　17
1.8　从mongodb导入数据到hdfs　20
1.9　使用pig从hdfs导出数据到mongodb　23
1.10　在greenplum外部表中使用hdfs　24
1.11　利用flume加载数据到hdfs中　26
第2章　hdfs　28
2.1　介绍　28
2.2　读写hdfs数据　29
2.3　使用lzo压缩数据　31
2.4　读写序列化文件数据　34
2.5　使用avro序列化数据　37
2.6　使用thrift序列化数据　41
.2.7　使用protocol buffers序列化数据　44
2.8　设置hdfs备份因子　48
2.9　设置hdfs块大小　49
第3章　抽取和转换数据　51
3.1　介绍　51
3.2　使用mapreduce将apache日志转换为tsv格式　52
3.3　使用apache pig过滤网络服务器日志中的爬虫访问量　54
3.4　使用apache pig根据时间戳对网络服务器日志数据排序　57
3.5　使用apache pig对网络服务器日志进行会话分析　59
3.6　通过python扩展apache pig的功能　61
3.7　使用mapreduce及二次排序计算页面访问量　62
3.8　使用hive和python清洗、转换地理事件数据　67
3.9　使用python和hadoop streaming执行时间序列分析　71
3.10　在mapreduce中利用multipleoutputs输出多个文件　75
3.11　创建用户自定义的hadoop writable及inputformat读取地理事件数据　78
第4章　使用hive、pig和mapreduce处理常见的任务　85
4.1　介绍　85
4.2　使用hive将hdfs中的网络日志数据映射为外部表　86
4.3　使用hive动态地为网络日志查询结果创建hive表　87
4.4　利用hive字符串udf拼接网络日志数据的各个字段　89
4.5　使用hive截取网络日志的ip字段并确定其对应的国家　92
4.6　使用mapreduce对新闻档案数据生成n-gram　94
4.7　通过mapreduce使用分布式缓存查找新闻档案数据中包含关键词的行　98
4.8　使用pig加载一个表并执行包含group by的select操作　102
第5章　高级连接操作　104
5.1　介绍　104
5.2　使用mapreduce对数据进行连接　104
5.3　使用apache pig对数据进行复制连接　108
5.4　使用apache pig对有序数据进行归并连接　110
5.5　使用apache pig对倾斜数据进行倾斜连接　111
5.6　在apache hive中通过map端连接对地理事件进行分析　113
5.7　在apache hive通过优化的全外连接分析地理事件数据　115
5.8　使用外部键值存储(redis)连接数据　118
第6章　大数据分析　123
6.1　介绍　123
6.2　使用mapreduce和combiner统计网络日志数据集中的独立ip数　124
6.3　运用hive日期udf对地理事件数据集中的时间日期进行转换与排序　129
6.4　使用hive创建基于地理事件数据的每月死亡报告　131
6.5　实现hive用户自定义udf用于确认地理事件数据的来源可靠性　133
6.6　使用hive的map/reduce操作以及python标记最长的无暴力发生的时间区间　136
6.7　使用pig计算audioscrobbler数据集中艺术家之间的余弦相似度　141
6.8　使用pig以及datafu剔除audioscrobbler数据集中的离群值　145
第7章　高级大数据分析　147
7.1　介绍　147
7.2　使用apache giraph计算pagerank　147
7.3　使用apache giraph计算单源最短路径　150
7.4　使用apache giraph执行分布式宽度优先搜索　158
7.5　使用apache mahout计算协同过滤　165
7.6　使用apache mahout进行聚类　168
7.7　使用apache mahout进行情感分类　171
第8章　调试　174
8.1　介绍　174
8.2　在mapreduce中使用counters监测异常记录　174
8.3　使用mrunit开发和测试mapreduce　177
8.4　本地模式下开发和测试mapreduce　179
8.5　运行mapreduce作业跳过异常记录　182
8.6　在流计算作业中使用counters　184
8.7　更改任务状态显示调试信息　185
8.8　使用illustrate调试pig作业　187
第9章　系统管理　189
9.1　介绍　189
9.2　在伪分布模式下启动hadoop　189
9.3　在分布式模式下启动hadoop　192
9.4　添加一个新节点　195
9.5　节点安全退役　197
9.6　namenode故障恢复　198
9.7　使用ganglia监控集群　199
9.8　mapreduce作业参数调优　201
第10章　使用apache accumulo进行持久化　204
10.1　介绍　204
10.2　在accumulo中设计行键存储地理事件　205
10.3　使用mapreduce批量导入地理事件数据到accumulo　213
10.4　设置自定义字段约束accumulo中的地理事件数据　220
10.5　使用正则过滤器限制查询结果　225
10.6　使用sumcombiner计算同一个键的不同版本的死亡数总和　228
10.7　使用accumulo实行单元级安全的扫描　232
10.8　使用mapreduce聚集accumulo中的消息源　237
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop实战手册
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop专家：管理、调优与SparkYARNHDFS安全
第Ⅰ部分 Hadoop架构与Hadoop集群介绍
第1章 Hadoop与Hadoop环境介绍 3
Hadoop简介 4
Hadoop 的特性 5
Hadoop 与大数据 5
Hadoop 的典型应用场景 6
传统数据库系统 7
数据湖 9
大数据、数据科学和Hadoop 10
Hadoop集群与集群计算 11
集群计算 11
Hadoop 集群 12
Hadoop组件和Hadoop生态 14
Hadoop管理员需要做些什么 16
Hadoop 管理—新的范式 17
关于Hadoop 管理你需要知道的 18
Hadoop 管理员的工具集 19
Hadoop 1和Hadoop 2的关键区别 19
架构区别 20
高可用性 20
多计算引擎 21
分离处理和调度 21
Hadoop 1 和Hadoop 2 中的资源分配 22
分布式数据处理：MapReduce和Spark、Hive、Pig 22
MapReduce 22
Apache Spark 23
Apache Hive 24
Apache Pig 24
数据整合：Apache Sqoop、Apache Flume和Apache Kafka 25
Hadoop管理中的关键领域 26
集群存储管理 26
集群资源分配 26
作业调度 27
Hadoop 数据安全 27
总结 28
第2章 Hadoop架构介绍 31
Hadoop与分布式计算 31
Hadoop 架构 32
Hadoop 集群 33
主节点和工作节点 33
Hadoop 服务 34
数据存储—Hadoop分布式文件系统 35
HDFS 特性 35
HDFS 架构 36
HDFS 文件系统 38
NameNode 操作 41
利用YARN（Hadoop操作系统）进行数据处理 45
YARN 的架构 46
ApplicationMaster 如何与ResourceManager 协作进行资源分配 51
总结 54
第3章 创建和配置一个简单的Hadoop集群 55
Hadoop发行版本和安装类型 56
Hadoop 发行版本 56
Hadoop 安装类型 57
设置一个伪分布式Hadoop集群 58
满足操作系统的要求 58
修改内核参数 59
设置SSH 64
Java 需求 65
安装Hadoop 66
创建必要的Hadoop 用户 66
创建必要的目录 67
Hadoop初始配置 67
环境变量配置文件 69
只读默认配置文件 70
site 专用配置文件 70
其他Hadoop 相关的配置文件 71
配置文件的优先级 72
可变扩展和配置参数 74
配置Hadoop 守护进程环境变量 74
配置Hadoop 的核心属性（使用core-site.xml 文件） 76
配置MapReduce（使用mapred-site.xml 文件） 78
配置YARN（使用yarn-site.xml 文件） 79
配置HDFS（使用hdfs-site.xml 文件） 80
操作新的Hadoop集群 82
格式化分布式文件系统 82
设置环境变量 82
启动HDFS 和YARN 服务 83
验证服务启动 85
关闭服务 85
总结 86
第4章 规划和创建一个完全分布式集群 87
规划Hadoop集群 88
集群规划注意事项 88
安排服务器 90
节点选择的标准 90
从单机架到多机架 91
调整Hadoop 集群 91
CPU、内存和存储选择的一般性原则 92
主节点的特殊要求 95
关于服务器大小的几点建议 96
集群增长 97
大型集群指南 97
创建一个多节点集群 98
如何设置测试集群 98
修改Hadoop的配置 102
更改HDFS 的配置（hdfs-site.xml 文件） 102
更改YARN 的配置 105
修改MapReduce 的配置 109
启动集群 110
使用脚本启动和关闭集群 112
快速检查新集群的文件系统 113
配置Hadoop服务、Web界面和端口 114
服务配置和Web 界面 115
设置Hadoop 服务的端口 117
Hadoop 客户端 120
总结 122
第Ⅱ部分 Hadoop应用架构
第5章 在集群上运行一个应用—MapReduce框架和Hive、Pig 125
MapReduce框架 125
MapReduce 模型 126
MapReduce 怎样工作 127
MapReduce 作业处理 129
一个简单的MapReduce 程序 130
通过运行WordCount 程序理解Hadoop 作业的处理过程 132
MapReduce 输入/ 输出目录 133
Hadoop 如何展示作业细节 133
Hadoop Streaming 135
Apache Hive 137
Hive 数据组织 138
使用Hive 表 138
将数据导入Hive 138
使用Hive 查询 139
Apache Pig 139
Pig 执行模型 140
一个简单的Pig 示例 140
总结 141
第6章 集群上的应用—Spark框架介绍 143
Spark是什么 144
为什么使用Spark 145
速度 145
易用性 147
通用框架 148
Spark 和Hadoop 148
Spark技术栈 149
安装Spark 151
Spark 示例 152
Spark 的主要文件和目录 153
编译Spark 二进制文件 153
减少Spark 日志 153
Spark运行模式 154
本地模式 154
集群模式 154
集群管理器 154
独立集群管理器 155
基于Apache Mesos 的Spark 157
基于YARN 的Spark 158
YARN 和Spark 如何协同合作 159
设置基于Hadoop 集群的Spark 159
Spark和数据获取 159
从Linux 文件系统加载数据 160
从HDFS 加载数据 160
从关系型数据库获取数据 161
总结 162
第7章 运行Spark应用程序 163
Spark编程模型 163
Spark 编程和RDD 164
Spark 编程 166
Spark应用程序 167
RDD 基础 168
创建RDD 168
RDD 操作 171
RDD 持久化 173
Spark应用的结构 174
Spark 术语 174
Spark 应用程序的组件 174
交互式运行Spark应用程序 175
Spark shell 和Spark 应用程序 176
Spark shell 176
使用Spark shell 176
Spark 集群执行概述 179
创建和提交Spark应用 180
构建Spark 应用 180
在独立的Spark 集群上运行应用 180
使用spark-submit 执行应用 181
在Mesos 上运行Spark 应用 183
在Hadoop YARN 集群上运行Spark 应用 183
使用JDBC/ODBC 服务 186
配置Spark应用 187
Spark 的配置属性 187
运行spark-submit 时的配置 187
监控Spark应用 188
使用Spark Streaming处理流式计算 189
Spark Streaming 如何工作 189
Spark Streaming 示例，又是WordCount 191
使用Spark SQL 处理结构化数据 192
数据框架 192
HiveContext 和SQLContext 193
使用Spark SQL 193
创建DataFrames 195
总结 195
第Ⅲ部分 管理和保护Hadoop数据和高可用性
第8章 NameNode的作用和HDFS的工作原理 199
HDFS—NameNode与DataNode之间的交互 200
客户端和HDFS 之间的交互 200
NameNode 与DataNode 之间的通信 201
机架感知与拓扑逻辑 203
如何在集群中配置机架感知策略 204
找出集群的机架信息 204
HDFS 数据副本 206
HDFS 数据组织和数据块 207
数据复制 207
文件块和副本状态 209
客户端如何读写HDFS数据 213
客户端如何读取HDFS 数据 213
客户端如何向HDFS 写数据 214
了解HDFS恢复过程 217
生成戳 218
租约恢复 218
块恢复 219
管道恢复 219
HDFS中的集中式缓存管理 220
Hadoop 和OS 的页面缓存 221
集中式缓存管理的关键原则 221
集中式缓存管理如何工作 221
配置缓存 222
缓存指令 223
缓存池 223
使用缓存 223
Hadoop归档存储、SSD和内存（异构存储） 225
不同存储类型的性能特点 225
对异构HDFS 存储的需求 226
存储体系结构的变化 227
文件的存储首选项 228
设置归档存储 228
管理存储策略 232
移动数据 232
实现归档 233
总结 234
第9章 HDFS命令、HDFS权限和HDFS存储 235
使用HDFS Shell命令管理HDFS 235
使用hdfs dfs 实用程序来管理HDFS 237
列出HDFS 文件和目录 239
创建HDFS 目录 241
删除HDFS 文件和目录 242
更改文件和目录所有权和组 242
使用dfsadmin实用程序执行HDFS操作 243
dfsadmin -report 命令 245
管理HDFS权限和用户 247
HDFS 文件权限 247
HDFS 用户和超级用户 249
管理HDFS存储 252
检查HDFS 磁盘使用情况 252
分配HDFS 空间配额 255
重新均衡HDFS数据 259
HDFS 数据不均衡的原因 260
运行均衡器以均衡HDFS 数据 260
使用hdfs dfsadmin 使事情更简单 263
何时运行均衡器 265
回收HDFS空间 266
删除文件和目录 266
降低复制因子 266
总结 268
第10章 数据保护、文件格式和访问HDFS 269
保护数据 270
使用HDFS 回收站防止意外数据删除 270
使用HDFS 快照保护重要数据 272
通过文件系统检查确保数据完整性 276
数据压缩 281
常用压缩格式 282
评估各种压缩方案 282
MapReduce 的各个阶段的压缩 283
Spark 的压缩 286
数据序列化 286
Hadoop文件格式 287
确定正确文件格式的标准 288
Hadoop 支持的文件格式 289
理想文件格式 294
Hadoop 小文件问题和合并文件 294
使用NameNode 联合架构克服小文件问题 295
使用Hadoop Archives 管理小文件 295
减小小文件的性能影响 298
使用Hadoop WebHDFS和HttpFS 299
WebHDFS—Hadoop REST API 299
使用WebHDFS API 300
了解WebHDFS 命令 301
使用HttpFS 网关从防火墙后面访问HDFS 304
总结 306
第11章 NameNode操作、高可用性和联合 307
了解NameNode操作 308
HDFS 元数据 309
NameNode 启动过程 311
NameNode 和DataNode 如何协同工作 311
检查点操作 313
Secondary NameNode、检查点节点、备份节点和Standby NameNode 314
配置检查点操作频率 315
管理检查点性能 316
检查点的机制 317
NameNode安全模式操作 319
自动安全模式操作 319
将NameNode 置于安全模式 320
NameNode 如何进行模式转换 321
备份和恢复NameNode 元数据 322
配置HDFS高可用性 324
NameNode HA 架构（QJM） 325
设置HDFS HA Quorum 集群 327
部署高可用性NameNode 331
管理HA NameNode 设置 335
HA 手动和自动故障转移 336
HDFS联合 338
联合NameNode 的体系结构 339
总结 340
第Ⅳ部分 数据迁移、资源分配、作业调度及安全
第12章 将数据导入和导出Hadoop 343
Hadoop数据传输工具简介 343
通过命令行将数据加载到HDFS 344
使用-cat 命令转储文件的内容 344
检测HDFS 文件 345
从HDFS 或向HDFS 复制或移动文件 346
使用-get 命令移动文件 347
向HDFS 或从HDFS 移动文件 348
使用-tail 和head 命令 348
使用DistCp在HDFS集群之间复制数据 349
如何使用DistCp 命令移动数据 349
DistCp 选项 351
使用Sqoop从关系型数据库获取数据 353
Sqoop 架构 354
部署Sqoop 355
使用Sqoop 移动数据 356
使用Sqoop 导入数据 356
将数据导入Hive 367
使用Sqoop 导出数据 369
通过Flume从外部来源采集数据 376
Flume 架构简介 376
配置Flume agent 378
简单的Flume 示例 379
使用Flume 将数据移动到HDFS 381
更复杂的Flume 示例 383
与Kafka交互数据 385
Kafka 的优点 386
Kafka 是如何工作的 386
设置Apache Kafka 集群 388
将Kafka 与Hadoop 和Storm 集成 392
总结 393
第13章 Hadoop集群中的资源分配 395
Hadoop中的资源分配 395
管理集群的工作负载 396
Hadoop 的资源调度器 397
FIFO调度器 398
容量调度器 399
队列和子队列 400
集群如何分配资源 405
抢占申请 408
启用容量调度器 409
一个典型的容量调度器 409
公平调度器 413
队列 414
配置公平调度器 415
作业是如何被放置到队列中的 417
公平调度器中的应用抢占 418
安全和资源池 419
一个fair-scheduler.xml 示例文件 419
将作业提交到调度器 421
在队列之间移动应用程序 421
监控公平调度器 422
容量调度器和公平调度器的对比 422
两个调度器之间的相似之处 422
两个调度器之间的差异 422
总结 423
第14章 使用Oozie管理作业工作流 425
使用Apache Oozie调度作业 425
Oozie架构 427
Oozie 服务器 427
Oozie 客户端 428
Oozie 数据库 428
在集群中部署Oozie 429
安装和配置Oozie 430
为Oozie 配置Hadoop 432
了解Oozie工作流 434
工作流、控制流和节点 434
使用workﬂow.xml 文件定义工作流 435
Oozie如何运行一个动作 436
配置动作节点 437
创建Oozie工作流 442
配置控制节点 443
配置作业 448
运行Oozie工作流作业 449
指定作业属性 449
部署Oozie 作业 451
创建动态工作流 451
Oozie 协调器 452
基于时间的协调器 453
基于数据的协调器 455
基于时间和数据的协调器 456
从命令行提交Oozie 协调器 457
管理和治理Oozie 458
常见的Oozie 命令 458
Oozie 故障排除 460
Oozie cron 调度和Oozie SLA 461
总结 462
第15章 Hadoop安全 463
Hadoop安全概览 464
认证、授权和审计 466
使用Kerberos进行Hadoop认证 467
Kerberos 及其工作原理 467
Kerberos 认证过程 469
Kerberos 互信 470
一个特殊主体 471
将Kerberos 添加到集群中 471
Hadoop 相关的Kerberos 设置 476
使用Kerberos 保护Hadoop 集群 480
Kerberos 如何验证用户和服务 486
管理Kerberized Hadoop 集群 487
Hadoop授权 490
HDFS 权限 491
服务级授权 495
基于角色的Apache Sentry 权限设置 497
Hadoop审计 503
审计HDFS 操作 504
审计YARN 操作 504
Hadoop数据安全 505
HDFS 透明加密 505
加密转换中的数据 508
其他Hadoop安全举措 509
使用Apache Knox 网关保护Hadoop 基础设施 509
Apache Ranger 安全管理 509
总结 510
第Ⅴ部分 监控、优化和故障排除
第16章 管理作业、使用Hue和执行常规任务 513
使用YARN命令管理Hadoop作业 514
查看YARN 应用程序 515
检查应用程序的状态 516
Kill 正在执行的作业 516
检查节点状态 517
检查YARN 的队列状态 517
获取作业的日志 517
YARN 管理命令 518
下线和上线节点 519
包含和剔除主机 520
下线DataNodes 和NodeManagers 521
重新上线节点 522
关于下线和重新上线的注意事项 523
添加新的DataNode 和NodeManager 524
高可用性ResourceManager 524
高可用性ResourceManager 架构 525
设置高可用性ResourceManager 525
ResourceManager 故障转移 526
使用ResourceManager 高可用性命令 528
执行常规管理任务 529
将NameNode 移动到不同的主机 529
管理高可用性NameNode 529
使用关闭/ 启动脚本来管理集群 530
均衡HDFS 530
均衡DataNodes 上存储 531
管理MySQL数据库 532
配置MySQL 数据库 532
配置高可用性MySQL 533
备份重要集群数据 535
备份HDFS 元数据 535
备份Metastore 数据库 537
使用Hue管理集群 537
允许用户使用Hue 538
安装Hue 538
配置集群以使用Hue 540
管理Hue 544
使用Hue 544
使用HDFS的附加功能 545
在多宿主网络中部署HDFS 和YARN 545
短路本地读取 546
可挂载的HDFS 548
使用NFS 网关将HDFS 挂载到本地文件系统 549
总结 551
第17章 监控、指标和Hadoop日志 553
监控Linux服务器 554
Linux 系统监控基础 554
Linux 系统监控工具 556
Hadoop指标 559
Hadoop 指标类型 560
使用Hadoop 指标 561
收集文件系统的指标 561
使用Ganglia进行监测 563
Ganglia 架构 563
Ganglia 和Hadoop 整合 564
设置Hadoop 指标 565
Hadoop日志记录 566
Hadoop 日志消息 566
守护进程和应用程序日志以及如何查看这些日志 568
应用程序日志记录的工作原理 568
Hadoop 如何使用HDFS 目录和本地目录 570
NodeManager 如何使用本地目录 571
通过日志聚合将作业日志存储在HDFS 中 576
使用Hadoop 守护程序日志 580
使用Hadoop的Web UI进行监控 582
使用ResourceManager Web UI 监控作业 583
JobHistoryServer Web UI 589
使用NameNode Web UI 进行监控 591
监控其他Hadoop组件 592
监控Hive 592
监控Spark 593
总结 593
第18章 调优集群资源，优化MapReduce作业和基准测试 595
如何分配YARN内存和CPU 596
分配内存 596
配置CPU 内核数量 604
内存与CPU 之间的关系 605
配置高性能 605
推测执行 605
减少系统上的I/O 负载 608
调整map和reduce任务，管理员可以做什么 608
map 任务调优 609
输入和输出 610
reduce 任务调优 613
MapReduce shufﬂe 进程调优 615
优化Pig和Hive作业 617
优化Hive 作业 618
优化pig 作业 619
对集群进行基准测试 621
使用TestDFSIO 测试I / O 性能 621
使用TeraSort 进行基准测试 623
使用Hadoop 的Rumen 和GridMix 进行基准测试 625
Hadoop计数器 629
文件系统计数器 629
作业计数器 631
MapReduce 框架计数器 632
自定义Java 计数器 633
限制计数器数量 633
优化MapReduce 633
map-only 与map 及reduce 作业 634
使用combiners 提升MapReduce 性能 634
使用partitioner 提高性能 635
在MapReduce 过程中压缩数据 636
太多的map 和reduce 任务 637
总结 639
第19章 在YARN上配置和调优Apache Spark 641
在YARN上配置Spark的资源分配 642
分配CPU 642
分配内存 642
如何把资源分配给Spark 642
Spark 应用程序的资源分配限制 643
将资源分配给驱动程序 645
为执行器配置资源 648
Spark 如何使用内存 652
要注意的事情 654
集群或客户端模式 656
配置Spark 相关网络参数 657
YARN Spark动态资源分配 658
动态和静态资源分配 658
如何管理动态资源分配 658
启用动态资源分配 659
存储格式和压缩数据 660
存储格式 660
文件大小 662
压缩 662
监控Spark应用程序 663
使用Spark Web UI 了解性能 663
Spark 系统和Metrics REST API 666
YARN 上的Spark 历史记录服务器 666
从命令行跟踪作业 668
调优垃圾回收 668
垃圾回收机制 668
如何收集GC 统计数据 669
调优Spark Streaming应用程序 670
减少批处理时间 670
设置正确的批次间隔 670
调优内存和垃圾回收 671
总结 671
第20章 优化Spark应用程序 673
重新审视Spark执行模型 674
Spark 执行模型 674
shufﬂe操作以及如何减少shufﬂe操作 676
WordCount 示例（再一次展示） 676
shufﬂe 操作的影响 678
配置shufﬂe 参数 679
分区和并行性（任务数） 684
并行度 685
极少数任务的问题 687
设置默认分区数 687
如何增加分区数量 688
使用Repartition（重新分区）和Coalesce（合并）
操作来更改RDD 中的分区数 689
两种类型的分区器 690
数据分区和如何避免shufﬂe 690
数据的序列化和压缩优化 691
数据序列化 691
配置压缩 692
Spark的SQL查询优化器 693
优化步骤 693
Spark 的推测执行功能 695
数据本地化的重要性 696
缓存数据 698
缓存容错 699
如何指定缓存 699
总结 704
第21章 Hadoop故障排除—样例 705
空间相关问题 705
处理Linux 文件系统100% 使用的情况 706
HDFS 空间问题 707
本地目录以及日志目录空间超出 707
磁盘容错 709
处理卡住的YARN作业 710
JVM内存分配与垃圾回收策略 712
理解JVM 垃圾回收 712
优化垃圾回收 713
Analyzing Memory Usage 713
内存不足问题 714
ApplicationMaster 内存问题 715
处理不同类型的错误 716
处理守护进程失败的情况 716
启动Hadoop 守护进程失败 717
任务和作业失败 718
Spark作业故障排除 719
Spark 的容错机制 720
杀死Spark 作业 720
一个作业的最大尝试次数 720
一个作业最大的失败次数 720
调试Spark应用 720
通过日志聚合访问日志 720
当日志聚合未开启时访问日志 721
重新审视启动环境 721
总结 722
附录A 安装VirtualBox和Linux以及虚拟机的克隆 723
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop专家：管理、调优与SparkYARNHDFS安全
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>写给大忙人的Hadoop 2
1 背景和概念 1
定义Apache Hadoop 1
Apache Hadoop 的发展简史 3
大数据的定义 4
Hadoop 作为数据湖 5
使用Hadoop:管理员、用户或两种身份兼具 7
原始的MapReduce. 7
Apache Hadoop 的设计原则 8
Apache Hadoop MapReduce 示例 8
MapReduce 的优势 10
Apache Hadoop V1 MapReduce 操作 11
使用Hadoop V2 超越MapReduce 13
Hadoop V2 YARN 操作设计 14
Apache Hadoop 项目生态系统 16
总结和补充资料 18
2 安装攻略 21
核心Hadoop 服务 21
Hadoop 配置文件 22
规划你的资源 23
硬件的选择 23
软件的选择 24
在台式机或笔记本电脑上安装 25
安装Hortonworks HDP 2.2 沙箱 25
用Apache 源代码安装Hadoop 32
配置单节点YARN 服务器的步骤 33
运行简单的MapReduce 示例 42
安装 Apache Pig（可选） 42
安装Apache Hive（可选） 43
使用Ambari 安装Hadoop 44
执行Ambari 安装 45
撤消Ambari 安装 59
使用Apache Whirr 在云中安装Hadoop 59
总结和补充资料 65
3 HDFS 基础知识 67
HDFS 设计的特点 67
HDFS 组件 68
HDFS 块复制 71
HDFS 安全模式 72
机架的识别 73
NameNode 高可用性 73
HDFS NameNode 联邦 75
HDFS 检查点和备份 76
HDFS 快照 76
HDFS NFS 网关 76
HDFS 用户命令 77
简要HDFS 命令参考 77
一般HDFS 命令 78
列出HDFS 中的文件 79
在HDFS 中创建一个目录 80
将文件复制到HDFS 80
从HDFS 复制文件 81
在HDFS 中复制文件 81
删除在HDFS 中的文件 81
删除在HDFS 中的目录 81
获取HDFS 状态报告 81
HDFS 的Web 图形用户界面 82
在程序中使用HDFS 82
HDFS Java 应用程序示例 82
HDFS C 应用程序示例 86
总结和补充资料 88
4 运行示例程序和基准测试程序 91
运行MapReduce 示例 91
列出可用的示例 92
运行Pi 示例 93
使用Web 界面监控示例 95
运行基本Hadoop 基准测试程序 101
运行Terasort 测试 101
运行TestDFSIO 基准 102
管理Hadoop MapReduce 作业 104
总结和补充资料 105
5 Hadoop MapReduce 框架 107
MapReduce 模型 107
MapReduce 并行数据流. 110
容错和推测执行 114
推测执行. 114
Hadoop MapReduce 硬件 115
总结和补充资料 115
6 MapReduce 编程 117
编译和运行Hadoop WordCount 的示例 117
使用流式接口 122
使用管道接口 125
编译和运行Hadoop Grep 链示例 127
调试MapReduce. 131
作业的列举、清除和状态查询 131
Hadoop 日志管理 131
启用YARN 日志聚合 132
Web 界面日志查看 133
命令行日志查看 133
总结和补充资料 135
7 基本的Hadoop 工具 137
使用Apache Pig 137
Pig 示例演练 138
使用Apache Hive 140
Hive 示例演练 140
更高级的Hive 示例 142
使用Apache Sqoop 获取关系型数据 145
Apache Sqoop 导入和导出方法 145
Apache Sqoop 版本更改 147
Sqoop 示例演练 148
使用Apache Flume 获取数据流 155
Flume 的示例演练 157
使用Apache Oozie 管理 Hadoop 工作流 160
Oozie 示例演练 162
使用Apache HBase 170
HBase 数据模型概述 170
HBase 示例演练 171
总结和补充资料 176
8 Hadoop YARN 应用程序 179
YARN 分布式shell 179
使用YARN 分布式shell 180
一个简单的示例 181
使用更多的容器 182
带有shell 参数的分布式 shell 示例 183
YARN 应用程序的结构 185
YARN 应用程序框架 187
Hadoop MapReduce 188
Apache Tez 188
Apache Giraph 189
Hoya：HBase on YARN 189
Dryad on YARN 189
Apache Spark 189
Apache Storm 190
Apache REEF：可持续计算执行框架 190
Hamster：Hadoop 和MPI 在同一集群 190
Apache Flink：可扩展的批处理和流式数据处理 191
Apache Slider：动态应用程序管理 191
总结和补充资料 192
9 用Apache Ambari 管理Hadoop 193
快速浏览 Apache Ambari 194
仪表板视图 194
服务视图. 197
主机视图. 199
管理视图. 201
查看视图. 201
Admin 下拉菜单 202
更改Hadoop 属性 206
总结和补充资料 212
10 基本的Hadoop 管理程序 213
基本的Hadoop YARN 管理 214
停用YARN 节点 214
YARN WebProxy 214
使用 JobHistoryServer 215
管理YARN 作业 215
设置容器内存 215
设置容器核心 216
设置MapReduce 属性 216
基本的HDFS 管理 217
NameNode 用户界面 217
将用户添加到HDFS 219
在HDFS 上执行FSCK 220
平衡HDFS 221
HDFS 安全模式 222
停用HDFS 节点 222
SecondaryNameNode 223
HDFS 快照 223
配置到HDFS 的NFSv3 网关 225
容量调度程序背景知识 229
Hadoop 2 的MapReduce 兼容性 231
启用应用主控程序的重新启动功能 231
计算一个节点的承载容量 232
运行Hadoop 1 的应用程序 233
总结和补充资料 235
附录A 本书的网页和代码下载 237
附录B 入门流程图和故障排除指南 239
入门流程图 239
常见的Hadoop 故障排除指南 239
规则1：不要惊慌 239
规则2：安装并使用Ambari 244
规则3：检查日志 244
规则4：简化情况 245
规则5：在互联网上提问 245
其他有用的提示 246
附录C 按主题列出的Apache Hadoop 资源汇总 253
常规的Hadoop 信息 253
Hadoop 安装攻略 253
HDFS 254
示例 255
MapReduce. 255
MapReduce 编程 255
基本工具 256
YARN 应用程序框架 257
Ambari 管理 257
基本的Hadoop 管理 257
附录D 安装Hue Hadoop GUI 259
Hue 安装 259
安装和配置Hue 262
启动Hue 263
Hue 用户界面 263
附录E 安装Apache Spark 267
在集群上安装Spark. 267
在整个集群中启动Spark. 268
在伪分布式的单节点安装版本中安装和启动Spark 270
运行Spark 示例 271
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>写给大忙人的Hadoop 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实战Hadoop : 开启通向云计算的捷径
《实战hadoop》
第1 章 神奇的大象——hadoop
1.1 初识神象 2
1.2 hadoop 初体验 4
1.2.1 了解hadoop 的构架 4
1.2.2 查看hadoop 活动 7
1.3 hadoop 族群 10
1.4 hadoop 安装 11
1.4.1 在linux 系统中安装hadoop 11
1.4.2 在windows 系统中安装hadoop 21
1.4.3 站在象背上说“hello” 29
1.4.4 eclipse 下的hadoop 应用开发 30
参考文献 34
第2 章 hdfs——不怕故障的海量存储
2.1 开源的gfs——hdfs 36
2.1.1 设计前提与目标 36
2.1.2 hdfs 体系结构 37
2.1.3 保障hdfs 可靠性措施 39
2.2 hdfs 常用操作 42
2.2.1 hdfs 下的文件操作 42
.2.2.2 管理与更新 45
2.3 hdfs api 之旅 48
2.4 实战：用hdfs 存储海量视频数据 55
2.4.1 应用场景 55
2.4.2 设计实现 55
参考文献 58
第3 章 分久必合——mapreduce
3.1 mapreduce 基础 60
3.1.1 mapreduce 编程模型 60
3.1.2 mapreduce 的集群行为 62
3.2 样例分析：单词计数 64
3.2.1 wordcount 源码分析 64
3.2.2 wordcount 处理过程 67
3.3 mapreduce，你够了解吗 69
3.3.1 没有map、reduce 的mapreduce 69
3.3.2 多少个reducers 最佳 72
3.4 实战：倒排索引 74
3.4.1 倒排索引简介 74
3.4.2 分析与设计 76
3.4.3 倒排索引完整源码 79
参考文献 83
第4 章 一张无限大的表——hbase
4.1 hbase 简介 85
4.1.1 逻辑模型 85
4.1.2 物理模型 86
4.1.3 region 服务器 87
4.1.4 主服务器 89
4.1.5 元数据表 89
4.2 hbase 入门 91
4.2.1 hbase 的安装配置 91
4.2.2 hbase 用户界面 97
实战hadoop —— 开启通向云计算的捷径
4.3 hbase 操作演练 100
4.3.1 基本shell 操作 100
4.3.2 基本api 使用 103
4.4 实战：使用mapreduce 构建hbase 索引 105
4.4.1 索引表蓝图 105
4.4.2 hbase 和mapreduce 107
4.4.3 实现索引 108
参考文献 112
第5 章 更上一层楼——mapreduce 进阶
5.1 简介 114
5.2 复合键值对的使用 115
5.2.1 把小的键值对合并成大的键值对 115
5.2.2 巧用复合键让系统完成排序 117
5.3 用户定制数据类型 123
5.3.1 hadoop 内置的数据类型 123
5.3.2 用户自定义数据类型的实现 124
5.4 用户定制输入/输出格式 126
5.4.1 hadoop 内置的数据输入格式和recordreader 126
5.4.2 用户定制数据输入格式与recordreader 127
5.4.3 hadoop 内置的数据输出格式与recordwriter 133
5.4.4 用户定制数据输出格式与recordwriter 134
5.4.5 通过定制数据输出格式实现多集合文件输出 134
5.5 用户定制partitioner 和combiner 137
5.5.1 用户定制partitioner 137
5.5.2 用户定制combiner 139
5.6 组合式mapreduce 计算作业 141
5.6.1 迭代mapreduce 计算任务 141
5.6.2 顺序组合式mapreduce 作业的执行 142
5.6.3 具有复杂依赖关系的组合式mapreduce 作业的执行 144
5.6.4 mapreduce 前处理和后处理步骤的链式执行 145
5.7 多数据源的连接 148
5.7.1 基本问题数据示例 149
5.7.2 用datajoin 类实现reduce 端连接 150
5.7.3 用全局文件复制方法实现map 端连接 158
5.7.4 带map 端过滤的reduce 端连接 162
5.7.5 多数据源连接解决方法的限制 162
5.8 全局参数/数据文件的传递与使用 163
5.8.1 全局作业参数的传递 163
5.8.2 查询全局mapreduce 作业属性 166
5.8.3 全局数据文件的传递 167
5.9 关系数据库的连接与访问 169
5.9.1 从数据库中输入数据 169
5.9.2 向数据库中输出计算结果 170
参考文献 172
第6 章 hive——飞进数据仓库的小蜜蜂
6.1 hive 的组成 174
6.2 搭建蜂房——hive 安装 176
6.3 hive 的服务 182
6.3.1 hive shell 182
6.3.2 jdbc/odbc 支持 183
6.3.3 thrift 服务 184
6.3.4 web 接口 185
6.3.5 元数据服务 186
6.4 hiveql 的使用 187
6.4.1 hiveql 的数据类型 187
6.4.2 hiveql 常用操作 188
6.5 hive 示例 196
6.5.1 udf 编程示例 196
实战hadoop —— 开启通向云计算的捷径
6.5.2 udaf 编程示例 198
6.6 实战：基于hive 的hadoop 日志分析 200
参考文献 209
第7 章 pig——一头什么都能吃的猪
7.1 pig 的基本框架 211
7.2 pig 的安装 212
7.2.1 开始安装pig 212
7.2.2 验证安装 213
7.3 pig 的使用 214
7.3.1 pig 的mapreduce 模式 214
7.3.2 使用pig 216
7.3.3 pig 的调试 219
7.4 pig latin 编程语言 224
7.4.1 数据模型 224
7.4.2 数据类型 225
7.4.3 运算符 226
7.4.4 常用操作 228
7.4.5 用户自定义函数 231
7.5 实战：基于pig 的通话记录查询 231
7.5.1 应用场景 231
7.5.2 设计实现 232
参考文献 238
第8 章 facebook 的女神——cassandra
8.1 洞察cassandra 的全貌 240
8.1.1 目标及特点 240
8.1.2 体系结构 241
8.1.3 存储机制 243
8.1.4 数据操作过程 244
8.2 让cassandra 飞 247
8.2.1 windows 7 下单机安装 247
8.2.2 linux 下分布式安装 249
8.3 cassandra 操作示例 253
8.3.1 客户端命令代码跟踪 253
8.3.2 增删cassandra 节点 262
8.3.3 jconsole 监控cassandra 263
8.4 cassandra 与mapreduce 结合 266
8.4.1 需求分析 266
8.4.2 编码流程分析 267
8.4.3 mapreduce 的核心代码 268
参考文献 269
第9 章 chukwa——收集数据的大乌龟
9.1 初识chukwa 271
9.1.1 为什么需要chukwa 271
9.1.2 什么是chukwa 272
9.2 chukwa 架构与设计 274
9.2.1 代理与适配器 276
9.2.2 元数据 277
9.2.3 收集器 278
9.2.4 mapreduce 作业 279
9.2.5 hicc 280
9.2.6 数据接口与支持 280
9.3 chukwa 安装与配置 281
9.3.1 chukwa 安装 281
9.3.2 源节点代理配置 284
9.3.3 收集器 288
9.3.4 demux 作业与hicc 配置 289
9.4 chukwa 小试 291
实战hadoop —— 开启通向云计算的捷径
9.4.1 数据生成 291
9.4.2 数据收集 292
9.4.3 数据处理 292
9.4.4 数据析取 293
9.4.5 数据稀释 294
9.4.6 数据显示 294
参考文献 295
第10 章 一统天下——zookeeper
10.1 zookeeper 是个谜 297
10.1.1 zookeeper 工作原理 298
10.1.2 zookeeper 的特性 301
10.2 zookeeper 安装和编程 303
10.2.1 zookeeper 的安装和配置 303
10.2.2 zookeeper 的编程实现 306
10.3 zookeeper 演练：进程调度系统 308
10.3.1 设计方案 308
10.3.2 设计实现 309
10.4 实战演练：zookeeper 实现namenode 自动切换 318
10.4.1 设计思想 319
10.4.2 详细设计 319
10.4.3 编码 321
10.4.4 实战总结 329
参考文献 329
第11 章 综合实战1——打造一个搜索引擎
11.1 系统工作原理 331
11.2 网页搜集与信息提取 333
11.2.1 网页搜集 334
11.2.2 网页信息的提取与存储 337
11.3 基于mapreduce 的预处理 338
11.3.1 元数据过滤 339
11.3.2 生成倒排文件 341
11.3.3 建立二级索引 353
11.3.4 小节 357
11.4 建立web 信息查询服务 358
11.4.1 建立前台查询接口 358
11.4.2 后台信息查询与合并 359
11.4.3 返回显示结果 360
11.5 系统优化 361
11.5.1 存储方面的优化 361
11.5.2 计算方面的优化 362
11.6 本章总结 363
参考文献 364
第12 章 综合实战2——生物信息学应用
12.1 背景 366
12.2 总体框架 368
12.3 系统实现 370
12.3.1 序列数据库的切分和存储 370
12.3.2 构造单词列表和扫描器 375
12.3.3 map：扫描和扩展 376
12.3.4 主控程序 378
12.4 扩展性能测试 381
12.5 本章总结 382
参考文献 383
第13 章 综合实战3——移动通信信令监测与查询
13.1 分析与设计 385
13.1.1 cdr 数据文件的检测与索引创建任务调度 388
13.1.2 从hdfs 读取数据并创建索引 389
实战hadoop —— 开启通向云计算的捷径
13.1.3 查询cdr 信息 390
13.2 实现代码 391
13.2.1 cdr 文件检测和索引创建任务调度程序 392
13.2.2 读取cdr 数据和索引创建处理 397
13.2.3 cdr 查询 402
13.3 本章总结 407
参考文献 407
第14 章 高枕无忧——hadoop 容错
14.1 hadoop 的可靠性 409
14.1.1 hdfs 中namenode 单点问题 409
14.1.2 hdfs 数据块副本机制 410
14.1.3 hdfs 心跳机制 411
14.1.4 hdfs 负载均衡 412
14.1.5 mapreduce 容错 413
14.2 hadoop 的secondarynamenode 机制 414
14.2.1 磁盘镜像与日志文件 414
14.2.2 secondarynamenode 更新镜像的流程 414
14.3 avatar 机制 418
14.3.1 系统架构 419
14.3.2 avatar 元数据同步机制 420
14.3.3 故障切换过程 423
14.3.4 avatar 运行流程 426
14.3.5 avatar 故障切换流程 431
14.4 avatar 实战 436
14.4.1 实验环境 436
14.4.2 编译avatar 437
14.4.3 avatar 安装和配置 440
14.4.4 avatar 启动运行与宕机切换 452
参考文献 456
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实战Hadoop : 开启通向云计算的捷径
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop数据分析
前言　　ix
第一部分　分布式计算入门
第1章　数据产品时代　　2
1.1　什么是数据产品　　2
1.2　使用Hadoop构建大规模数据产品　　4
1.2.1　利用大型数据集　　4
1.2.2　数据产品中的Hadoop　　5
1.3　数据科学流水线和Hadoop生态系统　　6
1.4　小结　　8
第2章　大数据操作系统　　9
2.1　基本概念　　10
2.2　Hadoop架构　　11
2.2.1　Hadoop集群　　12
2.2.2　HDFS　　14
2.2.3　YARN　　15
2.3　使用分布式文件系统　　16
2.3.1　基本的文件系统操作　　16
2.3.2　HDFS文件权限　　18
2.3.3　其他HDFS接口　　19
2.4　使用分布式计算　　20
2.4.1　MapReduce：函数式编程模型　　20
2.4.2　MapReduce：集群上的实现　　22
2.4.3　不止一个MapReduce：作业链　　27
2.5　向YARN 提交MapReduce 作业　　28
2.6　小结　　30
第3章　Python 框架和Hadoop Streaming　　31
3.1　Hadoop Streaming　　32
3.1.1　使用Streaming在CSV 数据上运行计算　　34
3.1.2　执行Streaming作业　　38
3.2　Python 的MapReduce框架　　39
3.2.1　短语计数　　42
3.2.2　其他框架　　45
3.3　MapReduce进阶　　46
3.3.1　combiner　　46
3.3.2　partitioner　　47
3.3.3　作业链　　47
3.4　小结　　50
第4章　Spark内存计算　　52
4.1　Spark基础　　53
4.1.1　Spark栈　　54
4.1.2　RDD　　55
4.1.3　使用RDD 编程　　56
4.2　基于PySpark的交互性Spark　　59
4.3　编写Spark应用程序　　61
4.4　小结　　67
第5章　分布式分析和模式　　69
5.1　键计算　　70
5.1.1　复合键　　71
5.1.2　键空间模式　　74
5.1.3　pair与stripe　　78
5.2　设计模式　　80
5.2.1　概要　　81
5.2.2　索引　　85
5.2.3　过滤　　90
5.3　迈向最后一英里分析　　95
5.3.1　模型拟合　　96
5.3.2　模型验证　　97
5.4　小结　　98
第二部分　大数据科学的工作流和工具
第6章　数据挖掘和数据仓　　102
6.1　Hive 结构化数据查询　　103
6.1.1　Hive 命令行接口（CLI）　　103
6.1.2　Hive 查询语言　　104
6.1.3　Hive 数据分析　　108
6.2　HBase　　113
6.2.1　NoSQL 与列式数据库　　114
6.2.2　HBase 实时分析　　116
6.3　小结　　122
第7章　数据采集　　123
7.1　使用Sqoop 导入关系数据　　124
7.1.1　从MySQL 导入HDFS　　124
7.1.2　从MySQL 导入Hive　　126
7.1.3　从MySQL 导入HBase　　128
7.2　使用Flume 获取流式数据　　130
7.2.1　Flume 数据流　　130
7.2.2　使用Flume 获取产品印象数据　　133
7.3　小结　　136
第8章　使用高级API 进行分析　　137
8.1　Pig　　137
8.1.1　Pig Latin　　138
8.1.2　数据类型　　142
8.1.3　关系运算符　　142
8.1.4　用户定义函数　　143
8.1.5　Pig 小结　　144
8.2　Spark 高级API　　144
8.2.1　Spark SQL　　146
8.2.2　DataFrame　　148
8.3　小结　　153
第9章　机器学习　　154
9.1　使用Spark 进行可扩展的机器学习　　154
9.1.1　协同过滤　　156
9.1.2　分类　　161
9.1.3　聚类　　163
9.2　小结　　166
第10章　总结：分布式数据科学实战　　167
10.1　数据产品生命周期　　168
10.1.1　数据湖泊　　169
10.1.2　数据采集　　171
10.1.3　计算数据存储　　172
10.2　机器学习生命周期　　173
10.3　小结　　175
附录A　创建Hadoop 伪分布式开发环境　　176
附录B　安装Hadoop 生态系统产品　　184
术语表　　193
关于作者　　211
关于封面　　211
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>NoSQL Hadoop组件及大数据实施
模块1　额外的Hadoop工具：ZooKeeper、Sqoop、Flume、YARN和Storm
第1讲　用ZooKeeper进行分布式处理协调　3
1．1　ZooKeeper简介　4
1．1．1　ZooKeeper的好处　4
1．1．2　ZooKeeper术语　6
1．1．3　ZooKeeper命令行界面（CLI）　7
1．2　安装和运行ZooKeeper　9
1．2．1　支持的平台　9
1．2．2　所需的软件　9
1．2．3　单服务器的安装　9
1．3　使用ZooKeeper　10
1．4　ZooKeeper应用程序　12
1．4．1　FS爬取　13
1．4．2　Katta　14
1．4．3　Yahoo!消息代理（YMB）　14
1．5　使用ZooKeeper构建应用程序　15
1．5．1　Exec．java　15
1．5．2　处理事件　17
1．5．3　监控数据　19
1．5．4　实现屏障和生产者-消费者队列　22
练习　30
备忘单　33
第2讲　利用Sqoop有效地传输批量数据　34
2．1　Sqoop简介　35
2．1．1　Sqoop中的工作流　36
2．1．2　Sqoop的特性　36
2．2　使用Sqoop 1　37
2．3　用Sqoop导入数据　41
2．3．1　导入完整的表　41
2．3．2　用HBase Sqoop导入带有复合键的表　42
2．3．3　指定目标目录　43
2．3．4　导入选择的行　43
2．3．5　密码保护　44
2．3．6　用不同的文件格式导入数据　44
2．3．7　导入数据压缩　45
2．4　控制并行　45
2．5　编码NULL值　47
2．6　将数据导入Hive表　47
2．7　将数据导入HBase　47
2．7．1　使用自由形式查询　48
2．7．2　重命名Sqoop作业　48
2．8　导出数据　49
2．8．1　批量导出　50
2．8．2　原子导出　50
2．9　将数据导出至列的子集　50
2．10　Sqoop中的驱动程序和连接器　51
2．10．1　驱动程序　51
2．10．2　连接器　52
2．10．3　连接到数据库　52
2．11　Sqoop架构概览　54
2．12　Sqoop 2　55
2．12．1　Sqoop 2的优势　56
2．12．2　易于扩展　56
2．12．3　安全　57
练习　58
备忘单　60
第3讲　Flume　62
3．1　Flume简介　63
3．1．1　Flume架构　64
3．1．2　流可靠性　66
3．2　Flume配置文件　66
3．2．1　流定义　67
3．2．2　配置单个组件　67
3．2．3　在代理中添加多个流　68
3．2．4　配置多代理流　69
3．2．5　配置流扇出　70
3．3　设置Flume　71
3．3．1　安装Flume　71
3．3．2　配置Flume代理　72
3．3．3　数据消费　74
3．4　构建Flume　77
3．4．1　获得源点　77
3．4．2　编译/测试Flume　77
3．4．3　开发自定义组件　77
练习　90
备忘单　92
第4讲　超越MapReduce—YARN　94
4．1　YARN简介　95
4．2　为什么用YARN　96
4．2．1　提高可扩展性　96
4．2．2　效率　97
4．2．3　集群共享　97
4．3　YARN生态系统　98
4．3．1　YARN架构　99
4．3．2　资源　100
4．3．3　资源管理器　101
4．3．4　ApplicationMaster　103
4．3．5　YARN的局限性　106
4．4　YARN API例子　107
4．4．1　YARN应用程序剖析　107
4．4．2　客户端　108
4．4．3　把它们整合到一起　115
4．5　Mesos和YARN的比较　116
4．5．1　Mesos简介　116
4．5．2　Mesos和Hadoop　118
练习　120
备忘单　122
第5讲　Storm on YARN　124
5．1　Storm和Hadoop　125
5．2　Storm简介　126
5．2．1　Storm架构　126
5．2．2　Storm应用剖析　129
5．3　Storm API　132
5．3．1　spout　132
5．3．2　bolt　134
5．4　Storm on YARN　134
5．4．1　Storm on YARN架构　135
5．4．2　Storm on YARN的局限性　136
5．5　安装Storm on YARN　136
5．5．1　先决条件　136
5．5．2　安装步骤　137
5．5．3　排错　138
5．5．4　管理YARN on Storm　138
5．6　Storm on YARN的例子　139
5．6．1　传感器数据spout　139
5．6．2　仪表盘bolt　140
5．6．3　HDFS日志记录器bolt　142
5．6．4　主程序　144
5．6．5　运行示例　146
练习　148
备忘单　151
模块2　利用NoSQL和Hadoop：实时、安全和云
第1讲　Hello NoSQL　155
1．1　看两个简单的例子　156
1．1．1　持久化偏好数据的一个简单集合——MongoDB　156
1．1．2　存储汽车品牌和型号数据——Apache Cassandra　162
1．2　利用语言绑定进行工作　171
1．2．1　MongoDB的驱动程序　171
1．2．2　初识Thrift　174
1．3　存储和访问数据　177
1．4　在MongoDB中存储和访问数据　178
1．5　在HBase中存储和访问数据　185
1．6　在Apache Cassandra中存储和访问数据　189
1．7　NoSQL数据存储的语言绑定　191
1．7．1　用Thrift进行诊断　191
1．7．2　Java的语言绑定　191
1．7．3　PHP的语言绑定　194
练习　195
备忘单　198
第2讲　使用NoSQL　199
2．1　创建记录　200
2．2　访问数据　213
2．2．1　访问来自MongoDB的文档　213
2．2．2　访问来自HBase的数据　214
2．2．3　查询Redis　215
2．3　更新和删除数据　216
2．4　MongoDB查询语言的能力　217
2．4．1　加载MovieLens数据　219
2．4．2　获取评级数据　221
2．4．3　MongoDB中的MapReduce　224
2．5　访问来自HBase这样的面向列的数据库的数据　228
练习　230
备忘单　234
第3讲　Hadoop安全　236
3．1　Hadoop安全挑战　238
3．2　认证　239
3．2．1　Kerberos认证　239
3．2．2　Kerberos RPC　244
3．2．3　基于Web的控制台的Kerberos　245
3．3　委托安全凭证　248
3．4　授权　253
3．4．1　HDFS文件权限　253
3．4．2　服务级别授权　257
3．4．3　作业授权　260
练习　261
备忘单　263
第4讲　在AWS上运行Hadoop应用程序　265
4．1　开始了解AWS　266
4．2　在AWS上运行Hadoop的选项　267
4．2．1　使用EC2实例的自定义安装　267
4．2．2　弹性MapReduce　268
4．3　了解EMR-Hadoop的关系　269
4．3．1　EMR架构　270
4．3．2　使用S3存储　271
4．3．3　最大化地利用EMR　272
4．3．4　使用CloudWatch和其他AWS组件　274
4．3．5　访问和使用EMR　274
4．4　使用AWS S3　280
4．4．1　了解桶的用法　280
4．4．2　利用控制台的内容浏览　282
4．4．3　编程访问S3中的文件　283
4．4．4　使用MapReduce上传多个文件至S3　294
4．5　自动化EMR作业流的创建和作业执行　296
4．6　组织协调EMR中作业的执行　301
4．6．1　使用EMR集群上的Oozie　301
4．6．2　AWS简单工作流　303
4．6．3　AWS数据管道　304
练习　306
备忘单　309
第5讲　实时Hadoop　311
5．1　实时Hadoop应用　312
5．2　使用HBase实现实时应用　313
5．2．1　将HBase用作照片管理系统　315
5．2．2　将HBase用作Lucene的后端　322
5．3　使用专门的实时Hadoop查询系统　342
5．3．1　Apache Drill　344
5．3．2　Impala　345
5．3．3　将实时查询系统与MapReduce比较　347
5．4　使用基于Hadoop的事件处理系统　347
5．4．1　HFlame　348
5．4．2　Storm　350
5．4．3　将事件处理与MapReduce作比较　352
练习　353
备忘单　356
模块3　Hadoop商业发行版和管理工具
第1讲　大数据简介　359
1．1　Cloudera基础　360
1．1．1　包含Apache Hadoop的Cloudera发行版　360
1．1．2　Cloudera管理器　361
1．1．3　Cloudera标准版　362
1．1．4　Cloudera企业版　363
1．2　Cloudera管理器简介　365
1．3　Cloudera管理器的管理控制台　367
1．3．1　启动并登录管理控制台　370
1．3．2　主页　370
1．4　添加和管理服务　371
1．4．1　添加新服务　371
1．4．2　启动服务　372
1．4．3　停止服务　372
1．4．4　重启服务　373
1．5　使用Cloudera管理器的业务案例　373
1．6　Cloudera管理器的安装要求　374
练习　375
备忘单　377
第2讲　Cloudera上的Hive和Cloudera管理　379
2．1　Apache Hive简介　380
2．1．1　Hive特性　380
2．1．2　HiveQL　380
2．2　Hive服务　381
2．2．1　Hive元数据服务器　382
2．2．2　Hive网关　382
2．2．3　升级Cloudera管理器　382
2．3　为Hive元存储配置模式　383
2．3．1　嵌入模式　383
2．3．2　本地模式　384
2．3．3　远程模式　385
2．4　配置Hive元存储　386
2．4．1　Red Hat操作系统　386
2．4．2　SLES操作系统　388
2．4．3　Debian/Ubuntu操作系统　388
2．5　为Hive设置Cloudera Manager 4．5　389
2．6　Hive复制　391
练习　394
备忘单　396
第3讲　Hortonworks和Greenplum Pivotal HD　397
3．1　Hortonworks数据平台　398
3．1．1　核心服务　400
3．1．2　数据服务　400
3．1．3　操作服务　401
3．2　系统需求和环境　402
3．2．1　系统需求　402
3．2．2　构建一个受支持的环境　404
3．3　安装HDP　405
3．4　使用Talend Open Studio　409
3．4．1　安装Talend Open Studio　410
3．4．2　将数据导入Talend Open Studio　411
3．4．3　执行数据分析　413
3．5　Greenplum Pivotal HD　417
练习　420
备忘单　422
第4讲　IBM InfoSphere BigInsights和MapR　424
4．1　InfoSphere BigInsights简介　425
4．1．1　Apache Hadoop发行版的InfoSphere BigInsights组件　426
4．1．2　额外的Hadoop技术　427
4．1．3　文本分析　428
4．1．4　IBM Big SQL服务器　428
4．1．5　InfoSphere BigInsights控制台　428
4．1．6　InfoSphere BigInsights的Eclipse工具　429
4．2　安装准备　430
4．2．1　复核系统需求　431
4．2．2　选择一个用户　431
4．2．3　配置浏览器　432
4．2．4　下载InfoSphere BigInsights　437
4．2．5　完成常见先决条件的任务　437
4．3　安装InfoSphere BigInsights　440
4．4　MapR简介　442
练习　445
备忘单　447
第5讲　应聘准备　449
5．1　大数据开发者需要的关键技术工具和框架　451
5．2　大数据开发者的工作角色和职责　452
5．3　大数据开发者职业机会领域　453
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>NoSQL Hadoop组件及大数据实施
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据实战权威指南
第一篇 大数据的基本概念和技术
第1章 绪论 3
1.1 时代背景 3
1.1.1 全球大数据浪潮 3
1.1.2 我国的大数据国家战略 5
1.2 大数据的概念 7
1.2.1 概念 7
1.2.2 特征 8
1.3 技术支撑体系 9
1.3.1 概览 9
1.3.2 大数据采集层 9
1.3.3 大数据存储层 10
1.3.4 大数据分析（处理与服务）层 11
1.3.5 大数据应用层 11
1.3.6 垂直视图 13
1.4 大数据人才及其能力要求 14
1.4.1 首席数据官 14
1.4.2 数据科学家（数据分析师） 15
1.4.3 大数据开发工程师 16
1.4.4 大数据运维工程师 17
1.5 本章小结 17
第2章 Hadoop大数据关键技术 19
2.1 Hadoop生态系统 19
2.1.1 架构的基本理论 19
2.1.2 主要组件及其关系 21
2.2 数据采集 24
2.2.1 结构化数据采集工具 24
2.2.2 日志文件采集工具与技术 25
2.3 大数据存储技术 29
2.3.1 相关概念 29
2.3.2 分布式文件存储系统 34
2.3.3 数据库与数据仓库 38
2.4 分布式计算框架 43
2.4.1 离线计算框架 43
2.4.2 实时流计算平台 50
2.5 数据分析平台与工具 57
2.5.1 面向大数据的数据挖掘与分析工具 57
2.5.2 机器学习 61
2.6 本章小结 66
第二篇 Hadoop大数据平台搭建与基本应用
第3章 Linux操作系统与集群搭建 69
3.1 Linux操作系统 69
3.1.1 概述 69
3.1.2 特点 70
3.1.3 Linux的组成 72
3.2 Linux安装与集群搭建 75
3.2.1 安装VMware Workstation 75
3.2.2 在VMware上安装Linux（CentOS7） 79
3.3 集群的配置 91
3.3.1 设置主机名 91
3.3.2 网络设置 93
3.3.3 关闭防火墙 98
3.3.4 安装JDK 99
3.3.5 免密钥登录配置 102
3.4 Linux基本命令 105
3.5 本章小结 112
第4章 HDFS安装与基本应用 113
4.1 HDFS概述 113
4.1.1 特点 113
4.1.2 主要组件与架构 114
4.2 HDFS架构分析 114
4.2.1 数据块 114
4.2.2 NameNode 115
4.2.3 DataNode 116
4.2.4 SecondaryNameNode 117
4.2.5 数据备份 117
4.2.6 通信协议 118
4.2.7 可靠性保证 118
4.3 文件操作过程分析 119
4.3.1 读文件 119
4.3.2 写文件 120
4.3.3 删除文件 122
4.4 Hadoop HDFS安装与配置 122
4.4.1 解压Hadoop安装包 122
4.4.2 配置Hadoop环境变量 123
4.4.3 配置Yarn环境变量 124
4.4.4 配置核心组件文件 125
4.4.5 配置文件系统 125
4.4.6 配置yarn-site.xml文件 126
4.4.7 配置MapReduce计算框架文件 128
4.4.8 配置Master的slaves文件 129
4.4.9 复制Master上的Hadoop到Slave节点 129
4.5 Hadoop集群的启动 130
4.5.1 配置操作系统环境变量 130
4.5.2 创建Hadoop数据目录 131
4.5.3 格式化文件系统 132
4.5.4 启动和关闭Hadoop 133
4.5.5 验证Hadoop是否启动成功 133
4.6 Hadoop集群的基本应用 136
4.6.1 HDFS基本命令 136
4.6.2 在Hadoop集群中运行程序 139
4.7 本章小结 141
第5章 MapReduce与Yarn 143
5.1 MapReduce程序的概念 143
5.1.1 基本编程模型 143
5.1.2 计算过程分析 144
5.2 深入理解Yarn 147
5.2.1 Yarn的基本架构 147
5.2.2 Yarn的工作流程 151
5.3 在Linux平台安装Eclipse 152
5.3.1 Eclipse简介 153
5.3.2 安装并启动Eclipse 154
5.4 开发MapReduce程序的基本方法 155
5.4.1 为Eclipse安装Hadoop插件 156
5.4.2 WordCount：第一个MapReduce程序 160
5.5 本章小结 175
第6章 Hive和HBase的安装与应用 177
6.1 在CentOS7下安装MySQL 177
6.1.1 下载或复制MySQL安装包 177
6.1.2 执行安装命令 178
6.1.3 启动MySQL 179
6.1.4 登录MySQL 179
6.1.5 使用MySQL 181
6.1.6 问题与解决办法 182
6.2 Hive安装与应用 183
6.2.1 下载并解压Hive安装包 183
6.2.2 配置Hive 184
6.2.3 启动并验证Hive 187
6.2.4 Hive的基本应用 189
6.3 ZooKeeper集群安装 190
6.3.1 ZooKeeper简介 190
6.3.2 安装ZooKeeper 191
6.3.3 配置ZooKeeper 191
6.3.4 启动和测试 193
6.4 HBase的安装与应用 195
6.4.1 解压并安装HBase 195
6.4.2 配置HBase 196
6.4.3 启动并验证HBase 199
6.4.4 HBase的基本应用 200
6.4.5 应用HBase中常见问题及其解决办法 203
6.5 本章小结 204
第7章 Sqoop和Kafka的安装与应用 205
7.1 安装部署Sqoop 205
7.1.1 下载或复制Sqoop安装包 205
7.1.2 解压并安装Sqoop 206
7.1.3 配置Sqoop 206
7.1.4 启动并验证Sqoop 208
7.1.5 测试Sqoop与MySQL的连接 209
7.2 安装部署Kafka集群 211
7.2.1 下载或复制Kafka安装包 211
7.2.2 解压缩Kafka安装包 211
7.2.3 配置Kafka集群 211
7.2.4 Kafka的初步应用 213
7.3 本章小结 218
第8章 Spark集群安装与开发环境配置 219
8.1 深入理解Spark 219
8.1.1 Spark系统架构 219
8.1.2 关键概念 221
8.2 安装与配置Scala 224
8.2.1 下载Scala安装包 225
8.2.2 安装Scala 225
8.2.3 启动并应用Scala 226
8.3 Spark集群的安装与配置 226
8.3.1 安装模式 226
8.3.2 Spark的安装 227
8.3.3 启动并验证Spark 230
8.3.4 几点说明 234
8.4 开发环境安装与配置 236
8.4.1 IDEA简介 236
8.4.2 IDEA的安装 236
8.4.3 IDEA的配置 238
8.5 本章小结 243
第9章 Spark应用基础 245
9.1 Spark程序的运行模式 245
9.1.1 Spark on Yarn-cluster 245
9.1.2 Spark on Yarn-client 246
9.2 Spark应用设计 247
9.2.1 分布式估算圆周率 248
9.2.2 基于Spark MLlib的贷款风险预测 265
9.3 本章小结 285
第三篇 数据处理与项目开发术
第10章 交互式数据处理 289
10.1 数据预处理 289
10.1.1 查看数据 289
10.1.2 数据扩展 291
10.1.3 数据过滤 292
10.1.4 数据上传 293
10.2 创建数据仓库 294
10.2.1 创建Hive数据仓库的基本命令 294
10.2.2 创建Hive分区表 296
10.3 数据分析 299
10.3.1 基本统计 299
10.3.2 用户行为分析 301
10.3.3 实时数据 303
10.4 本章小结 304
第11章 协同过滤推荐系统 305
11.1 推荐算法概述 305
11.1.1 基于人口统计学的推荐 305
11.1.2 基于内容的推荐 306
11.1.3 协同过滤推荐 307
11.2 协同过滤推荐算法分析 308
11.2.1 基于用户的协同过滤推荐 308
11.2.2 基于物品的协同过滤推荐 310
11.3 Spark MLlib推荐算法应用 312
11.3.1 ALS算法原理 312
11.3.2 ALS的应用设计 315
11.4 本章小结 329
第12章 销售数据分析系统 331
12.1 数据采集 331
12.1.1 在Windows下安装JDK 331
12.1.2 在Windows下安装Eclipse 334
12.1.3 将WebCollector项目导入Eclipse 335
12.1.4 在Windows下安装MySQL 336
12.1.5 连接JDBC 339
12.1.6 运行爬虫程序 340
12.2 在HBase集群上准备数据 342
12.2.1 将数据导入到MySQL 342
12.2.2 将MySQL表中的数据导入到HBase表中 344
12.3 安装Phoenix中间件 347
12.3.1 Phoenix架构 347
12.3.2 解压安装Phoenix 348
12.3.3 Phoenix环境配置 349
12.3.4 使用Phoenix 350
12.4 基于Web的前端开发 353
12.4.1 将Web前端项目导入Eclipse 353
12.4.2 安装Tomcat 355
12.4.3 在Eclipse中配置Tomcat 355
12.4.4 在Web浏览器中查看执行结果 359
12.5 本章小结 361
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop大数据实战权威指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop高级编程——构建与实现大数据解决方案 : 构建与实现大数据解决方案
目 录
第1章 大数据和Hadoop生态系统 1
1.1 当大数据遇见Hadoop 2
1.1.1 Hadoop：直面大数据的挑战 3
1.1.2 商业世界中的数据科学 4
1.2 Hadoop生态系统 6
1.3 Hadoop核心组件 7
1.4 Hadoop发行版 9
1.5 使用Hadoop开发企业级应用 10
1.6 小结 14
第2章 Hadoop数据存储 15
2.1 HDFS 15
2.1.1 HDFS架构 15
2.1.2 使用HDFS文件 19
2.1.3 Hadoop特定的文件类型 21
2.1.4 HDFS联盟和高可用性 26
2.2 HBase 28
2.2.1 HBase架构 28
2.2.2 HBase结构设计 34
2.2.3 HBase编程 35
2.2.4 HBase新特性 42
2.3 将HDFS和HBase的组合用于高效数据存储 45
2.4 使用Apache Avro 45
2.5 利用HCatalog管理元数据 49
2.6 为应用程序选择合适的Hadoop数据组织形式 51
2.7 小结 53
第3章 使用MapReduce处理数据 55
3.1 了解MapReduce 55
3.1.1 MapReduce执行管道 56
3.1.2 MapReduce中的运行时协调和任务管理 59
3.2 第一个MapReduce应用程序 61
3.3 设计MapReduce实现 69
3.3.1 将MapReduce用作并行处理框架 70
3.3.2 使用MapReduce进行简单的数据处理 71
3.3.3 使用MapReduce构建连接 72
3.3.4 构建迭代式MapReduce应用程序 77
3.3.5 是否使用MapReduce 82
3.3.6 常见的MapReduce设计陷阱 83
3.4 小结 84
第4章 自定义MapReduce执行 85
4.1 使用InputFormat控制MapReduce执行 85
4.1.1 为计算密集型应用程序实现InputFormat 87
4.1.2 实现InputFormat以控制Map的数量 93
4.1.3 实现用于多个HBase表的InputFormat 99
4.2 使用自定义RecordReader以自己的方式读取数据 102
4.2.1 实现基于队列的RecordReader 102
4.2.2 为XML数据实现RecordReader 105
4.3 使用自定义输出格式组织输出数据 109
4.4 使用自定义记录写入器以自己的方式写入数据 119
4.5 使用组合器优化MapReduce执行 121
4.6 使用分区器控制Reducer执行 124
4.7 在Hadoop中使用非Java代码 128
4.7.1 Pipes 128
4.7.2 Hadoop Streaming 128
4.7.3 使用JNI 129
4.8 小结 131
第5章 构建可靠的MapReduce应用程序 133
5.1 单元测试MapReduce应用程序 133
5.1.1 测试Mapper 136
5.1.2 测试Reducer 137
5.1.3 集成测试 138
5.2 使用Eclipse进行本地应用程序测试 139
5.3 将日志用于Hadoop测试 141
5.4 使用作业计数器报告指标 146
5.5 MapReduce中的防御性编程 149
5.6 小结 151
第6章 使用Oozie自动化数据处理 153
6.1 认识Oozie 154
6.2 Oozie Workflow 155
6.2.1 在Oozie Workflow中执行异步操作 159
6.2.2 Oozie的恢复能力 164
6.2.3 Oozie Workflow作业的生命周期 164
6.3 Oozie Coordinator 165
6.4 Oozie Bundle 170
6.5 用表达式语言对Oozie进行参数化 174
6.5.1 Workflow函数 175
6.5.2 Coordinator函数 175
6.5.3 Bundle函数 175
6.5.4 其他EL函数 175
6.6 Oozie作业执行模型 176
6.7 访问Oozie 179
6.8 Oozie SLA 180
6.9 小结 185
第7章 使用Oozie 187
7.1 使用探测包验证位置相关信息的正确性 187
7.2 设计基于探测包的地点正确性验证 188
7.3 设计Oozie Workflow 190
7.4 实现Oozie Workflow应用程序 193
7.4.1 实现数据准备Workflow 193
7.4.2 实现考勤指数和聚类探测包串Workflow 201
7.5 实现 Workflow行为 203
7.5.1 发布来自java动作的执行上下文 204
7.5.2 在Oozie Workflow中使用MapReduce作业 204
7.6 实现Oozie Coordinator应用程序 207
7.7 实现Oozie Bundle应用程序 212
7.8 部署、测试和执行Oozie应用程序 213
7.8.1 部署Oozie应用程序 213
7.8.2 使用Oozie CLI执行Oozie应用程序 215
7.8.3 向Oozie作业传递参数 218
7.9 使用Oozie控制台获取Oozie应用程序信息 221
7.9.1 了解Oozie控制台界面 221
7.9.2 获取 Coordinator作业信息 225
7.10 小结 227
第8章 高级Oozie特性 229
8.1 构建自定义Oozie Workflow动作 230
8.1.1 实现自定义Oozie Workflow动作 230
8.1.2 部署Oozie自定义Workflow动作 235
8.2 向Oozie Workflow添加动态执行 237
8.2.1 总体实现方法 237
8.2.2 一个机器学习模型、参数和算法 240
8.2.3 为迭代过程定义Workflow 241
8.2.4 动态Workflow生成 244
8.3 使用Oozie Java API 247
8.4 在Oozie应用中使用uber jar包 251
8.5 数据吸收传送器 256
8.6 小结 263
第9章 实时Hadoop 265
9.1 现实世界中的实时应用 266
9.2 使用HBase来实现实时应用 266
9.2.1 将HBase用作图片管理系统 268
9.2.2 将HBase用作Lucene后端 275
9.3 使用专门的实时Hadoop查询系统 295
9.3.1 Apache Drill 296
9.3.2 Impala 298
9.3.3 实时查询和MapReduce的对比 299
9.4 使用基于Hadoop的事件处理系统 300
9.4.1 HFlame 301
9.4.2 Storm 302
9.4.3 事件处理和MapReduce的对比 305
9.5 小结 305
第10章 Hadoop安全 307
10.1 简要的历史：理解Hadoop安全的挑战 308
10.2 认证 309
10.2.1 Kerberos认证 310
10.2.2 委派安全凭据 318
10.3 授权 323
10.3.1 HDFS文件访问权限 323
10.3.2 服务级授权 327
10.3.3 作业授权 329
10.4 Oozie认证和授权 329
10.5 网络加密 331
10.6 使用Rhino项目增强安全性 332
10.6.1 HDFS磁盘级加密 333
10.6.2 基于令牌的认证和统一的授权框架 333
10.6.3 HBase单元格级安全 334
10.7 将所有内容整合起来——保证Hadoop安全的最佳实践 334
10.7.1 认证 335
10.7.2 授权 335
10.7.3 网络加密 336
10.7.4 敬请关注Hadoop的增强功能 336
10.8 小结 336
第11章 在AWS上运行Hadoop应用 337
11.1 初识AWS 338
11.2 在AWS上运行Hadoop的可选项 339
11.2.1 使用EC2实例的自定义安装 339
11.2.2 弹性MapReduce 339
11.2.3 做出选择前的额外考虑 339
11.3 理解EMR-Hadoop的关系 340
11.3.1 EMR架构 341
11.3.2 使用S3存储 343
11.3.3 最大化EMR的使用 343
11.3.4 利用CloudWatch和其他AWS组件 345
11.3.5 访问和使用EMR 346
11.4 使用AWS S3 351
11.4.1 理解桶的使用 352
11.4.2 使用控制台浏览内容 354
11.4.3 在S3中编程访问文件 355
11.4.4 使用MapReduce上传多个文件到S3 365
11.5 自动化EMR作业流创建和作业执行 367
11.6 管理EMR中的作业执行 372
11.6.1 在EMR集群上使用Oozie 372
11.6.2 AWS 简单工作流 374
11.6.3 AWS数据管道 375
11.7 小结 376
第12章 为Hadoop实现构建企业级安全解决方案 377
12.1 企业级应用的安全顾虑 378
12.1.1 认证 380
12.1.2 授权 380
12.1.3 保密性 380
12.1.4 完整性 381
12.1.5 审计 381
12.2 Hadoop安全没有为企业级应用原生地提供哪些机制 381
12.2.1 面向数据的访问控制 382
12.2.2 差分隐私 382
12.2.3 加密静止的数据 383
12.2.4 企业级安全集成 384
12.3 保证使用Hadoop的企业级应用安全的方法 384
12.3.1 使用Accumulo进行访问控制保护 385
12.3.2 加密静止数据 394
12.3.3 网络隔离和分隔方案 395
12.4 小结 397
第3章 Hadoop的未来 399
13.1 使用DSL简化MapReduce编程 400
13.1.1 什么是DSL 400
13.1.2 Hadoop的DSL 401
13.2 更快、更可扩展的数据处理 412
13.2.1 Apache YARN 412
13.2.2 Tez 414
13.3 安全性的改进 415
13.4 正在出现的趋势 415
13.5 小结 416
附录 有用的阅读 417
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Hadoop高级编程——构建与实现大数据解决方案 : 构建与实现大数据解决方案
