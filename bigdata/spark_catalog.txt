>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark技术内幕
序
前言
第1章　Spark简介1
1.1Spark的技术背景1
1.2Spark的优点2
1.3Spark架构综述4
1.4Spark核心组件概述5
1.4.1Spark Streaming5
1.4.2MLlib6
1.4.3Spark SQL7
1.4.4　GraphX8
1.5Spark的整体代码结构规模8
第2章　Spark学习环境的搭建9
2.1源码的获取与编译9
2.1.1源码获取9
2.1.2源码编译10
2.2构建Spark的源码阅读环境11
2.3小结15
第3章　RDD实现详解16
3.1概述16
3.2什么是RDD17
3.2.1RDD的创建19
3.2.2RDD的转换20
3.2.3　RDD的动作22
3.2.4RDD的缓存23
3.2.5RDD的检查点24
3.3RDD的转换和DAG的生成25
3.3.1RDD的依赖关系26
3.3.2DAG的生成30
3.3.3Word Count的RDD转换和DAG划分的逻辑视图30
3.4RDD的计算33
3.4.1Task简介33
3.4.2Task的执行起点33
3.4.3缓存的处理35
3.4.4checkpoint的处理37
3.4.5RDD的计算逻辑39
3.5RDD的容错机制39
3.6小结40
第4章　Scheduler 模块详解41
4.1模块概述41
4.1.1整体架构41
4.1.2Scheduler的实现概述43
4.2DAGScheduler实现详解45
4.2.1DAGScheduler的创建46
4.2.2Job的提交48
4.2.3Stage的划分49
4.2.4任务的生成54
4.3任务调度实现详解57
4.3.1TaskScheduler的创建57
4.3.2Task的提交概述58
4.3.3任务调度具体实现61
4.3.4Task运算结果的处理65
4.4Word Count调度计算过程详解72
4.5小结74
第5章　Deploy模块详解76
5.1　Spark运行模式概述76
5.1.1　local77
5.1.2Mesos78
5.1.3YARN82
5.2模块整体架构86
5.3消息传递机制详解87
5.3.1Master和Worker87
5.3.2Master和Client89
5.3.3Client和Executor91
5.4集群的启动92
5.4.1Master的启动92
5.4.2Worker的启动96
5.5集群容错处理98
5.5.1Master 异常退出98
5.5.2Worker异常退出99
5.5.3Executor异常退出101
5.6Master HA实现详解102
5.6.1Master启动的选举和数据恢复策略103
5.6.2集群启动参数的配置105
5.6.3Curator Framework简介 106
5.6.4ZooKeeperLeaderElectionAgent的实现109
5.7小结110
第6章　Executor模块详解112
6.1Standalone模式的Executor分配详解113
6.1.1SchedulerBackend创建AppClient114
6.1.2AppClient向Master注册Application116
6.1.3Master根据AppClient的提交选择Worker119
6.1.4Worker根据Master的资源分配结果创建Executor121
6.2Task的执行122
6.2.1依赖环境的创建和分发123
6.2.2任务执行125
6.2.3任务结果的处理128
6.2.4Driver端的处理130
6.3　参数设置131
6.3.1　spark.executor.memory131
6.3.2日志相关132
6.3.3spark.executor.heartbeatInterval132
6.4小结133
第7章　Shuffle模块详解134
7.1Hash Based Shuffle Write135
7.1.1Basic Shuffle Writer实现解析136
7.1.2存在的问题138
7.1.3Shuffle Consolidate Writer139
7.1.4小结140
7.2Shuffle Pluggable 框架141
7.2.1org.apache.spark.shuffle.ShuffleManager141
7.2.2org.apache.spark.shuffle.ShuffleWriter143
7.2.3org.apache.spark.shuffle.ShuffleBlockManager143
7.2.4org.apache.spark.shuffle.ShuffleReader144
7.2.5如何开发自己的Shuffle机制144
7.3Sort Based Write144
7.4Shuffle Map Task运算结果的处理148
7.4.1Executor端的处理148
7.4.2Driver端的处理150
7.5Shuffle Read152
7.5.1整体流程152
7.5.2数据读取策略的划分155
7.5.3本地读取156
7.5.4远程读取158
7.6性能调优160
7.6.1spark.shuffle.manager160
7.6.2spark.shuffle.spill162
7.6.3spark.shuffle.memoryFraction和spark.shuffle.safetyFraction162
7.6.4spark.shuffle.sort.bypassMergeThreshold 163
7.6.5spark.shuffle.blockTransferService 163
7.6.6spark.shuffle.consolidateFiles 163
7.6.7spark.shuffle.compress和 spark.shuffle.spill.compress164
7.6.8spark.reducer.maxMbInFlight165
7.7小结165
第8章　Storage模块详解167
8.1模块整体架构167
8.1.1整体架构167
8.1.2源码组织结构170
8.1.3Master 和Slave的消息传递详解173
8.2存储实现详解181
8.2.1存储级别181
8.2.2模块类图184
8.2.3org.apache.spark.storage.DiskStore实现详解186
8.2.4org.apache.spark.storage.MemoryStore实现详解188
8.2.5org.apache.spark.storage.TachyonStore实现详解189
8.2.6Block存储的实现190
8.3性能调优194
8.3.1spark.local.dir194
8.3.2spark.executor.memory194
8.3.3spark.storage.memoryFraction194
8.3.4spark.streaming.blockInterval195
8.4小结195
第9章　企业应用概述197
9.1Spark在百度197
9.1.1现状197
9.1.2百度开放云BMR的Spark198
9.1.3在Spark中使用Tachyon199
9.2Spark在阿里200
9.3Spark在腾讯200
9.4小结201
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark技术内幕
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速大数据分析
目录
推荐序  xi
译者序  xiv
序  xvi
前言  xvii
第1章 Spark数据分析导论  1
1.1 Spark是什么  1
1.2 一个大一统的软件栈  2
1.2.1 Spark Core  2
1.2.2 Spark SQL  3
1.2.3 Spark Streaming  3
1.2.4 MLlib  3
1.2.5 GraphX  3
1.2.6 集群管理器  4
1.3 Spark的用户和用途  4
1.3.1 数据科学任务  4
1.3.2 数据处理应用  5
1.4 Spark简史  5
1.5 Spark的版本和发布  6
1.6 Spark的存储层次  6
第2章 Spark下载与入门  7
2.1 下载Spark  7
2.2 Spark中Python和Scala的shell  9
2.3 Spark 核心概念简介  12
2.4 独立应用  14
2.4.1 初始化SparkContext  15
2.4.2 构建独立应用  16
2.5 总结  19
第3章 RDD编程  21
3.1 RDD基础  21
3.2 创建RDD  23
3.3 RDD操作  24
3.3.1 转化操作  24
3.3.2 行动操作  26
3.3.3 惰性求值  27
3.4 向Spark传递函数  27
3.4.1 Python  27
3.4.2 Scala  28
3.4.3 Java  29
3.5 常见的转化操作和行动操作  30
3.5.1 基本RDD  30
3.5.2 在不同RDD类型间转换  37
3.6 持久化( 缓存)  39
3.7 总结  40
第4章 键值对操作  41
4.1 动机  41
4.2 创建Pair RDD  42
4.3 Pair RDD的转化操作  42
4.3.1 聚合操作  45
4.3.2 数据分组  49
4.3.3 连接  50
4.3.4 数据排序  51
4.4 Pair RDD的行动操作  52
4.5 数据分区（进阶）  52
4.5.1 获取RDD的分区方式  55
4.5.2 从分区中获益的操作  56
4.5.3 影响分区方式的操作  57
4.5.4 示例：PageRank  57
4.5.5 自定义分区方式  59
4.6 总结  61
第5章 数据读取与保存  63
5.1 动机  63
5.2 文件格式  64
5.2.1 文本文件  64
5.2.2 JSON  66
5.2.3 逗号分隔值与制表符分隔值  68
5.2.4 SequenceFile  71
5.2.5 对象文件  73
5.2.6 Hadoop输入输出格式  73
5.2.7 文件压缩  77
5.3 文件系统  78
5.3.1 本地/“常规”文件系统  78
5.3.2 Amazon S3  78
5.3.3 HDFS  79
5.4 Spark SQL中的结构化数据  79
5.4.1 Apache Hive  80
5.4.2 JSON  80
5.5 数据库  81
5.5.1 Java数据库连接  81
5.5.2 Cassandra  82
5.5.3 HBase  84
5.5.4 Elasticsearch  85
5.6 总结  86
第6章 Spark编程进阶  87
6.1 简介  87
6.2 累加器  88
6.2.1 累加器与容错性  90
6.2.2 自定义累加器  91
6.3 广播变量  91
6.4 基于分区进行操作  94
6.5 与外部程序间的管道  96
6.6 数值RDD 的操作  99
6.7 总结  100
第7章 在集群上运行Spark  101
7.1 简介  101
7.2 Spark运行时架构  101
7.2.1 驱动器节点  102
7.2.2 执行器节点  103
7.2.3 集群管理器  103
7.2.4 启动一个程序  104
7.2.5 小结  104
7.3 使用spark-submit 部署应用  105
7.4 打包代码与依赖  107
7.4.1 使用Maven构建的用Java编写的Spark应用  108
7.4.2 使用sbt构建的用Scala编写的Spark应用  109
7.4.3 依赖冲突   111
7.5 Spark应用内与应用间调度  111
7.6 集群管理器  112
7.6.1 独立集群管理器  112
7.6.2 Hadoop YARN  115
7.6.3 Apache Mesos  116
7.6.4 Amazon EC2  117
7.7 选择合适的集群管理器  120
7.8 总结  121
第8章 Spark调优与调试  123
8.1 使用SparkConf配置Spark  123
8.2 Spark执行的组成部分：作业、任务和步骤  127
8.3 查找信息  131
8.3.1 Spark网页用户界面  131
8.3.2 驱动器进程和执行器进程的日志  134
8.4 关键性能考量  135
8.4.1 并行度  135
8.4.2 序列化格式  136
8.4.3 内存管理  137
8.4.4 硬件供给  138
8.5 总结  139
第9章 Spark SQL  141
9.1 连接Spark SQL  142
9.2 在应用中使用Spark SQL  144
9.2.1 初始化Spark SQL  144
9.2.2 基本查询示例  145
9.2.3 SchemaRDD  146
9.2.4 缓存  148
9.3 读取和存储数据  149
9.3.1 Apache Hive  149
9.3.2 Parquet  150
9.3.3 JSON  150
9.3.4 基于RDD  152
9.4 JDBC/ODBC服务器  153
9.4.1 使用Beeline  155
9.4.2 长生命周期的表与查询  156
9.5 用户自定义函数  156
9.5.1 Spark SQL UDF  156
9.5.2 Hive UDF  157
9.6 Spark SQL性能  158
9.7 总结  159
第10章 Spark Streaming  161
10.1 一个简单的例子  162
10.2 架构与抽象  164
10.3 转化操作  167
10.3.1 无状态转化操作  167
10.3.2 有状态转化操作  169
10.4 输出操作  173
10.5 输入源  175
10.5.1 核心数据源  175
10.5.2 附加数据源  176
10.5.3 多数据源与集群规模  179
10.6 24/7不间断运行  180
10.6.1 检查点机制  180
10.6.2 驱动器程序容错  181
10.6.3 工作节点容错  182
10.6.4 接收器容错  182
10.6.5 处理保证  183
10.7 Streaming用户界面  183
10.8 性能考量  184
10.8.1 批次和窗口大小  184
10.8.2 并行度  184
10.8.3 垃圾回收和内存使用  185
10.9 总结  185
第11章 基于MLlib的机器学习  187
11.1 概述  187
11.2 系统要求  188
11.3 机器学习基础  189
11.4 数据类型  192
11.5 算法  194
11.5.1 特征提取  194
11.5.2 统计  196
11.5.3 分类与回归  197
11.5.4 聚类  202
11.5.5 协同过滤与推荐  203
11.5.6 降维  204
11.5.7 模型评估  206
11.6 一些提示与性能考量  206
11.6.1 准备特征  206
11.6.2 配置算法  207
11.6.3 缓存RDD以重复使用  207
11.6.4 识别稀疏程度  207
11.6.5 并行度  207
11.7 流水线API  208
11.8 总结  209
作者简介  210
封面介绍  210
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>运动改造大脑
前言 运动，重新建立身心连接！
人类天生就要动
运动让大脑保持最佳状态
运动是最佳的健脑丸
引言 不一样的体育课！
尽全力比跑得快更重要
四肢很发达，头脑不简单
01学习——越动越多的脑细胞
运动平衡大脑
运动让神奇的”脑细胞肥料“变多
大脑真的会长大
锻炼身体的同时，也在锻炼大脑
做出运动选择：兼顾技巧训练和有氧运动
02 压力——最艰巨的挑战
压力让你专注，也会让你上瘾
压力促进能量补给，也能拖垮大脑
侵害效应：过量的压力
运动：阻断大脑压力反馈回路的推手
员工爱运动，公司倒不了
03 焦虑——没什么好担心的
焦虑：表现糟糕的元凶
运动让大脑知道，焦虑是一种认知错误
身体真的可以影响心理
反击计划：找人一起运动
04 抑郁——让沉睡的大脑苏醒
马拉松：”内啡肽“旋风
改变运动习惯，等于改变抑郁症的患病几率
运动才是王道
抑郁：大脑神经细胞连结困难
摆脱抑郁枷锁：别让大脑留在离线状态
由上而下建立自信，由下而上涌现活力
05 专注力——远离分心障碍
集体分心倾向
麻烦的征兆
注意力系统与运动紧紧相连
和分心症共处：让缺点变优势
运动是神经浓汤的最佳食谱
06 成瘾——拿回自己的主动权
从瘾君子变成运动家
用运动戒瘾
天然的快感
拿回主动权：让大脑开启运动模式
07 激素——对女性大脑健康的影响
经前综合症：自然的潮起潮落
动一动，恢复脑内平衡
孕期：动？还是不动？
产后抑郁症：突如其来的低潮
停经：巨大的改变
努力保持身材的女人，IQ、EQ都不差
08 衰老——益智健康之道
运动可以预防大脑老化
认知的衰退：别让你的心智字典萎缩
情绪的衰退：别让生命热情凋谢
闲散的大脑是魔鬼的工坊
运动，持之以恒就对了
09 大脑训练计划——塑造你的大脑
体能越好，大脑越有复原力
步行：从轻度运动开始， 养成运动习惯
慢跑: 中等强度的运动，释放多种因子，让大脑更强壮
快跑：以强烈运动做间歇训练，大幅提升HGH浓度
瑜伽、太极拳或重量训练：让大脑重拾年轻活力
迈出第一步，让自己动起来
后记 让灵光持续绽放
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>运动改造大脑
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>细节
O1 向“大众”借力 ── 001
O2 “小众”的反作用力 ── 011
O3 非常态VS常态 ── 019
O4 强大的环境暗示 ── 025
O5 改个名字，改变一切 ── 031
O6 如何化敌为友 ── 037
O7 预测他人的喜好、渴望与需求 ── 043
O8 主动承诺的力量 ── 049
O9 承诺要行动，要公开 ── 055
1O “心安理得”效应 ── 061
11 如何为员工鼓劲儿加油 ── 067
12 如何避开决策陷阱 ── 073
13 巧用“执行意向” ── 079
14 推迟一点儿会更好 ── 085
15 为了将来的自己 ── 091
16 目标设得好，干劲儿会更足 ── 097
17 损失规避原则 ── 103
18 如何克服拖延症 ── 109
19 如何留住顾客跟定你 ── 115
2O 把潜力变成现实 ── 121
21 把会议开得更高效 ── 127
22 服装的影响力 ── 133
23 亮出专家身份 ── 137
24 不确定的说服力 ── 143
25 中心位置的影响力 ── 147
26 如何激发创意 ── 151
27 主场还是客场 ── 155
28 如何让自己变得更强大 ── 159
29 你所需的只是爱 ── 165
3O 完美礼物哪里找 ── 169
31 为互助留出余地 ── 175
32 表达感激好处多 ── 181
33 出乎意料与抛砖引玉 ── 187
34 如何获得帮助 ── 193
35 先下手为强 ── 197
36 报价精确一点儿会更好 ── 203
37 定价末尾数字有玄机 ── 209
38 顺序改一改，生意滚滚来 ── 217
39 如何事半功倍 ── 223
4O 化整为零 ── 229
41 鲜明生动的细节 ── 235
42 指出机会成本 ── 241
43 如何激励他人（还有你自己）完成任务 ── 247
44 如何提高客户忠诚度 ── 253
45 如何让一加一大于二 ── 259
46 退后一步看问题 ── 263
47 从他人的错误中汲取教训 ── 269
48 对错误进行管理 ── 275
49 当天就点评 ── 281
5O 给邮件加点儿料，让谈判更顺畅 ── 287
51 碰触的魔力 ── 293
52 把最好的留到最后 ── 299
额外附赠 ── 304
致谢 ── 311
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>细节
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
第1章　Spark的环境搭建与运行　　1
1.1　Spark的本地安装与配置　　2
1.2　Spark集群　　3
1.3　Spark编程模型　　4
1.3.1　SparkContext类与SparkConf 类　　4
1.3.2　Spark shell　　5
1.3.3　弹性分布式数据集　　6
1.3.4　广播变量和累加器　　10
1.4　Spark Scala编程入门　　11
1.5　Spark Java编程入门　　14
1.6　Spark Python编程入门　　17
1.7　在Amazon EC2上运行Spark　　18
1.8　小结　　23
第2章　设计机器学习系统　　24
2.1　MovieStream介绍　　24
2.2　机器学习系统商业用例　　25
2.2.1　个性化　　26
2.2.2　目标营销和客户细分　　26
2.2.3　预测建模与分析　　26
2.3　机器学习模型的种类　　27
2.4　数据驱动的机器学习系统的组成　　27
2.4.1　数据获取与存储　　28
2.4.2　数据清理与转换　　28
2.4.3　模型训练与测试回路　　29
2.4.4　模型部署与整合　　30
2.4.5　模型监控与反馈　　30
2.4.6　批处理或实时方案的选择　　31
2.5　机器学习系统架构　　31
2.6　小结　　33
第3章　Spark上数据的获取、处理与准备　　34
3.1　获取公开数据集　　35
3.2　探索与可视化数据　　37
3.2.1　探索用户数据　　38
3.2.2　探索电影数据　　41
3.2.3　探索评级数据　　43
3.3　处理与转换数据　　46
3.4　从数据中提取有用特征　　48
3.4.1　数值特征　　48
3.4.2　类别特征　　49
3.4.3　派生特征　　50
3.4.4　文本特征　　51
3.4.5　正则化特征　　55
3.4.6　用软件包提取特征　　56
3.5　小结　　57
第4章　构建基于Spark的推荐引擎　　58
4.1　推荐模型的分类　　59
4.1.1　基于内容的过滤　　59
4.1.2　协同过滤　　59
4.1.3　矩阵分解　　60
4.2　提取有效特征　　64
4.3　训练推荐模型　　67
4.3.1　使用MovieLens 100k数据集训练模型　　67
4.3.2　使用隐式反馈数据训练模型　　68
4.4　使用推荐模型　　69
4.4.1　用户推荐　　69
4.4.2　物品推荐　　72
4.5　推荐模型效果的评估　　75
4.5.1　均方差　　75
4.5.2　K值平均准确率　　77
4.5.3　使用MLlib内置的评估函数　　81
4.6　小结　　82
第5章　Spark构建分类模型　　83
5.1　分类模型的种类　　85
5.1.1　线性模型　　85
5.1.2　朴素贝叶斯模型　　89
5.1.3　决策树　　90
5.2　从数据中抽取合适的特征　　91
5.3　训练分类模型　　93
5.4　使用分类模型　　95
5.5　评估分类模型的性能　　96
5.5.1　预测的正确率和错误率　　96
5.5.2　准确率和召回率　　97
5.5.3　ROC曲线和AUC　　99
5.6　改进模型性能以及参数调优　　101
5.6.1　特征标准化　　101
5.6.2　其他特征　　104
5.6.3　使用正确的数据格式　　106
5.6.4　模型参数调优　　107
5.7　小结　　115
第6章　Spark构建回归模型　　116
6.1　回归模型的种类　　116
6.1.1　最小二乘回归　　117
6.1.2　决策树回归　　117
6.2　从数据中抽取合适的特征　　118
6.3　回归模型的训练和应用　　123
6.4　评估回归模型的性能　　125
6.4.1　均方误差和均方根误差　　125
6.4.2　平均绝对误差　　126
6.4.3　均方根对数误差　　126
6.4.4　R-平方系数　　126
6.4.5　计算不同度量下的性能　　126
6.5　改进模型性能和参数调优　　127
6.5.1　变换目标变量　　128
6.5.2　模型参数调优　　132
6.6　小结　　140
第7章　Spark构建聚类模型　　141
7.1　聚类模型的类型　　142
7.1.1　K-均值聚类　　142
7.1.2　混合模型　　146
7.1.3　层次聚类　　146
7.2　从数据中提取正确的特征　　146
7.3　训练聚类模型　　150
7.4　使用聚类模型进行预测　　151
7.5　评估聚类模型的性能　　155
7.5.1　内部评价指标　　155
7.5.2　外部评价指标　　156
7.5.3　在MovieLens数据集计算性能　　156
7.6　聚类模型参数调优　　156
7.7　小结　　158
第8章　Spark应用于数据降维　　159
8.1　降维方法的种类　　160
8.1.1　主成分分析　　160
8.1.2　奇异值分解　　160
8.1.3　和矩阵分解的关系　　161
8.1.4　聚类作为降维的方法　　161
8.2　从数据中抽取合适的特征　　162
8.3　训练降维模型　　169
8.4　使用降维模型　　172
8.4.1　在LFW数据集上使用PCA投影数据　　172
8.4.2　PCA和SVD模型的关系　　173
8.5　评价降维模型　　174
8.6　小结　　176
第9章　Spark高级文本处理技术　　177
9.1　处理文本数据有什么特别之处　　177
9.2　从数据中抽取合适的特征　　177
9.2.1　短语加权表示　　178
9.2.2　特征哈希　　179
9.2.3　从20新闻组数据集中提取TF-IDF特征　　180
9.3　使用TF-IDF模型　　192
9.3.1　20 Newsgroups数据集的文本相似度和TF-IDF特征　　192
9.3.2　基于20 Newsgroups数据集使用TF-IDF训练文本分类器　　194
9.4　评估文本处理技术的作用　　196
9.5　Word2Vec 模型　　197
9.6　小结　　200
第10章　Spark Streaming在实时机器学习上的应用　　201
10.1　在线学习　　201
10.2　流处理　　202
10.2.1　Spark Streaming介绍　　202
10.2.2　使用Spark Streaming缓存和容错　　205
10.3　创建Spark Streaming应用　　206
10.3.1　消息生成端　　207
10.3.2　创建简单的流处理程序　　209
10.3.3　流式分析　　211
10.3.4　有状态的流计算　　213
10.4　使用Spark Streaming进行在线学习　　215
10.4.1　流回归　　215
10.4.2　一个简单的流回归程序　　216
10.4.3　流K-均值　　220
10.5　在线模型评估　　221
10.6　小结　　224
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark高级数据分析
推荐序　　ix
译者序　　xi
序　　xiii
前言　　xv
第1章　大数据分析　　1
1.1　数据科学面临的挑战　　2
1.2　认识Apache Spark　　4
1.3　关于本书　　5
第2章　用Scala和Spark进行数据分析　　7
2.1　数据科学家的Scala　　8
2.2　Spark 编程模型　　9
2.3　记录关联问题　　9
2.4　小试牛刀：Spark shell和SparkContext　　10
2.5　把数据从集群上获取到客户端　　15
2.6　把代码从客户端发送到集群　　18
2.7　用元组和case class对数据进行结构化　　19
2.8　聚合　　23
2.9　创建直方图　　24
2.10　连续变量的概要统计　　25
2.11　为计算概要信息创建可重用的代码　　26
2.12　变量的选择和评分简介　　30
2.13　小结　　31
第3章　音乐推荐和Audioscrobbler数据集　　33
3.1　数据集　　34
3.2　交替最小二乘推荐算法　　35
3.3　准备数据　　37
3.4　构建第一个模型　　39
3.5　逐个检查推荐结果　　42
3.6　评价推荐质量　　43
3.7　计算AUC　　44
3.8　选择超参数　　46
3.9　产生推荐　　48
3.10　小结　　49
第4章　 用决策树算法预测森林植被　　51
4.1　回归简介　　52
4.2　向量和特征　　52
4.3　样本训练　　53
4.4　决策树和决策森林　　54
4.5　Covtype数据集　　56
4.6　准备数据　　57
4.7　第一棵决策树　　58
4.8　决策树的超参数　　62
4.9　决策树调优　　63
4.10　重谈类别型特征　　65
4.11　随机决策森林　　67
4.12　进行预测　　69
4.13　小结　　69
第5章　基于K均值聚类的网络流量异常检测　　71
5.1　异常检测　　72
5.2　K均值聚类　　72
5.3　网络入侵　　73
5.4　KDD Cup 1999数据集　　73
5.5　初步尝试聚类　　74
5.6　K 的选择　　76
5.7　基于R的可视化　　79
5.8　特征的规范化　　81
5.9　类别型变量　　83
5.10　利用标号的熵信息　　84
5.11　聚类实战　　85
5.12　小结　　86
第6章　基于潜在语义分析算法分析维基百科　　89
6.1　词项-文档矩阵　　90
6.2　获取数据　　91
6.3　分析和准备数据　　92
6.4　词形归并　　93
6.5　计算TF-IDF　　94
6.6　奇异值分解　　97
6.7　找出重要的概念　　98
6.8　基于低维近似的查询和评分　　101
6.9　词项-词项相关度　　102
6.10　文档-文档相关度　　103
6.11　词项-文档相关度　　105
6.12　多词项查询　　106
6.13　小结　　107
第7章　用GraphX分析伴生网络　　109
7.1　对MEDLINE文献引用索引的网络分析　　110
7.2　获取数据　　111
7.3　用Scala XML工具解析XML文档　　113
7.4　分析MeSH主要主题及其伴生关系　　114
7.5　用GraphX来建立一个伴生网络　　116
7.6　理解网络结构　　119
7.6.1　连通组件　　119
7.6.2　度的分布　　122
7.7　过滤噪声边　　124
7.7.1　处理EdgeTriplet　　125
7.7.2　分析去掉噪声边的子图　　126
7.8　小世界网络　　127
7.8.1　系和聚类系数　　128
7.8.2　用Pregel计算平均路径长度　　129
7.9　小结　　133
第8章　纽约出租车轨迹的空间和时间数据分析　　135
8.1　数据的获取　　136
8.2　基于Spark的时间和空间数据分析　　136
8.3　基于JodaTime和NScalaTime的时间数据处理　　137
8.4　基于Esri Geometry API和Spray的地理空间数据处理　　138
8.4.1　认识Esri Geometry API　　139
8.4.2　GeoJSON简介　　140
8.5　纽约市出租车客运数据的预处理　　142
8.5.1　大规模数据中的非法记录处理　　143
8.5.2　地理空间分析　　147
8.6　基于Spark的会话分析　　149
8.7　小结　　153
第9章　基于蒙特卡罗模拟的金融风险评估　　155
9.1　术语　　156
9.2　VaR计算方法　　157
9.2.1　方差-协方差法　　157
9.2.2　历史模拟法　　157
9.2.3　蒙特卡罗模拟法　　157
9.3　我们的模型　　158
9.4　获取数据　　158
9.5　数据预处理　　159
9.6　确定市场因素的权重　　162
9.7　采样　　164
9.8　运行试验　　167
9.9　回报分布的可视化　　170
9.10　结果的评估　　171
9.11　小结　　173
第10章　基因数据分析和BDG项目　　175
10.1　分离存储与模型　　176
10.2　用ADAM CLI导入基因学数据　　178
10.3　从ENCODE数据预测转录因子结合位点　　185
10.4　查询1000 Genomes项目中的基因型　　191
10.5　小结　　193
第11章　基于PySpark和Thunder的神经图像数据分析　　195
11.1　PySpark简介　　196
11.2　Thunder工具包概况和安装　　199
11.3　用Thunder加载数据　　200
11.4　用Thunder对神经元进行分类　　207
11.5　小结　　211
附录A　Spark进阶　　213
附录B　即将发布的MLlib Pipelines API　　221
作者介绍　　226
封面介绍　　226
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark高级数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解SPARK
前言
准　备　篇
第1章　环境准备 2
1.1　运行环境准备 2
1.1.1　安装JDK 3
1.1.2　安装Scala 3
1.1.3　安装Spark 4
1.2　Spark初体验 4
1.2.1　运行spark-shell 4
1.2.2　执行word count 5
1.2.3　剖析spark-shell 7
1.3　阅读环境准备 11
1.4　Spark源码编译与调试 13
1.5　小结 17
第2章　Spark设计理念与基本架构 18
2.1　初识Spark 18
2.1.1　Hadoop MRv1的局限 18
2.1.2　Spark使用场景 20
2.1.3　Spark的特点 20
2.2　Spark基础知识 20
2.3　Spark基本设计思想 22
2.3.1　Spark模块设计 22
2.3.2　Spark模型设计 24
2.4　Spark基本架构 25
2.5　小结 26
核心设计篇
第3章　SparkContext的初始化 28
3.1　SparkContext概述 28
3.2　创建执行环境SparkEnv 30
3.2.1　安全管理器SecurityManager 31
3.2.2　基于Akka的分布式消息系统ActorSystem 31
3.2.3　map任务输出跟踪器mapOutputTracker 32
3.2.4　实例化ShuffleManager 34
3.2.5　shuffle线程内存管理器ShuffleMemoryManager 34
3.2.6　块传输服务BlockTransferService 35
3.2.7　BlockManagerMaster介绍 35
3.2.8　创建块管理器BlockManager 36
3.2.9　创建广播管理器Broadcast-Manager 36
3.2.10　创建缓存管理器CacheManager 37
3.2.11　HTTP文件服务器HttpFile-Server 37
3.2.12　创建测量系统MetricsSystem 39
3.2.13　创建SparkEnv 40
3.3　创建metadataCleaner 41
3.4　SparkUI详解 42
3.4.1　listenerBus详解 43
3.4.2　构造JobProgressListener 46
3.4.3　SparkUI的创建与初始化 47
3.4.4　Spark UI的页面布局与展示 49
3.4.5　SparkUI的启动 54
3.5　Hadoop相关配置及Executor环境变量 54
3.5.1　Hadoop相关配置信息 54
3.5.2　Executor环境变量 54
3.6　创建任务调度器TaskScheduler 55
3.6.1　创建TaskSchedulerImpl 55
3.6.2　TaskSchedulerImpl的初始化 57
3.7　创建和启动DAGScheduler 57
3.8　TaskScheduler的启动 60
3.8.1　创建LocalActor 60
3.8.2　ExecutorSource的创建与注册 62
3.8.3　ExecutorActor的构建与注册 64
3.8.4　Spark自身ClassLoader的创建 64
3.8.5　启动Executor的心跳线程 66
3.9　启动测量系统MetricsSystem 69
3.9.1　注册Sources 70
3.9.2　注册Sinks 70
3.9.3　给Sinks增加Jetty的Servlet-ContextHandler 71
3.10　创建和启动ExecutorAllocation-Manager 72
3.11　ContextCleaner的创建与启动 73
3.12　Spark环境更新 74
3.13　创建DAGSchedulerSource和BlockManagerSource 76
3.14　将SparkContext标记为激活 77
3.15　小结 78
第4章　存储体系 79
4.1　存储体系概述 79
4.1.1　块管理器BlockManager的实现 79
4.1.2　Spark存储体系架构 81
4.2　shuffle服务与客户端 83
4.2.1　Block的RPC服务 84
4.2.2　构造传输上下文Transpor-tContext 85
4.2.3　RPC客户端工厂Transport-ClientFactory 86
4.2.4　Netty服务器TransportServer 87
4.2.5　获取远程shuffle文件 88
4.2.6　上传shuffle文件 89
4.3　BlockManagerMaster对Block-Manager的管理 90
4.3.1　BlockManagerMasterActor 90
4.3.2　询问Driver并获取回复方法 92
4.3.3　向BlockManagerMaster注册BlockManagerId 93
4.4　磁盘块管理器DiskBlockManager 94
4.4.1　DiskBlockManager的构造过程 94
4.4.2　获取磁盘文件方法getFile 96
4.4.3　创建临时Block方法create-TempShuffleBlock 96
4.5　磁盘存储DiskStore 97
4.5.1　NIO读取方法getBytes 97
4.5.2　NIO写入方法putBytes 98
4.5.3　数组写入方法putArray 98
4.5.4　Iterator写入方法putIterator 98
4.6　内存存储MemoryStore 99
4.6.1　数据存储方法putBytes 101
4.6.2　Iterator写入方法putIterator详解 101
4.6.3　安全展开方法unrollSafely 102
4.6.4　确认空闲内存方法ensureFreeSpace 105
4.6.5　内存写入方法putArray 107
4.6.6　尝试写入内存方法tryToPut 108
4.6.7　获取内存数据方法getBytes 109
4.6.8　获取数据方法getValues 110
4.7　Tachyon存储TachyonStore 110
4.7.1　Tachyon简介 111
4.7.2　TachyonStore的使用 112
4.7.3　写入Tachyon内存的方法putIntoTachyonStore 113
4.7.4　获取序列化数据方法getBytes 113
4.8　块管理器BlockManager 114
4.8.1　移出内存方法dropFrom-Memory 114
4.8.2　状态报告方法reportBlockStatus 116
4.8.3　单对象块写入方法putSingle 117
4.8.4　序列化字节块写入方法putBytes 118
4.8.5　数据写入方法doPut 118
4.8.6　数据块备份方法replicate 121
4.8.7　创建DiskBlockObjectWriter的方法getDiskWriter 125
4.8.8　获取本地Block数据方法getBlockData 125
4.8.9　获取本地shuffle数据方法doGetLocal 126
4.8.10　获取远程Block数据方法doGetRemote 127
4.8.11　获取Block数据方法get 128
4.8.12　数据流序列化方法dataSerializeStream 129
4.9　metadataCleaner和broadcastCleaner 129
4.10　缓存管理器CacheManager 130
4.11　压缩算法 133
4.12　磁盘写入实现DiskBlockObjectWriter 133
4.13　块索引shuffle管理器IndexShuffleBlockManager 135
4.14　shuffle内存管理器ShuffleMemoryManager 137
4.15　小结 138
第5章　任务提交与执行 139
5.1　任务概述 139
5.2　广播Hadoop的配置信息 142
5.3　RDD转换及DAG构建 144
5.3.1　为什么需要RDD 144
5.3.2　RDD实现分析 146
5.4　任务提交 152
5.4.1　任务提交的准备 152
5.4.2　finalStage的创建与Stage的划分 157
5.4.3　创建Job 163
5.4.4　提交Stage 164
5.4.5　提交Task 165
5.5　执行任务 176
5.5.1　状态更新 176
5.5.2　任务还原 177
5.5.3　任务运行 178
5.6　任务执行后续处理 179
5.6.1　计量统计与执行结果序列化 179
5.6.2　内存回收 180
5.6.3　执行结果处理 181
5.7　小结 187
第6章　计算引擎 188
6.1　迭代计算 188
6.2　什么是shuffle 192
6.3　map端计算结果缓存处理 194
6.3.1　map端计算结果缓存聚合 195
6.3.2　map端计算结果简单缓存 200
6.3.3　容量限制 201
6.4　map端计算结果持久化 204
6.4.1　溢出分区文件 205
6.4.2排序与分区分组 207
6.4.3　分区索引文件 209
6.5　reduce端读取中间计算结果 210
6.5.1　获取map任务状态 213
6.5.2　划分本地与远程Block 215
6.5.3　获取远程Block 217
6.5.4　获取本地Block 218
6.6　reduce端计算 219
6.6.1　如何同时处理多个map任务的中间结果 219
6.6.2　reduce端在缓存中对中间计算结果执行聚合和排序 220
6.7　map端与reduce端组合分析 221
6.7.1　在map端溢出分区文件，在reduce端合并组合 221
6.7.2　在map端简单缓存、排序分组，在reduce端合并组合 222
6.7.3　在map端缓存中聚合、排序分组，在reduce端组合 222
6.8　小结 223
第7章　部署模式 224
7.1　local部署模式 225
7.2　local-cluster部署模式 225
7.2.1　LocalSparkCluster的启动 226
7.2.2　CoarseGrainedSchedulerBackend的启动 236
7.2.3　启动AppClient 237
7.2.4　资源调度 242
7.2.5　local-cluster模式的任务执行 253
7.3　Standalone部署模式 255
7.3.1　启动Standalone模式 255
7.3.2　启动Master分析 257
7.3.3　启动Worker分析 259
7.3.4　启动Driver Application分析 261
7.3.5　Standalone模式的任务执行 263
7.3.6　资源回收 263
7.4　容错机制 266
7.4.1　Executor异常退出 266
7.4.2　Worker异常退出 268
7.4.3　Master异常退出 269
7.5　其他部署方案 276
7.5.1　YARN 277
7.5.2　Mesos 280
7.6　小结 282
扩　展　篇
第8章　Spark SQL 284
8.1　Spark SQL总体设计 284
8.1.1　传统关系型数据库SQL运行原理 285
8.1.2　Spark SQL运行架构 286
8.2　字典表Catalog 288
8.3　Tree和TreeNode 289
8.4　词法解析器Parser的设计与实现 293
8.4.1　SQL语句解析的入口 294
8.4.2　建表语句解析器DDLParser 295
8.4.3　SQL语句解析器SqlParser 296
8.4.4　Spark代理解析器SparkSQLParser 299
8.5　Rule和RuleExecutor 300
8.6　Analyzer与Optimizer的设计与实现 302
8.6.1　语法分析器Analyzer 304
8.6.2　优化器Optimizer 305
8.7　生成物理执行计划 306
8.8　执行物理执行计划 308
8.9　Hive 311
8.9.1　Hive SQL语法解析器 311
8.9.2　Hive SQL元数据分析 313
8.9.3　Hive SQL物理执行计划 314
8.10　应用举例：JavaSparkSQL 314
8.11　小结 320
第9章　流式计算 321
9.1　Spark Streaming总体设计 321
9.2　StreamingContext初始化 323
9.3　输入流接收器规范Receiver 324
9.4　数据流抽象DStream 325
9.4.1　Dstream的离散化 326
9.4.2　数据源输入流InputDStream 327
9.4.3　Dstream转换及构建DStream Graph 329
9.5　流式计算执行过程分析 330
9.5.1　流式计算例子CustomReceiver 331
9.5.2　Spark Streaming执行环境构建 335
9.5.3　任务生成过程 347
9.6　窗口操作 355
9.7　应用举例 357
9.7.1　安装mosquitto 358
9.7.2　启动mosquitto 358
9.7.3　MQTTWordCount 359
9.8　小结 361
第10章　图计算 362
10.1　Spark GraphX总体设计 362
10.1.1　图计算模型 363
10.1.2　属性图 365
10.1.3　GraphX的类继承体系 367
10.2　图操作 368
10.2.1　属性操作 368
10.2.2　结构操作 368
10.2.3　连接操作 369
10.2.4　聚合操作 370
10.3　Pregel API 371
10.3.1　Dijkstra算法 373
10.3.2　Dijkstra的实现 376
10.4　Graph的构建 377
10.4.1　从边的列表加载Graph 377
10.4.2　在Graph中创建图的方法 377
10.5　顶点集合抽象VertexRDD 378
10.6　边集合抽象EdgeRDD 379
10.7　图分割 380
10.8　常用算法 382
10.8.1　网页排名 382
10.8.2　Connected Components的应用 386
10.8.3　三角关系统计 388
10.9　应用举例 390
10.10　小结 391
第11章　机器学习 392
11.1机器学习概论 392
11.2　Spark MLlib总体设计 394
11.3　数据类型 394
11.3.1　局部向量 394
11.3.2标记点 395
11.3.3局部矩阵 396
11.3.4分布式矩阵 396
11.4基础统计 398
11.4.1摘要统计 398
11.4.2相关统计 399
11.4.3分层抽样 401
11.4.4假设检验 401
11.4.5随机数生成 402
11.5分类和回归 405
11.5.1数学公式 405
11.5.2线性回归 407
11.5.3分类 407
11.5.4回归 410
11.6决策树 411
11.6.1基本算法 411
11.6.2使用例子 412
11.7随机森林 413
11.7.1基本算法 414
11.7.2使用例子 414
11.8梯度提升决策树 415
11.8.1基本算法 415
11.8.2使用例子 416
11.9朴素贝叶斯 416
11.9.1算法原理 416
11.9.2使用例子 418
11.10保序回归 418
11.10.1算法原理 418
11.10.2使用例子 419
11.11协同过滤 419
11.12聚类 420
11.12.1K-means 420
11.12.2高斯混合 422
11.12.3快速迭代聚类 422
11.12.4latent Dirichlet allocation 422
11.12.5流式K-means 423
11.13维数减缩 424
11.13.1奇异值分解 424
11.13.2主成分分析 425
11.14特征提取与转型 425
11.14.1术语频率反转 425
11.14.2单词向量转换 426
11.14.3标准尺度 427
11.14.4正规化尺度 428
11.14.5卡方特征选择器 428
11.14.6Hadamard积 429
11.15频繁模式挖掘 429
11.16预言模型标记语言 430
11.17管道 431
11.17.1管道工作原理 432
11.17.2管道API介绍 433
11.17.3交叉验证 435
11.18小结 436
附录A　Utils 437
附录B　Akka 446
附录C　Jetty 450
附录D　Metrics 453
附录E　Hadoop word count 456
附录F　CommandUtils 458
附录G　Netty 461
附录H　源码编译错误 465
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解SPARK
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>PySpark实战指南
Contents?目 录
译者序
序
前言
关于作者
第1章 了解Spark 1
1.1 什么是Apache Spark 1
1.2 Spark作业和API 2
1.2.1 执行过程 2
1.2.2 弹性分布式数据集 3
1.2.3 DataFrame 4
1.2.4 Dataset 5
1.2.5 Catalyst优化器 5
1.2.6 钨丝计划 5
1.3 Spark 2.0的架构 6
1.3.1 统一Dataset和DataFrame 7
1.3.2 SparkSession介绍 8
1.3.3 Tungsten Phase 2 8
1.3.4 结构化流 10
1.3.5 连续应用 10
1.4 小结 11
第2章 弹性分布式数据集 12
2.1 RDD的内部运行方式 12
2.2 创建RDD 13
2.2.1 Schema 14
2.2.2 从文件读取 14
2.2.3 Lambda表达式 15
2.3 全局作用域和局部作用域 16
2.4 转换 17
2.4.1 .map(...)转换 17
2.4.2 .filter(...)转换 18
2.4.3 .flatMap(...)转换 18
2.4.4 .distinct(...)转换 18
2.4.5 .sample(...)转换 19
2.4.6 .leftOuterJoin(...)转换 19
2.4.7 .repartition(...)转换 20
2.5 操作 20
2.5.1 .take(...)方法 21
2.5.2 .collect(...)方法 21
2.5.3 .reduce(...)方法 21
2.5.4 .count(...)方法 22
2.5.5 .saveAsTextFile(...)方法 22
2.5.6 .foreach(...)方法 23
2.6 小结 23
第3章 DataFrame 24
3.1 Python到RDD之间的通信 24
3.2 Catalyst优化器刷新 25
3.3 利用DataFrame加速PySpark 27
3.4 创建DataFrame 28
3.4.1 生成自己的JSON数据 29
3.4.2 创建一个DataFrame 29
3.4.3 创建一个临时表 30
3.5 简单的DataFrame查询 31
3.5.1 DataFrame API查询 32
3.5.2 SQL查询 32
3.6 RDD的交互操作 33
3.6.1 使用反射来推断模式 33
3.6.2 编程指定模式 34
3.7 利用DataFrame API查询 35
3.7.1 行数 35
3.7.2 运行筛选语句 35
3.8 利用SQL查询 36
3.8.1 行数 36
3.8.2 利用where子句运行筛选语句 36
3.9 DataFrame场景——实时飞行性能 38
3.9.1 准备源数据集 38
3.9.2 连接飞行性能和机场 39
3.9.3 可视化飞行性能数据 40
3.10 Spark数据集（Dataset）API 41
3.11 小结 42
第4章 准备数据建模 43
4.1 检查重复数据、未观测数据和异常数据（离群值） 43
4.1.1 重复数据 43
4.1.2 未观测数据 46
4.1.3 离群值 50
4.2 熟悉你的数据 51
4.2.1 描述性统计 52
4.2.2 相关性 54
4.3 可视化 55
4.3.1 直方图 55
4.3.2 特征之间的交互 58
4.4 小结 60
第5章 MLlib介绍 61
5.1 包概述 61
5.2 加载和转换数据 62
5.3 了解你的数据 65
5.3.1 描述性统计 66
5.3.2 相关性 67
5.3.3 统计测试 69
5.4 创建最终数据集 70
5.4.1 创建LabeledPoint形式的RDD 70
5.4.2 分隔培训和测试数据 71
5.5 预测婴儿生存机会 71
5.5.1 MLlib中的逻辑回归 71
5.5.2 只选择最可预测的特征 72
5.5.3 MLlib中的随机森林 73
5.6 小结 74
第6章 ML包介绍 75
6.1 包的概述 75
6.1.1 转换器 75
6.1.2 评估器 78
6.1.3 管道 80
6.2 使用ML预测婴儿生存几率 80
6.2.1 加载数据 80
6.2.2 创建转换器 81
6.2.3 创建一个评估器 82
6.2.4 创建一个管道 82
6.2.5 拟合模型 83
6.2.6 评估模型的性能 84
6.2.7 保存模型 84
6.3 超参调优 85
6.3.1 网格搜索法 85
6.3.2 Train-validation 划分 88
6.4 使用PySpark ML的其他功能 89
6.4.1 特征提取 89
6.4.2 分类 93
6.4.3 聚类 95
6.4.4 回归 98
6.5 小结 99
第7章 GraphFrames 100
7.1 GraphFrames介绍 102
7.2 安装GraphFrames 102
7.2.1 创建库 103
7.3 准备你的航班数据集 105
7.4 构建图形 107
7.5 执行简单查询 108
7.5.1 确定机场和航班的数量 108
7.5.2 确定这个数据集中的最长延误时间 108
7.5.3 确定延误和准点/早到航班的数量对比 109
7.5.4 哪一班从西雅图出发的航班最有可能出现重大延误 109
7.5.5 西雅图出发到哪个州的航班最有可能出现重大延误 110
7.6 理解节点的度 110
7.7 确定最大的中转机场 112
7.8 理解Motif 113
7.9 使用PageRank确定机场排名 114
……
第8章 TensorFrames 120
8.1 深度学习是什么 120
8.1.1 神经网络和深度学习的必要性 123
8.1.2 特征工程是什么 125
8.1.3 桥接数据和算法 125
8.2 TensorFlow是什么 127
8.2.1 安装PIP 129
8.2.2 安装TensorFlow 129
8.2.3 使用常量进行矩阵乘法 130
8.2.4 使用placeholder进行矩阵乘法 131
8.2.5 讨论 132
8.3 TensorFrames介绍 133
8.4 TensorFrames快速入门 134
8.4.1 配置和设置 134
8.4.2 使用TensorFlow向已有列添加常量 136
8.4.3 Blockwise reducing操作示例 137
8.5 小结 139
第9章 使用Blaze实现混合持久化
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>PySpark实战指南
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据Spark企业级实战
第1章　Spark编程模型	1
1.1  Spark：一体化、多元化的高速
大数据通用计算平台和库	1
1.1.1  为什么需要使用Spark	5
1.1.2  Spark技术生态系统简介	9
1.2  Spark大数据处理框架	20
1.2.1  Spark速度为何如此之快	20
1.2.2  RDD：分布式函数式编程	24
1.3  Spark子框架解析	28
1.3.1  图计算框架Spark GraphX	28
1.3.2  实时流处理框架（Spark Streaming）	41
1.3.3  交互式SQL处理框架Spark SQL	46
1.3.4  机器学习框架（Spark MLlib）	49
第2章　构建Spark分布式集群	55
2.1  搭建Hadoop单机版本和伪分布式开发环境	55
2.1.1  开发Hadoop需要的基本软件	56
2.1.2  安装每个软件	58
2.1.3  配置Hadoop单机模式并运行Wordcount示例	76
2.1.3  配置Hadoop伪分布模式并运行Wordcount示例	84
2. 2  搭建 Hadoop分布式集群的	92
2.2.1  在VMWare 中准备第二、第三台运行Ubuntu系统的机器	92
2.2.2  按照配置伪分布式模式的方式配置新创建运行Ubuntu系统的机器	93
2.2.3  配置Hadoop分布式集群环境	94
2.2.4  测试Hadoop分布式集群环境	105
2.3  Spark集群的动手搭建	108
2.3.1  Spark集群需要的软件	108
2.3.2  安装每个软件	110
2.3.3  启动并查看集群的状况	116
2.4  构建Hadoop单机版本和伪分布式环境	120
2.4.1  通过Spark的shell测试Spark的工作	121
2.4.2  使用Spark的cache机制观察一下效率的提升	125
第3章　Spark开发环境及其测试	129
3.1  搭建和设置IDEA开发环境	129
3.1.1  构建Spark的IDE开发环境	129
3.1.2  配置Spark的IDE开发环境	132
3.2  测试IDEA环境	146
3.3  实战：在IDEA中开发代码，并运行在Spark集群中	148
第4章　Spark RDD与编程API实战	159
4.1  深度解析Spark RDD	159
4.2  Transformation Operations动手实战	165
4.3  Action Operations动手实战	175
4.4  Spark API综合实战	179
第5章　Spark运行模式深入解析	191
5.1  Spark运行模式概述	192
5.1.1  Spark的运行模式列表	196
5.1.2  Spark的基本工作流程	197
5.2  Standalone模式	201
5.2.1  部署及程序运行	202
5.2.2  内部实现原理	206
5.3  Yarn-Cluster模式	234
5.3.1  部署及程序运行	235
5.3.2  内部实现原理	237
5.4  Yarn-Client模式	243
5.4.1  部署及运行程序	243
5.4.2  内部实现原理	244
第6章　Spark内核解析	247
6.1  Spark内核初探	247
6.1.1  Spark内核核心术语解析	247
6.1.2  Spark集群概览	250
6.1.3  Spark核心组件	251
6.1.4  Spark任务调度系统初见	252
6.2  Spark内核核心源码解读	256
6.2.1  SparkContext核心源码解析初体验	256
6.2.2  TaskSceduler启动源码解析初体验	260
6.2.3  DAGScheduler源码解读初体验	261
6.2.4  Spark的Web监控页面	262
6.3  以RDD的count操作为例触发Job全生命周期源码研究	263
6.4  Akka驱动下的Driver、Master、Worker	276
6.4.1  Driver中的AppClient源码解析	276
6.4.2  AppClient注册Master	279
6.4.3  Worker中Executor启动过程源代码解析	282
第7章　GraphX大规模图计算与图挖掘实战	287
7.1  Spark GraphX概览	288
7.2  Spark GraphX设计实现的核心原理	291
7.3  Table operator和Graph Operator	295
7.4  Vertices、edges、triplets	296
7.5  以最原始的方式构建graph	299
7.6  动手编写第一个Graph代码实例并进行Vertices、edges、triplets操作	299
7.7  在Spark集群上使用文件中的数据加载成为graph并进行操作	310
7.8  在Spark集群上掌握比较重要的图操作	320
7.9  Spark GraphX图算法	342
7.10  淘宝对Spark GraphX的大规模使用	347
第8章　Spark SQL原理与实战	349
8.1  为什么使用Spark SQL	349
8.1.1  Spark SQL的发展历程	349
8.1.2  Spark SQL的性能	351
8.2  Spark SQL运行架构	355
8.2.1  Tree和Rule	357
8.2.2  sqlContext的运行过程	360
8.2.3  hiveContext的运行过程	362
8.2.4  catalyst优化器	365
8.3  解析Spark SQL组件	367
8.3.1  LogicalPlan	367
8.3.2  SqlParser	370
8.3.3  Analyzer	378
8.3.4  Optimizer	381
8.4  深入了解Spark SQL运行的计划	383
8.4.1  hive/console的安装过程和原理	383
8.4.2  常用操作	386
8.4.3  不同数据源的运行计划	388
8.4.4  不同查询的运行计划	391
8.4.5  查询的优化	393
8.5  搭建测试环境	396
8.5.1  搭建虚拟集群（Hadoop1、Hadoop2、Hadoop3）	397
8.5.2  搭建客户端	398
8.5.3  文件数据的准备工作	399
8.5.4  Hive数据的准备工作	399
8.6  Spark SQL之基础应用	400
8.6.1  sqlContext的基础应用	402
8.6.2  hiveContext的基础应用	405
8.6.3  混合使用	408
8.6.4  缓存的使用	409
8.6.5  DSL的使用	410
8.7  ThriftServer和CLI	411
8.7.1  令人惊讶的CLI	411
8.7.2  ThriftServer	414
8.8  Spark SQL之综合应用	418
8.8.1  店铺分类	419
8.8.2  PageRank	421
8.9  Spark SQL之调优	424
8.9.1  并行性	424
8.9.2  高效的数据格式	425
8.9.3  内存的使用	427
8.9.4  合适的Task	428
8.9.5  其他的一些建议	428
第9章　Machine Learning on Spark	431
9.1  Spark MLlib机器学习	431
9.1.1  机器学习快速入门	432
9.1.2  Spark MLlib介绍	442
9.1.3  Spark MLlib架构解析	447
9.1.4  Spark Mllib核心解析	458
9.2  MLlib经典算法解析和案例实战	462
9.2.1  Linear Regression解析和实战	462
9.2.2  K-Means解析和实战	484
9.2.3  协同过滤算法分析和案例实战	502
9.3  MLLib其他常用算法解析和代码实战	552
9.3.1  Basic Statics解析和实战	553
9.3.2  MLlib朴素贝叶斯解析和实战	560
9.3.3  MLlib决策树解析和实战	562
第10章　Tachyon文件系统	565
10.1  Tachyon文件系统概述	565
10.1.1  Tachyon文件系统简介	565
10.1.2  HDFS与Tachyon	566
10.1.3  Tachyon设计原理	568
10.2  Tachyon入门	568
10.2.1  Tachyon部署	568
10.2.2  Tachyon API的使用	570
10.2.3  在MapReduce、Spark上使用Tachyon	572
10.3  Tachyon深度解析	573
10.3.1  Tachyon整体设计概述	573
10.3.2  Tachyon Master启动流程分析	574
10.3.3  Tachyon Worker启动流程分析	577
10.3.4  客户端读写文件源码分析	577
10.4  Tachyon配置参数一览	579
10.5  小结	580
第11章　Spark Streaming原理与实战	581
11.1  Spark Streaming原理	581
11.1.1  原理和运行场景	581
11.1.2  编程模型DStream	584
11.1.3  持久化、容错和优化	588
11.2  Spark Streaming实战	589
11.2.1  源码解析	589
11.2.2  Spark Streaming实战案例	600
第12章　Spark多语言编程	605
12.1  Spark多语言编程的特点	605
12.2  Spark编程模型	609
12.3  深入Spark多语言编程	611
12.4  Spark多语言编程综合实例	622
第13章　R语言的分布式编程之SparkR	627
13.1  R语言快速入门	627
13.1.1  R语言是什么	627
13.1.2  R语言的特点	629
13.1.3  R语言的安装	630
13.1.4  R的核心概念	630
13.1.5  R动手实战	631
13.2  使用SparkR	661
13.2.1  SparkR的安装	661
13.2.2  使用SparkR编写WordCount	662
13.2.3  使用SparkR的更多代码示例	662
第14章　Spark性能调优和最佳实践	665
14.1  Spark性能调优	665
14.1.1  Spark性能优化的12大问题及其解决方法	665
14.1.2  Spark内存优化	669
14.1.3  RDD分区	672
14.1.4  Spark性能优化实例	674
14.2  Spark性能调优细节	675
14.2.1  broadcast和accumulator	675
14.2.2  reduce 和 reduceByKey	676
14.2.3  深入reduceByKey	677
第15章　Spark源码解析	679
15.1  BlockManager源码解析	679
15.2  Cache源码解析	707
15.3  Checkpoint源码解析	725
附录A　动手实战Scala三部曲	733
第一部动手体验Scala	735
第二部　动手实战Scala面向对象编程	746
第三部动手实战Scala函数式编程	761
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据Spark企业级实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析
目录
前言
致谢
关于作者
1 引言：为什么要超越 Hadoop Map-Reduce  1
Hadoop的适用范围  3
大数据分析之机器学习实现的革命  10
第一代机器学习工具 /范式  11
第二代机器学习工具 /范式  11
第三代机器学习工具 /范式  14
小结  18
参考文献  19
2 何为伯克利数据分析栈（BDAS）  23
实现 BDAS的动机  24
Spark：动机  25
Shark：动机  26
Mesos：动机  28
BDAS的设计及架构  29
Spark：高效的集群数据处理的范式  34
Spark的弹性分布式数据集  36
Spark的实现  40
Spark VS. 分布式共享内存系统  42
RDD的表达性  44
类似 Spark的系统  45
Shark：分布式系统上的 SQL接口  46
Spark为 Shark提供的扩展  47
列内存存储  49
分布式数据加载  50
完全分区智能连接  50
分区修剪  50
机器学习的支持  51
Mesos：集群调度及管理系统  51
Mesos组件  52
资源分配  54
隔离  55
容错性  57
小结  58
参考文献  59
3 使用 Spark实现机器学习算法  66
机器学习基础知识  66
机器学习：随机森林示例  68
逻辑回归：概述  72
二元形式的逻辑回归  73
逻辑回归估计  75
多元逻辑回归  76
Spark中的逻辑回归算法  77
支持向量机  80
复杂决策面  81
支持向量机背后的数学原理  82
Spark中的支持向量机  84
Spark对 PMML的支持  85
PMML结构  87
PMML的生产者及消费者  92
Spark对朴素贝叶斯的 PMML支持  94
Spark对线性回归的 PMML支持  95
在 Spark中使用 MLbase进行机器学习  97
参考文献  99
4 实现实时的机器学习算法 101
Storm简介  101
数据流  103
拓扑  104
Storm集群  105
简单的实时计算例子  106
数据流组  108
Storm的消息处理担保  109
基于 Storm的设计模式  111
分布式远程过程调用  111
Trident：基于 Storm的实时聚合  115
实现基于 Storm的逻辑回归算法  116
实现基于 Storm的支持向量机算法  120
Storm对朴素贝叶斯 PMML的支持  122
实时分析的应用  126
工业日志分类  126
互联网流量过滤器  130
Storm的替代品  131
Spark流  133
D-Streams的动机  133
参考文献  135
5 图处理范式 138
Pregel：基于 BSP的图处理框架  139
类似的做法  141
开源的 Pregel实现  143
Giraph  143
GoldenORB  145
Phoebus  145
Apache Hama  146
Stanford GPS  146
GraphLab  147
GraphLab：多核版本  148
分布式的 GraphLab  150
PowerGraph  152
通过 GraphLab实现网页排名算法  156
顶点程序  158
基于 GraphLab实现随机梯度下降算法  163
参考文献  167
6 结论：超越Hadoop Map-Reduce的大数据分析  171
Hadoop YARN概览  172
Hadoop YARN的动机  172
作为资源调度器的 YARN  174
YARN上的其他框架  175
大数据分析的未来是怎样的  177
参考文献  180
附录A 代码笔记  182
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法
序 1
前言 3
第1章二次排序：简介 19
二次排序问题解决方案 21
MapReduce/Hadoop的二次排序解决方案 25
Spark的二次排序解决方案 29
第2章二次排序：详细示例 42
二次排序技术 43
二次排序的完整示例 46
运行示例——老版本Hadoop API 50
运行示例——新版本Hadoop API 52
第3章 Top 10 列表 54
Top N 设计模式的形式化描述 55
MapReduce/Hadoop实现：唯一键 56
Spark实现：唯一键 62
Spark实现：非唯一键 73
使用takeOrdered()的Spark Top 10 解决方案 84
MapReduce/Hadoop Top 10 解决方案：非唯一键 91
第4章左外连接 96
左外连接示例 96
MapReduce左外连接实现 99
Spark左外连接实现 105
使用leftOuterJoin()的Spark实现 117
第5章反转排序 127
反转排序模式示例 128
反转排序模式的MapReduce/Hadoop实现 129
运行示例 134
第6章移动平均 137
示例1：时间序列数据（股票价格） 137
示例2：时间序列数据（URL访问数） 138
形式定义 139
POJO移动平均解决方案 140
MapReduce/Hadoop移动平均解决方案 143
第7章购物篮分析 155
MBA目标 155
MBA的应用领域 157
使用MapReduce的购物篮分析 157
Spark解决方案 166
运行Spark实现的YARN 脚本 179
第8章共同好友 182
输入 183
POJO共同好友解决方案 183
MapReduce算法 184
解决方案1: 使用文本的Hadoop实现 187
解决方案2: 使用ArrayListOfLongsWritable 的Hadoop实现 189
Spark解决方案 191
第9章使用MapReduce实现推荐引擎 201
购买过该商品的顾客还购买了哪些商品 202
经常一起购买的商品 206
推荐连接 210
第10章基于内容的电影推荐 225
输入 226
MapReduce阶段1 226
MapReduce阶段2和阶段3 227
Spark电影推荐实现 234
第11章使用马尔可夫模型的智能邮件营销 .253
马尔可夫链基本原理 254
使用MapReduce的马尔可夫模型 256
Spark解决方案 269
第12章 K-均值聚类 282
什么是K-均值聚类? 285
聚类的应用领域 285
K-均值聚类方法非形式化描述：分区方法 286
K-均值距离函数 286
K-均值聚类形式化描述 287
K-均值聚类的MapReduce解决方案 288
K-均值算法Spark实现 292
第13章 k-近邻 296
kNN分类 297
距离函数 297
kNN示例 298
kNN算法非形式化描述 299
kNN算法形式化描述 299
kNN的类Java非MapReduce 解决方案 299
Spark的kNN算法实现 301
第14章朴素贝叶斯 315
训练和学习示例 316
条件概率 319
深入分析朴素贝叶斯分类器 319
朴素贝叶斯分类器：符号数据的MapReduce解决方案 322
朴素贝叶斯分类器Spark实现 332
使用Spark和Mahout 347
第15章情感分析 349
情感示例 350
情感分数：正面或负面 350
一个简单的MapReduce情感分析示例 351
真实世界的情感分析 353
第16章查找、统计和列出大图中的所有三角形 354
基本的图概念 355
三角形计数的重要性 356
MapReduce/Hadoop解决方案 357
Spark解决方案 364
第17章 K-mer计数 375
K-mer计数的输入数据 376
K-mer计数应用 376
K-mer计数MapReduce/Hadoop解决方案 377
K-mer计数Spark解决方案 378
第18章 DNA测序 390
DNA测序的输入数据 392
输入数据验证 393
DNA序列比对 393
DNA测试的MapReduce算法 394
第19章 Cox回归 413
Cox模型剖析 414
使用R的Cox回归 415
Cox回归应用 416
Cox回归 POJO解决方案 417
MapReduce输入 418
使用MapReduce的Cox回归 419
第20章 Cochran-Armitage趋势检验 426
Cochran-Armitage算法 427
Cochran-Armitage应用 432
MapReduce解决方案 435
第21章等位基因频率 443
基本定义 444
形式化问题描述 448
等位基因频率分析的MapReduce解决方案 449
MapReduce解决方案, 阶段1 449
MapReduce解决方案，阶段2 459
MapReduce解决方案, 阶段3 463
染色体X 和Y的特殊处理 466
第22章 T检验 468
对bioset完成T检验 469
MapReduce问题描述 472
输入 472
期望输出 473
MapReduce解决方案 473
Spark实现 476
第23章皮尔逊相关系数 488
皮尔逊相关系数公式 489
皮尔逊相关系数示例 491
皮尔逊相关系数数据集 492
皮尔逊相关系数POJO 解决方案 492
皮尔逊相关系数MapReduce解决方案 493
皮尔逊相关系数的Spark 解决方案 496
运行Spark程序的YARN 脚本 516
使用Spark计算斯皮尔曼相关系数 517
第24章 DNA碱基计数 520
FASTA 格式 521
FASTQ 格式 522
MapReduce解决方案：FASTA 格式 522
运行示例 524
MapReduce解决方案: FASTQ 格式 528
Spark 解决方案: FASTA 格式 533
Spark解决方案: FASTQ 格式 537
第25章 RNA测序 543
数据大小和格式 543
MapReduce工作流 544
RNA测序分析概述 544
RNA测序MapReduce算法 548
第26章基因聚合 553
输入 554
输出 554
MapReduce解决方案（按单个值过滤和按平均值过滤） 555
基因聚合的Spark解决方案 567
Spark解决方案：按单个值过滤 567
Spark解决方案：按平均值过滤 576
第27章线性回归 586
基本定义 587
简单示例 587
问题描述 588
输入数据 589
期望输出 590
使用SimpleRegression的MapReduce解决方案 590
Hadoop实现类 593
使用R线性模型的MapReduce解决方案 593
第28章 MapReduce和幺半群 600
概述 600
幺半群的定义 602
幺半群和非幺半群示例 603
MapReduce示例：非幺半群 606
MapReduce示例：幺半群 608
使用幺半群的Spark示例 612
使用幺半群的结论 618
函子和幺半群 619
第29章小文件问题 622
解决方案1：在客户端合并小文件 623
解决方案2：用CombineFileInputFormat解决小文件问题 629
其他解决方案 634
第30章 MapReduce的大容量缓存 635
实现方案 636
缓存问题形式化描述 637
一个精巧、可伸缩的解决方案 637
实现LRUMap缓存 640
使用LRUMap的MapReduce解决方案 646
第31章 Bloom过滤器 651Bloom
过滤器性质 651
一个简单的Bloom过滤器示例 653
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>天才的13个思维工具
致谢
前言
第一章 对思考的重新思考
第二章 想像力教育
第三章 观察
第四章 形象思维
第五章 抽象
第六章 识别模式(规律)
第七章 形成模式
第八章 类比
第九章 身体思考
第十章 同情理解
第十一章 空间思维
第十二章 模型
第十三章 游戏
第十四章 转换
第十五章 整合
第十六章 整合教育
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>天才的13个思维工具
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理：技术、应用与性能优化
前　言
第1章　Spark简介  1
1.1　Spark是什么  1
1.2　Spark生态系统BDAS  4
1.3　Spark架构  6
1.4　Spark分布式架构与单机多核
架构的异同  9
1.5　Spark的企业级应用  10
1.5.1　Spark在Amazon中的应用  11
1.5.2　Spark在Yahoo!的应用  15
1.5.3　Spark在西班牙电信的应用  17
1.5.4　Spark在淘宝的应用  18
1.6　本章小结  20
第2章　Spark集群的安装与部署  21
2.1　Spark的安装与部署  21
2.1.1　在Linux集群上安装与配置Spark  21
2.1.2　在Windows上安装与配置Spark  30
2.2　Spark集群初试  33
2.3　本章小结  35
第3章　Spark计算模型  36
3.1　Spark程序模型  36
3.2　弹性分布式数据集  37
3.2.1　RDD简介  38
3.2.2　RDD与分布式共享内存的异同  38
3.2.3　Spark的数据存储  39
3.3　Spark算子分类及功能  41
3.3.1　Value型Transformation算子  42
3.3.2　Key-Value型Transformation算子  49
3.3.3　Actions算子  53
3.4　本章小结  59
第4章　Spark工作机制详解  60
4.1　Spark应用执行机制  60
4.1.1　Spark执行机制总览  60
4.1.2　Spark应用的概念  62
4.1.3　应用提交与执行方式  63
4.2　Spark调度与任务分配模块  65
4.2.1　Spark应用程序之间的调度  66
4.2.2　Spark应用程序内Job的调度  67
4.2.3　Stage和TaskSetManager调度方式  72
4.2.4　Task调度  74
4.3　Spark I/O机制  77
4.3.1　序列化  77
4.3.2　压缩  78
4.3.3　Spark块管理  80
4.4　Spark通信模块  93
4.4.1　通信框架AKKA  94
4.4.2　Client、Master和Worker间的通信  95
4.5　容错机制  104
4.5.1　Lineage机制  104
4.5.2　Checkpoint机制  108
4.6　Shuffle机制  110
4.7　本章小结  119
第5章　Spark开发环境配置及流程  120
5.1　Spark应用开发环境配置  120
5.1.1　使用Intellij开发Spark程序  120
5.1.2　使用Eclipse开发Spark程序  125
5.1.3　使用SBT构建Spark程序  129
5.1.4　使用Spark Shell开发运行Spark程序  130
5.2　远程调试Spark程序  130
5.3　Spark编译  132
5.4　配置Spark源码阅读环境  135
5.5　本章小结  135
第6章　Spark编程实战  136
6.1　WordCount  136
6.2　Top K  138
6.3　中位数  140
6.4　倒排索引  141
6.5　CountOnce  143
6.6　倾斜连接  144
6.7　股票趋势预测  146
6.8　本章小结  153
第7章　Benchmark使用详解  154
7.1　Benchmark简介  154
7.1.1　Intel Hibench与Berkeley BigDataBench  155
7.1.2　Hadoop GridMix  157
7.1.3　Bigbench、BigDataBenchmark与TPC-DS  158
7.1.4　其他Benchmark  161
7.2　Benchmark的组成  162
7.2.1　数据集  162
7.2.2　工作负载  163
7.2.3　度量指标  167
7.3　Benchmark的使用  168
7.3.1　使用Hibench  168
7.3.2　使用TPC-DS  170
7.3.3　使用BigDataBench  172
7.4　本章小结  176
第8章　BDAS简介  177
8.1　SQL on Spark  177
8.1.1　使用Spark SQL的原因  178
8.1.2　Spark SQL架构分析  179
8.1.3　Shark简介  182
8.1.4　Hive on Spark  184
8.1.5　未来展望  185
8.2　Spark Streaming  185
8.2.1　Spark Streaming简介  186
8.2.2　Spark Streaming架构  188
8.2.3　Spark Streaming原理剖析  189
8.2.4　Spark Streaming调优  198
8.2.5　Spark Streaming 实例  198
8.3　GraphX  205
8.3.1　GraphX简介  205
8.3.2　GraphX的使用  206
8.3.3　GraphX架构  209
8.3.4　运行实例  211
8.4　MLlib  215
8.4.1　MLlib简介  217
8.4.2　MLlib的数据存储  219
8.4.3　数据转换为向量（向量空间模型VSM）  222
8.4.4　MLlib中的聚类和分类  223
8.4.5　算法应用实例  228
8.4.6　利用MLlib进行电影推荐  230
8.5　本章小结  237
第9章　Spark性能调优  238
9.1　配置参数  238
9.2　调优技巧  239
9.2.1　调度与分区优化  240
9.2.2　内存存储优化  243
9.2.3　网络传输优化  249
9.2.4　序列化与压缩  251
9.2.5　其他优化方法  253
9.3　本章小结  255
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理：技术、应用与性能优化
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速数据处理
译者序
作者简介
前言
第1章　安装Spark以及构建Spark集群 / 1
1.1　单机运行Spark / 4
1.2　在EC2上运行Spark / 5
1.3　在ElasticMapReduce上部署Spark / 11
1.4　用Chef(opscode)部署Spark / 12
1.5　在Mesos上部署Spark / 14
1.6　在Yarn上部署Spark / 15
1.7　通过SSH部署集群 / 16
1.8　链接和参考 / 21
1.9　小结 / 21
第2章　Spark shell的使用 / 23
2.1　加载一个简单的text文件 / 24
2.2　用Spark shell运行逻辑回归 / 26
2.3　交互式地从S3加载数据 / 28
2.4　小结 / 30
第3章　构建并运行Spark应用 / 31
3.1　用sbt构建Spark作业 / 32
3.2　用Maven构建Spark作业 / 36
3.3　用其他工具构建Spark作业 / 39
3.4　小结 / 39
第4章　创建SparkContext / 41
4.1　Scala / 43
4.2　Java / 43
4.3　Java和Scala共享的API / 44
4.4　Python / 45
4.5　链接和参考 / 45
4.6　小结 / 46
第5章　加载与保存数据 / 47
5.1　RDD / 48
5.2　加载数据到RDD中 / 49
5.3　保存数据 / 54
5.4　连接和参考 / 55
5.5　小结 / 55
第6章　操作RDD / 57
6.1　用Scala和Java操作RDD / 58
6.2　用Python操作RDD / 79
6.3　链接和参考 / 83
6.4　小结 / 84
第7章　Shark-Hive和Spark的综合运用 / 85
7.1　为什么用Hive/Shark / 86
7.2　安装Shark / 86
7.3　运行Shark / 88
7.4　加载数据 / 88
7.5　在Spark程序中运行HiveQL查询 / 89
7.6　链接和参考 / 92
7.7　小结 / 93
第8章　测试 / 95
8.1　用Java和Scala测试 / 96
8.2　用Python测试 / 103
8.3　链接和参考 / 104
8.4　小结 / 105
第9章　技巧和窍门 / 107
9.1　日志位置 / 108
9.2　并发限制 / 108
9.3　内存使用与垃圾回收 / 109
9.4　序列化 / 110
9.5　IDE集成环境 / 111
9.6　Spark与其他语言 / 112
9.7　安全提示 / 113
9.8　邮件列表 / 113
9.9　链接和参考 / 113
9.10　小结 / 114
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速数据处理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Apache Spark源码剖析
第一部分Spark概述1
第1章初识Spark   3
1.1 大数据和Spark 3
1.1.1 大数据的由来4
1.1.2 大数据的分析4
1.1.3 Hadoop 5
1.1.4 Spark简介6
1.2 与Spark的第一次亲密接触7
1.2.1 环境准备7
1.2.2 下载安装Spark 8
1.2.3 Spark下的WordCount 8
第二部分Spark核心概念13
第2章Spark整体框架 15
2.1 编程模型15
2.1.1 RDD 17
2.1.2 Operation 17
2.2 运行框架18
2.2.1 作业提交18
2.2.2 集群的节点构成18
2.2.3 容错处理19
2.2.4 为什么是Scala 19
2.3 源码阅读环境准备19
2.3.1 源码下载及编译19
2.3.2 源码目录结构21
2.3.3 源码阅读工具21
2.3.4 本章小结22
第3章SparkContext初始化 23
3.1 spark-shell 23
3.2 SparkContext的初始化综述27
3.3 Spark Repl综述30
3.3.1 Scala Repl执行过程31
3.3.2 Spark Repl 32
第4章Spark作业提交 33
4.1 作业提交33
4.2 作业执行38
4.2.1 依赖性分析及Stage划分39
4.2.2 Actor Model和Akka 46
4.2.3 任务的创建和分发47
4.2.4 任务执行53
4.2.5 Checkpoint和Cache 62
4.2.6 WebUI和Metrics 62
4.3 存储机制71
4.3.1 Shuffle结果的写入和读取71
4.3.2 Memory Store 80
4.3.3 存储子模块启动过程分析81
4.3.4 数据写入过程分析82
4.3.5 数据读取过程分析84
4.3.6 TachyonStore 88
第5章部署方式分析 91
5.1 部署模型91
5.2 单机模式local 92
5.3 伪集群部署local-cluster 93
5.4 原生集群Standalone Cluster 95
5.4.1 启动Master 96
5.4.2 启动Worker 97
5.4.3 运行spark-shell 102
5.4.4 容错性分析106
5.5 Spark On YARN 112
5.5.1 YARN的编程模型112
5.5.2 YARN中的作业提交112
5.5.3 Spark On YARN实现详解113
5.5.4 SparkPi on YARN 122
第三部分Spark Lib 129
第6章Spark Streaming  131
6.1 Spark Streaming整体架构131
6.1.1 DStream 132
6.1.2 编程接口133
6.1.3 Streaming WordCount 134
6.2 Spark Streaming执行过程135
6.2.1 StreamingContext初始化过程136
6.2.2 数据接收141
6.2.3 数据处理146
6.2.4 BlockRDD 155
6.3 窗口操作158
6.4 容错性分析159
6.5 Spark Streaming vs. Storm 165
6.5.1 Storm简介165
6.5.2 Storm和Spark Streaming对比168
6.6 应用举例168
6.6.1 搭建Kafka Cluster 168
6.6.2 KafkaWordCount 169
第7章SQL   173
7.1 SQL语句的通用执行过程分析175
7.2 SQL On Spark的实现分析178
7.2.1 SqlParser 178
7.2.2 Analyzer 184
7.2.3 Optimizer 191
7.2.4 SparkPlan 192
7.3 Parquet 文件和JSON数据集196
7.4 Hive简介197
7.4.1 Hive 架构197
7.4.2 HiveQL On MapReduce执行过程分析199
7.5 HiveQL On Spark详解200
7.5.1 Hive On Spark环境搭建206
7.5.2 编译支持Hadoop 2.x的Spark 211
7.5.3 运行Hive On Spark测试用例213
第8章GraphX  215
8.1 GraphX简介215
8.1.1 主要特点216
8.1.2 版本演化216
8.1.3 应用场景217
8.2 分布式图计算处理技术介绍218
8.2.1 属性图218
8.2.2 图数据的存储与分割219
8.3 Pregel计算模型220
8.3.1 BSP 220
8.3.2 像顶点一样思考220
8.4 GraphX图计算框架实现分析223
8.4.1 基本概念223
8.4.2 图的加载与构建226
8.4.3 图数据存储与分割227
8.4.4 操作接口228
8.4.5 Pregel在GraphX中的源码实现230
8.5 PageRank 235
8.5.1 什么是PageRank 235
8.5.2 PageRank核心思想235
第9章MLLib   239
9.1 线性回归239
9.1.1 数据和估计240
9.1.2 线性回归参数求解方法240
9.1.3 正则化245
9.2 线性回归的代码实现246
9.2.1 简单示例246
9.2.2 入口函数train 247
9.2.3 最优化算法optimizer 249
9.2.4 权重更新update 256
9.2.5 结果预测predict 257
9.3 分类算法257
9.3.1 逻辑回归258
9.3.2 支持向量机260
9.4 拟牛顿法261
9.4.1 数学原理261
9.4.2 代码实现265
9.5 MLLib与其他应用模块间的整合268
第四部分附录271
附录A Spark源码调试 273
附录B 源码阅读技巧 283
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Apache Spark源码剖析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>一切与创造有关
序 宣扬创新和一项新综合研究
第一部分 棍棒和石头：第一个创新
第一章 会创新的灵长类动物
第二章 古人类血统中最后站立的人
第二部分 晚饭吃什么：人类是如何变得会创新的
第三章 让我们一起做把刀吧
第四章 杀死并吃掉，等等
第五章 排队的美好
第六章 食品安全的实现
第三部分 战争与性：人类是如何塑造出一个世界的
第七章 创造战争（与和平）
第八章 有创意的性
第四部分 伟大的作品：人类是如何创造出宇宙的
第九章 宗教的基础
第十章 艺术的翅膀
第十一章 科学架构
尾 声 创新人生的节拍
注 释
致 谢
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>一切与创造有关
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理技术
第1章 Spark系统概述 1
1.1 大数据处理框架 1
1.2 Spark大数据处理框架 3
1.2.1 RDD表达能力 3
1.2.2 Spark子系统 4
1.3 小结 7
第2章 Spark RDD及编程接口 9
2.1 Spark程序“Hello World” 9
2.2 Spark RDD 12
2.2.1 RDD分区（partitions） 13
2.2.2 RDD优先位置（preferredLocations） 13
2.2.3 RDD依赖关系（dependencies） 15
2.2.4 RDD分区计算（compute） 19
2.2.5 RDD分区函数（partitioner） 20
2.3 创建操作 23
2.3.1 集合创建操作 23
2.3.2 存储创建操作 23
2.4 转换操作 26
2.4.1 RDD基本转换操作 26
2.4.2 键值RDD转换操作 35
2.4.3 再论RDD依赖关系 43
2.5 控制操作（control operation） 46
2.6 行动操作（action operation） 47
2.6.1 集合标量行动操作 47
2.6.2 存储行动操作 52
2.7 小结 56
第3章 Spark运行模式及原理 57
3.1 Spark运行模式概述 57
3.1.1 Spark运行模式列表 57
3.1.2 Spark基本工作流程 58
3.1.3 相关基本类 59
3.2 Local模式 62
3.2.1 部署及程序运行 62
3.2.2 内部实现原理 63
3.3 Standalone模式 64
3.3.1 部署及程序运行 64
3.3.2 内部实现原理 67
3.4 Local cluster模式 68
3.4.1 部署及程序运行 68
3.4.2 内部实现原理 69
3.5 Mesos模式 69
3.5.1 部署及程序运行 69
3.5.2 内部实现原理 70
3.6 YARN standalone / YARN cluster模式 72
3.6.1 部署及程序运行 72
3.6.2 内部实现原理 75
3.7 YARN client模式 76
3.7.1 部署及程序运行 76
3.7.2 内部实现原理 77
3.8 各种模式的实现细节比较 78
3.8.1 环境变量的传递 78
3.8.2 JAR包和各种依赖文件的分发 80
3.8.3 任务管理和序列化 82
3.8.4 用户参数配置 83
3.8.5 用户及权限控制 84
3.9 Spark 1.0版本之后的变化 85
3.10 小结 86
第4章 Spark调度管理原理 87
4.1 Spark作业调度管理概述 87
4.2 Spark调度相关基本概念 88
4.3 作业调度模块顶层逻辑概述 89
4.4 作业调度具体工作流程 92
4.4.1 调度阶段的拆分 94
4.4.2 调度阶段的提交 97
4.4.3 任务集的提交 99
4.4.4 完成状态的监控 99
4.4.5 任务结果的获取 101
4.5 任务集管理模块详解 102
4.6 调度池和调度模式分析 104
4.7 其他调度相关内容 106
4.7.1 Spark应用之间的调度关系 106
4.7.2 调度过程中的数据本地性问题 106
4.8 小结 107
第5章 Spark的存储管理 109
5.1 存储管理模块整体架构 109
5.1.1 通信层架构 110
5.1.2 通信层消息传递 112
5.1.3 注册存储管理模块 113
5.1.4 存储层架构 114
5.1.5 数据块 (Block) 116
5.2 RDD 持久化 116
5.2.1 RDD分区和数据块的关系 117
5.2.2 内存缓存 118
5.2.3 磁盘缓存 119
5.2.4 持久化选项 120
5.2.5 如何选择不同的持久化选项 122
5.3 Shuffle数据持久化 122
5.4 广播（Broadcast）变量持久化 125
5.5 小结 126
第6章 Spark监控管理 127
6.1 UI管理 127
6.1.1 实时UI管理 128
6.1.2 历史UI管理 132
6.2 Metrics管理 133
6.2.1 Metrics系统架构 133
6.2.2 Metrics系统配置 135
6.2.3 输入源（Metrics Source）介绍 136
6.2.4 输出方式（Metrics Sink）介绍 138
6.3 小结 139
第7章 Shark架构与安装配置 141
7.1 Shark架构浅析 142
7.2 Hive/Shark各功能组件对比 143
7.2.1 MetaStore 143
7.2.2 CLI/ Beeline 143
7.2.3 JDBC/ODBC 144
7.2.4 Hive Server/2 与 Shark Server/2 144
7.2.5 Driver 145
7.2.6 SQL Parser 146
7.2.7 查询优化器（Query Optimizer） 147
7.2.8 物理计划与执行 147
7.3 Shark安装配置与使用 148
7.3.1 安装前准备工作 149
7.3.2 在不同运行模式下安装Shark 149
7.4 Shark SQL命令行工具（CLI） 152
7.5 使用Shark Shell命令 155
7.6 启动Shark Server 155
7.7 Shark Server2配置与启动 156
7.8 缓存数据表 157
7.8.1 数据缓存级别 158
7.8.2 创建不同缓存级别的Shark数据表 158
7.8.3 指定数据表缓存策略 159
7.8.4 使用Tachyon 160
7.9 常见问题分析 160
7.9.1 OutOfMemory异常 160
7.9.2 数据处理吞吐量低 161
7.9.3 Shark查询比Hive慢 161
7.10 小结 162
第8章 SQL程序扩展 163
8.1 程序扩展并行运行模式 164
8.2 Evaluator和ObjectInspector 164
8.3 自定义函数扩展 168
8.3.1 自定义函数扩展分类 168
8.3.2 CLI中的用户自定义函数扩展相关命令 170
8.3.3 用户自定义函数（UDF） 171
8.3.4 通用用户自定义函数（Generic UDF） 175
8.3.5 用户自定义聚合函数（UDAF） 178
8.3.6 通用用户自定义聚合函数（Generic UDAF） 182
8.3.7 通用用户自定义表函数（Generic UDTF） 186
8.4 自定义数据存取格式 190
8.4.1 SerDe 190
8.4.2 StorageHandler 197
8.5 小结 198
第9章 Spark SQL 199
9.1 Spark SQL逻辑架构 199
9.1.1 Catalyst功能边界 200
9.1.2 SQL解析阶段 201
9.1.3 逻辑计划元数据绑定和语义分析阶段 202
9.1.4 逻辑计划优化阶段 202
9.1.5 物理计划生成阶段 202
9.1.6 Shark和Spark SQL对比 203
9.2 Catalyst上下文（Context） 204
9.2.1 SQLContext 204
9.2.2 HiveContext 205
9.3 SQL DSL API 206
9.3.1 数据源管理 206
9.3.2 SchemaRDD 208
9.3.3 Row API 210
9.3.4 数据类型 211
9.3.5 DSL API举例 213
9.3.6 表达式计算 214
9.3.7 Parquet列式存储文件 218
9.3.8 代码演示 218
9.4 Java API 221
9.5 Python API 224
9.6 Spark SQL CLI 225
9.7 Thrift服务 225
9.8 小结 225
第10章 Spark Streaming流数据处理框架 227
10.1 快速入门 227
10.2 Spark Streaming基本概念 229
10.2.1 链接和初始化 229
10.2.2 时间和窗口概念 231
10.2.3 DStream原理 232
10.2.4 DStream输入源 234
10.2.5 DStream 操作 235
10.2.6 DStream持久化 237
10.3 性能调优 238
10.3.1 运行时间优化 238
10.3.2 内存使用优化 238
10.4 容错处理 239
10.4.1 工作节点失效 239
10.4.2 驱动节点失效 240
10.5 DStream作业的产生和调度 242
10.5.1 作业产生 242
10.5.2 作业调度 243
10.5.3 Streaming作业与Spark作业之间的关系 244
10.6 DStream与RDD关系 246
10.7 数据接收原理 248
10.8 自定义数据输入源 251
10.9 自定义监控接口（StreamingListener） 253
10.10 Spark Streaming案例分析 254
10.11 小结 256
第11章 GraphX计算框架 259
11.1 图并行计算 259
11.1.1 数据并行与图并行计算 259
11.1.2 图并行计算框架简介 260
11.1.3 GraphX简介 264
11.2 GraphX模型设计 264
11.2.1 数据模型 264
11.2.2 图计算接口 265
11.3 GraphX模型实现 269
11.3.1 图的分布式存储 269
11.3.2 图操作执行策略 278
11.3.3 图操作执行优化 280
11.3.4 序列化和反序列化 283
11.3.5 GraphX内置算法库 284
11.4 GraphX应用 285
11.4.1 Pregel模型 285
11.4.2 N维邻接关系计算 288
11.5 小结 291
第12章 Tachyon存储系统 293
12.1 设计原理 294
12.1.1 高效的内存读写 294
12.1.2 无副本的可靠性实现——Lineage 297
12.2 框架设计 299
12.2.1 主节点 300
12.2.2 工作节点 304
12.2.3 客户端 306
12.2.4 读写工作流程 307
12.3 Tachyon的部署 313
12.3.1 单机部署 313
12.3.2 分布式部署 316
12.3.3 Tachyon的配置 317
12.4 Tachyon应用 321
12.4.1 Shark原始表（RawTable） 321
12.4.2 Spark的堆外RDD 325
12.4.3 Tachyon用户接口（API） 327
12.5 相关项目讨论 335
12.6 小结 336
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理技术
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>创客学
第1章 提问激发创新：这是一门技术，人人都可以熟练掌握
如何提问才能激发别人思考？
从“与众不同”的“为什么”开始
绝杀式提问，惠普通过创新盈利的秘决
第2章 既定假设：突破框框，点子才会喷涌而出
经验可能会毁掉你的创意
做几个练习，让大脑细胞跳动起来
逃离你的舒适区
借助“突发事件”，触发新点子
颠覆性产品才是真正的利润来源
第3章 创新抗体：创意遭遇阻力时，如何寻求帮助
如何充分利用反对者的意见？
逐类说服，让反对者点头称是
勇气是创客的必备素质
第4章 FIRE法则：从点子到结果的最短路径
FIRE四步创新法
过送筛选，直达最佳的点子
如何争取管理层的支持？
第5章 产品创新：你真的了解产品的客户群体吗？
如何识别客户需求的细微变化？
绝杀式提问实战演练
第6章 产品核心价值：宁选高价值，不选低价格
如何把低成本的产品买个高价钱？
“斯普尼克时刻”提醒你，要么创新，要么没落
第7章 价值链：创客是从研发到营销的全能大师
为产品选取最佳上市时机
价值链的盲点，也是利润的薄弱点
原创，其实是升级版的模仿
模块化创新 VS 开放式创新
价值链重构，把产品摆到更多买家面前
还在按昨天的方式管理企业？
第8章 头脑风暴：如何做到既高产又高效？
会议的一个主题与六项准则
如何制定时间计划表？
这样准备头脑风暴更高产
这样主持头脑风暴更高效
结果在于会后的跟踪与执行
第9章 创新路线图：用案例说明如何制定属于你的创新体系
克罗格零成本改善客户体验
美国教育部的创新项目
如何提炼创新项目的绝杀式提问？
后记：想到就要做到，执行才能得到
附录：资深创客麦肯尼屡试不爽的绝杀式提问清单
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>创客学
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark最佳实践
第1章 　Spark与大数据　　1
1.1 　大数据的发展及现状　　1
1.1.1 　大数据时代所面临的问题　　1
1.1.2 　谷歌的大数据解决方案　　2
1.1.3 　Hadoop生态系统　　3
1.2 　Spark应时而生　　4
1.2.1 　Spark的起源　　4
1.2.2 　Spark的特点　　5
1.2.3 　Spark的未来发展　　6
第2章 　Spark基础　　8
2.1 　Spark本地单机模式体验　　8
2.1.1 　安装虚拟机　　8
2.1.2 　安装JDK　　19
2.1.3 　下载Spark预编译包　　21
2.1.4 　本地体验Spark　　22
2.2 　高可用Spark分布式集群部署　　25
2.2.1 　集群总览　　26
2.2.2 　集群机器的型号选择　　28
2.2.3 　初始化集群机器环境　　29
2.2.4 　部署ZooKeeper集群　　33
2.2.5 　编译Spark　　35
2.2.6 　部署Spark Standalone集群　　37
2.2.7 　高可用Hadoop集群　　40
2.2.8 　让Spark运行在YARN上　　40
2.2.9 　一键部署高可用Hadoop +Spark集群　　42
2.3 　Spark编程指南　　43
2.3.1 　交互式编程　　43
2.3.2 　RDD创建　　44
2.3.3 　RDD操作　　47
2.3.4 　使用其他语言开发Spark程序　　54
2.4 　打包和提交　　54
2.4.1 　编译、链接、打包　　54
2.4.2 　提交　　56
第3章 　Spark工作机制　　58
3.1 　调度管理　　58
3.1.1 　集群概述及名词解释　　58
3.1.2 　Spark程序之间的调度　　60
3.1.3 　Spark程序内部的调度　　63
3.2 　内存管理　　65
3.2.1 　RDD持久化　　65
3.2.2 　共享变量　　66
3.3 　容错机制　　67
3.3.1 　容错体系概述　　67
3.3.2 　Master节点失效　　68
3.3.3 　Slave节点失效　　69
3.4 　监控管理　　69
3.4.1 　Web界面　　69
3.4.2 　REST API　　72
3.4.3 　Metrics指标体系　　73
3.4.4 　其他监控工具　　73
3.5 　Spark程序配置管理　　73
3.5.1 　Spark程序配置加载过程　　74
3.5.2 　环境变量配置　　74
3.5.3 　Spark属性项配置　　74
3.5.4 　查看当前的配置　　76
3.5.5 　配置Spark日志　　76
第4章 　Spark内核讲解　　77
4.1 　Spark核心数据结构RDD　　77
4.1.1 　RDD的定义　　78
4.1.2 　RDD的Transformation　　80
4.1.3 　RDD的Action　　82
4.1.4 　Shuffle　　83
4.2 　SparkContext　　84
4.2.1 　SparkConf配置　　84
4.2.2 　初始化过程　　85
4.2.3 　其他功能接口　　87
4.3 　DAG调度　　87
4.3.1 　DAGScheduler　　87
4.3.2 　TaskScheduler　　90
第5章 　Spark SQL与数据仓库　　92
5.1 　Spark SQL基础　　93
5.1.1 　分布式SQL引擎　　93
5.1.2 　支持的SQL语法　　97
5.1.3 　支持的数据类型　　98
5.1.4 　DataFrame　　99
5.1.5 　DataFrame数据源　　103
5.1.6 　性能调优　　104
5.2 　Spark SQL原理和运行机制　　104
5.2.1 　Spark SQL整体架构　　105
5.2.2 　Catalyst执行优化器　　105
5.3 　应用场景：基于淘宝数据建立电商数据仓库　　110
5.3.1 　电商数据仓库场景　　111
5.3.2 　数据准备和表设计　　111
5.3.3 　用Spark SQL来完成日常运营数据分析　　115
5.3.4 　Spark SQL在大规模数据下的性能表现　　120
第6章 　Spark流式计算　　122
6.1 　Spark Streaming基础知识　　123
6.1.1 　入门简单示例　　123
6.1.2 　基本概念　　124
6.1.3 　高级操作　　129
6.2 　深入理解Spark Streaming　　132
6.2.1 　DStream的两类操作　　132
6.2.2 　容错处理　　134
6.2.3 　性能调优　　136
6.2.4 　与Storm的对比　　137
6.3 　应用场景：一个类似百度统计的流式实时系统　　139
6.3.1 　Web log实时统计场景　　139
6.3.2 　日志实时采集　　140
6.3.3 　流式分析系统实现　　140
第7章 　Spark图计算　　149
7.1 　什么是图计算　　149
7.1.1 　图的基本概念　　149
7.1.2 　图计算的应用　　150
7.2 　Spark GraphX简介　　151
7.2.1 　GraphX实现　　151
7.2.2 　GraphX常用API介绍　　152
7.3 　应用场景：基于新浪微博数据的社交网络分析　　153
7.3.1 　社交网络分析的主要应用　　153
7.3.2 　社区发现算法简介　　154
7.3.3 　用GraphX实现Louvain算法　　156
7.3.4 　小试牛刀：谁是你的闺蜜　　162
7.3.5 　真实的场景：新浪微博关系
分析　　164
第8章 　Spark MLlib　　169
8.1 　机器学习简介　　169
8.1.1 　什么是机器学习　　169
8.1.2 　机器学习示例　　171
8.1.3 　机器学习的基本方法　　172
8.1.4 　机器学习的常见技巧　　173
8.1.5 　机器学习参考资料　　174
8.2 　MLlib库简介　　174
8.2.1 　基础数据类型　　174
8.2.2 　主要的库　　175
8.2.3 　附带的示例程序　　176
8.3 　应用场景：搜索广告点击率预估系统　　178
8.3.1 　应用场景　　178
8.3.2 　逻辑回归　　179
8.3.3 　学习算法　　181
8.3.4 　模型评估　　184
8.3.5 　数据准备　　186
8.3.6 　模型训练　　187
8.3.7 　模型调优　　195
附录 　Scala语言参考　　197
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark最佳实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark MLlib机器学习
第一部分　Spark MLlib基础
第1章　Spark机器学习简介	2
1.1　机器学习介绍	2
1.2　Spark介绍	3
1.3　Spark MLlib介绍	4
第2章　Spark数据操作	6
2.1　Spark RDD操作	6
2.1.1　Spark RDD创建操作	6
2.1.2　Spark RDD转换操作	7
2.1.3　Spark RDD行动操作	14
2.2　MLlib Statistics统计操作	15
2.2.1　列统计汇总	15
2.2.2　相关系数	16
2.2.3　假设检验	18
2.3　MLlib数据格式	18
2.3.1　数据处理	18
2.3.2　生成样本	22
第3章　Spark MLlib矩阵向量	26
3.1　Breeze介绍	26
3.1.1　Breeze创建函数	27
3.1.2　Breeze元素访问及操作函数	29
3.1.3　Breeze数值计算函数	34
3.1.4　Breeze求和函数	35
3.1.5　Breeze布尔函数	36
3.1.6　Breeze线性代数函数	37
3.1.7　Breeze取整函数	39
3.1.8　Breeze常量函数	40
3.1.9　Breeze复数函数	40
3.1.10　Breeze三角函数	40
3.1.11　Breeze对数和指数函数	40
3.2　BLAS介绍	41
3.2.1　BLAS向量-向量运算	42
3.2.2　BLAS矩阵-向量运算	42
3.2.3　BLAS矩阵-矩阵运算	43
3.3　MLlib向量	43
3.3.1　MLlib向量介绍	43
3.3.2　MLlib Vector接口	44
3.3.3　MLlib DenseVector类	46
3.3.4　MLlib SparseVector类	49
3.3.5　MLlib Vectors伴生对象	50
3.4　MLlib矩阵	57
3.4.1　MLlib矩阵介绍	57
3.4.2　MLlib Matrix接口	57
3.4.3　MLlib DenseMatrix类	59
3.4.4　MLlib SparseMatrix类	64
3.4.5　MLlib Matrix伴生对象	71
3.5　MLlib BLAS	77
3.6　MLlib分布式矩阵	93
3.6.1　MLlib分布式矩阵介绍	93
3.6.2　行矩阵（RowMatrix）	94
3.6.3　行索引矩阵（IndexedRowMatrix）	96
3.6.4　坐标矩阵（CoordinateMatrix）	97
3.6.5　分块矩阵（BlockMatrix）	98
第二部分　Spark MLlib回归算法
第4章　Spark MLlib线性回归算法	102
4.1　线性回归算法	102
4.1.1　数学模型	102
4.1.2　最小二乘法	105
4.1.3　梯度下降算法	105
4.2　源码分析	106
4.2.1　建立线性回归	108
4.2.2　模型训练run方法	111
4.2.3　权重优化计算	114
4.2.4　线性回归模型	121
4.3　实例	123
4.3.1　训练数据	123
4.3.2　实例代码	123
第5章　Spark MLlib逻辑回归算法	126
5.1　逻辑回归算法	126
5.1.1　数学模型	126
5.1.2  梯度下降算法	128
5.1.3　正则化	129
5.2　源码分析	132
5.2.1　建立逻辑回归	134
5.2.2　模型训练run方法	137
5.2.3　权重优化计算	137
5.2.4　逻辑回归模型	144
5.3　实例	148
5.3.1　训练数据	148
5.3.2　实例代码	148
第6章　Spark MLlib保序回归算法	151
6.1　保序回归算法	151
6.1.1　数学模型	151
6.1.2　L2保序回归算法	153
6.2　源码分析	153
6.2.1　建立保序回归	154
6.2.2　模型训练run方法	156
6.2.3　并行PAV计算	156
6.2.4　PAV计算	157
6.2.5　保序回归模型	159
6.3　实例	164
6.3.1　训练数据	164
6.3.2　实例代码	164
第三部分　Spark MLlib分类算法
第7章　Spark MLlib贝叶斯分类算法	170
7.1　贝叶斯分类算法	170
7.1.1　贝叶斯定理	170
7.1.2　朴素贝叶斯分类	171
7.2　源码分析	173
7.2.1　建立贝叶斯分类	173
7.2.2　模型训练run方法	176
7.2.3　贝叶斯分类模型	179
7.3　实例	181
7.3.1　训练数据	181
7.3.2　实例代码	182
第8章　Spark MLlib SVM支持向量机算法	184
8.1　SVM支持向量机算法	184
8.1.1　数学模型	184
8.1.2　拉格朗日	186
8.2　源码分析	189
8.2.1　建立线性SVM分类	191
8.2.2　模型训练run方法	194
8.2.3　权重优化计算	194
8.2.4　线性SVM分类模型	196
8.3　实例	199
8.3.1　训练数据	199
8.3.2　实例代码	199
第9章　Spark MLlib决策树算法	202
9.1　决策树算法	202
9.1.1　决策树	202
9.1.2　特征选择	203
9.1.3　决策树生成	205
9.1.4　决策树生成实例	206
9.1.5　决策树的剪枝	208
9.2　源码分析	209
9.2.1　建立决策树	211
9.2.2　建立随机森林	216
9.2.3　建立元数据	220
9.2.4　查找特征的分裂及划分	223
9.2.5　查找最好的分裂顺序	228
9.2.6　决策树模型	231
9.3　实例	234
9.3.1　训练数据	234
9.3.2　实例代码	234
第四部分　Spark MLlib聚类算法
第10章　Spark MLlib KMeans聚类算法	238
10.1　KMeans聚类算法	238
10.1.1　KMeans算法	238
10.1.2　演示KMeans算法	239
10.1.3　初始化聚类中心点	239
10.2　源码分析	240
10.2.1　建立KMeans聚类	242
10.2.2　模型训练run方法	247
10.2.3　聚类中心点计算	248
10.2.4　中心点初始化	251
10.2.5　快速距离计算	254
10.2.6　KMeans聚类模型	255
10.3　实例	258
10.3.1　训练数据	258
10.3.2　实例代码	259
第11章　Spark MLlib LDA主题模型算法	261
11.1　LDA主题模型算法	261
11.1.1　LDA概述	261
11.1.2　LDA概率统计基础	262
11.1.3　LDA数学模型	264
11.2　GraphX基础	267
11.3　源码分析	270
11.3.1　建立LDA主题模型	272
11.3.2　优化计算	279
11.3.3　LDA模型	283
11.4　实例	288
11.4.1　训练数据	288
11.4.2　实例代码	288
第五部分　Spark MLlib关联规则挖掘算法
第12章　Spark MLlib FPGrowth关联规则算法	292
12.1　FPGrowth关联规则算法	292
12.1.1　基本概念	292
12.1.2　FPGrowth算法	293
12.1.3　演示FP树构建	294
12.1.4　演示FP树挖掘	296
12.2　源码分析	298
12.2.1　FPGrowth类	298
12.2.2　关联规则挖掘	300
12.2.3　FPTree类	303
12.2.4　FPGrowthModel类	306
12.3　实例	306
12.3.1　训练数据	306
12.3.2　实例代码	306
第六部分　Spark MLlib推荐算法
第13章　Spark MLlib ALS交替最小二乘算法	310
13.1　ALS交替最小二乘算法	310
13.2　源码分析	312
13.2.1　建立ALS	314
13.2.2　矩阵分解计算	322
13.2.3　ALS模型	329
13.3　实例	334
13.3.1　训练数据	334
13.3.2　实例代码	334
第14章　Spark MLlib协同过滤推荐算法	337
14.1　协同过滤推荐算法	337
14.1.1　协同过滤推荐概述	337
14.1.2　用户评分	338
14.1.3　相似度计算	338
14.1.4　推荐计算	340
14.2　协同推荐算法实现	341
14.2.1　相似度计算	344
14.2.2　协同推荐计算	348
14.3　实例	350
14.3.1　训练数据	350
14.3.2　实例代码	350
第七部分　Spark MLlib神经网络算法
第15章　Spark MLlib神经网络算法综述	354
15.1　人工神经网络算法	354
15.1.1　神经元	354
15.1.2　神经网络模型	355
15.1.3  信号前向传播	356
15.1.4　误差反向传播	357
15.1.5　其他参数	360
15.2　神经网络算法实现	361
15.2.1　神经网络类	363
15.2.2　训练准备	370
15.2.3　前向传播	375
15.2.4　误差反向传播	377
15.2.5　权重更新	381
15.2.6　ANN模型	382
15.3　实例	384
15.3.1　测试数据	384
15.3.2　测试函数代码	387
15.3.3　实例代码	388
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark MLlib机器学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark核心技术与高级应用
前言
基础篇
第1章Spark简介2
1.1什么是Spark2
1.1.1概述3
1.1.2Spark大数据处理框架3
1.1.3Spark的特点4
1.1.4Spark应用场景5
1.2Spark的重要扩展6
1.2.1SparkSQL和DataFrame6
1.2.2SparkStreaming7
1.2.3SparkMLlib和ML8
1.2.4GraphX8
1.2.5SparkR9
1.3本章小结10
第2章Spark部署和运行11
2.1部署准备11
2.1.1下载Spark11
2.1.2编译Spark版本12
2.1.3集群部署概述14
2.2Spark部署15
2.2.1Local模式部署16
2.2.2Standalone模式部署16
2.2.3YARN模式部署18
2.3运行Spark应用程序19
2.3.1Local模式运行Spark应用程序19
2.3.2Standalone模式运行Spark应用程序20
2.3.3YARN模式运行Spark22
2.3.4应用程序提交和参数传递23
2.4本章小结26
第3章Spark程序开发27
3.1使用SparkShell编写程序27
3.1.1启动SparkShell28
3.1.2加载text文件28
3.1.3简单RDD操作28
3.1.4简单RDD操作应用29
3.1.5RDD缓存30
3.2构建Spark的开发环境30
3.2.1准备环境30
3.2.2构建Spark的Eclipse开发环境31
3.2.3构建Spark的IntelliJIDEA开发环境32
3.3独立应用程序编程40
3.3.1创建SparkContext对象40
3.3.2编写简单应用程序40
3.3.3编译并提交应用程序40
3.4本章小结43
第4章编程模型44
4.1RDD介绍44
4.1.1RDD特征45
4.1.2RDD依赖45
4.2创建RDD47
4.2.1集合（数组）创建RDD47
4.2.2存储创建RDD48
4.3RDD操作49
4.3.1转换操作50
4.3.2执行操作52
4.3.3控制操作54
4.4共享变量56
4.4.1广播变量57
4.4.2累加器57
4.5本章小结58
第5章作业执行解析59
5.1基本概念59
5.1.1Spark组件59
5.1.2RDD视图60
5.1.3DAG图61
5.2作业执行流程62
5.2.1基于Standalone模式的Spark架构62
5.2.2基于YARN模式的Spark架构64
5.2.3作业事件流和调度分析65
5.3运行时环境67
5.3.1构建应用程序运行时环境68
5.3.2应用程序转换成DAG68
5.3.3调度执行DAG图70
5.4应用程序运行实例71
5.5本章小结72
第6章SparkSQL与DataFrame73
6.1概述73
6.1.1SparkSQL发展74
6.1.2SparkSQL架构74
6.1.3SparkSQL特点76
6.1.4SparkSQL性能76
6.2DataFrame77
6.2.1DataFrame和RDD的区别78
6.2.2创建DataFrame78
6.2.3DataFrame操作80
6.2.4RDD转化为DataFrame82
6.3数据源84
6.3.1加载保存操作84
6.3.2Parquet文件85
6.3.3JSON数据集88
6.3.4Hive表89
6.3.5通过JDBC连接数据库91
6.3.6多数据源整合查询的小例子92
6.4分布式的SQLEngine93
6.4.1运行ThriftJDBC／ODBC服务93
6.4.2运行SparkSQLCLI94
6.5性能调优94
6.5.1缓存数据94
6.5.2调优参数94
6.5.3增加并行度95
6.6数据类型95
6.7本章小结96
第7章深入了解SparkStreaming97
7.1基础知识97
7.1.1SparkStreaming工作原理98
7.1.2DStream编程模型99
7.2DStream操作100
7.2.1InputDStream100
7.2.2DStream转换操作102
7.2.3DStream状态操作104
7.2.4DStream输出操作106
7.2.5缓存及持久化107
7.2.6检查点108
7.3性能调优109
7.3.1优化运行时间109
7.3.2设置合适的批次大小111
7.3.3优化内存使用111
7.4容错处理112
7.4.1文件输入源112
7.4.2基于Receiver的输入源112
7.4.3输出操作113
7.5一个例子113
7.6本章小结115
第8章SparkMLlib与机器学习116
8.1机器学习概述116
8.1.1机器学习分类117
8.1.2机器学习算法117
8.2SparkMLlib介绍118
8.3SparkMLlib库119
8.3.1MLlib数据类型120
8.3.2MLlib的算法库与实例123
8.4ML库142
8.4.1主要概念143
8.4.2算法库与实例145
8.5本章小结147
第9章GraphX图计算框架与应用148
9.1概述148
9.2SparkGraphX架构149
9.3GraphX编程150
9.3.1GraphX的图操作152
9.3.2常用图算法161
9.4应用场景164
9.4.1图谱体检平台164
9.4.2多图合并工具165
9.4.3能量传播模型165
9.5本章小结166
第10章SparkR（RonSpark）167
10.1概述167
10.1.1SparkR介绍168
10.1.2SparkR的工作原理168
10.1.3R语言介绍169
10.1.4R语言与其他语言的通信170
10.2安装SparkR170
10.2.1安装R语言与rJava171
10.2.2SparkR的安装171
10.3SparkR的运行与应用示例172
10.3.1运行SparkR172
10.3.2SparkR示例程序173
10.3.3R的DataFrame操作方法175
10.3.4SparkR的DataFrame183
10.4本章小结186
实战篇
第11章大数据分析系统188
11.1背景188
11.2数据格式189
11.3应用架构189
11.4业务实现190
11.4.1流量、性能的实时分析190
11.4.2流量、性能的统计分析192
11.4.3业务关联分析193
11.4.4离线报表分析195
11.5本章小结199
第12章系统资源分析平台200
12.1业务背景200
12.1.1业务介绍201
12.1.2实现目标201
12.2应用架构201
12.2.1总体架构202
12.2.2模块架构202
12.3代码实现203
12.3.1Kafka集群203
12.3.2数据采集207
12.3.3离线数据处理207
12.3.4数据表现207
12.4结果验证213
12.5本章小结214
第13章在Spark上训练LR模型215
13.1逻辑回归简介215
13.2数据格式216
13.3MLlib中LR模型源码介绍217
13.3.1逻辑回归分类器217
13.3.2优化方法219
13.3.3算法效果评估221
13.4实现案例223
13.4.1训练模型223
13.4.2计算AUC223
13.5本章小结224
第14章获取二级邻居关系图225
14.1理解PageRank225
14.1.1初步理解PageRank225
14.1.2深入理解PageRank227
14.2PageRank算法基于Spark的实现228
14.3基于PageRank的二级邻居获取232
14.3.1系统设计232
14.3.2系统实现232
14.3.3代码提交命令235
14.4本章小结236
高级篇
第15章调度管理238
15.1调度概述238
15.1.1应用程序间的调度239
15.1.2应用程序中的调度241
15.2调度器242
15.2.1调度池243
15.2.2Job调度流程243
15.2.3调度模块245
15.2.4Job的生与死249
15.3本章小结253
第16章存储管理254
16.1硬件环境254
16.1.1存储系统254
16.1.2本地磁盘255
16.1.3内存255
16.1.4网络和CPU255
16.2Storage模块256
16.2.1通信层256
16.2.2存储层258
16.3Shuffle数据持久化261
16.4本章小结263
第17章监控管理264
17.1Web界面264
17.2SparkUI历史监控266
17.2.1使用spark—server的原因266
17.2.2配置spark—server266
17.3监控工具269
17.3.1Metrics工具269
17.3.2其他工具271
17.4本章小结272
第18章性能调优273
18.1文件的优化273
18.1.1输入采用大文件273
18.1.2lzo压缩处理274
18.1.3Cache压缩275
18.2序列化数据277
18.3缓存278
18.4共享变量278
18.4.1广播变量279
18.4.2累加器279
18.5流水线优化280
18.6本章小结280
扩展篇
第19章Spark—jobserver实践282
19.1Spark—jobserver是什么282
19.2编译、部署及体验283
19.2.1编译及部署283
19.2.2体验286
19.3Spark—jobserver程序实战288
19.3.1创建步骤288
19.3.2一些常见的问题289
19.4使用场景：用户属性分布计算289
19.4.1项目需求290
19.4.2计算架构290
19.4.3使用NamedRDD291
19.5本章小结291
第20章SparkTachyon实战292
20.1Tachyon文件系统292
20.1.1文件系统概述293
20.1.2HDFS和Tachyon294
20.1.3Tachyon设计原理294
20.1.4Tachyon特性295
20.2Tachyon入门295
20.2.1Tachyon部署295
20.2.2TachyonAPI297
20.2.3在Spark上使用Tachyon298
20.3容错机制299
20.4本章小结300
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark核心技术与高级应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>图解Spark：核心技术与案例实战
第一篇 基础篇
第1章  Spark及其生态圈概述
1.1  Spark简介
1.1.1  什么是Spark
1.1.2  Spark与MapReduce比较
1.1.3  Spark的演进路线图
1.2  Spark生态系统
1.2.1  Spark Core
1.2.2  Spark Streaming
1.2.3  Spark SQL
1.2.4  BlinkDB
1.2.5  MLBase/MLlib
1.2.6  GraphX
1.2.7  SparkR
1.2.8  Alluxio
1.3  小结
第2章  搭建Spark实战环境
2.1  基础环境搭建
2.1.1  搭建集群样板机
2.1.2  配置集群环境
2.2  编译Spark源代码
2.2.1  配置Spark编译环境
2.2.2  使用Maven编译Spark
2.2.3  使用SBT编译Spark
2.2.4  生成Spark部署包
2.3  搭建Spark运行集群
2.3.1  修改配置文件
2.3.2  启动Spark
2.3.3  验证启动
2.3.4  第一个实例
2.4  搭建Spark实战开发环境
2.4.1  CentOS中部署IDEA
2.4.2  使用IDEA开发程序
2.4.3  使用IDEA阅读源代码
2.5  小结
第二篇  核心篇
第3章  Spark编程模型
3.1  RDD概述
3.1.1  背景
3.1.2  RDD简介
3.1.3  RDD的类型
3.2  RDD的实现
3.2.1  作业调度
3.2.2  解析器集成
3.2.3  内存管理
3.2.4  检查点支持
3.2.5  多用户管理
3.3  编程接口
3.3.1  RDD分区（Partitions）
3.3.2  RDD首选位置（PreferredLocations）
3.3.3  RDD依赖关系（Dependencies）
3.3.4  RDD分区计算（Iterator）
3.3.5  RDD分区函数（Partitioner）
3.4  创建操作
3.4.1  并行化集合创建操作
3.4.2  外部存储创建操作
3.5  转换操作
3.5.1  基础转换操作
3.5.2  键值转换操作
3.6  控制操作
3.7  行动操作
3.7.1  集合标量行动操作
3.7.2  存储行动操作
3.8  小结
第4章  Spark核心原理
4.1  消息通信原理
4.1.1  Spark消息通信架构
4.1.2  Spark启动消息通信
4.1.3  Spark运行时消息通信
4.2  作业执行原理
4.2.1  概述
4.2.2  提交作业
4.2.3  划分调度阶段
4.2.4  提交调度阶段
4.2.5  提交任务
4.2.6  执行任务
4.2.7  获取执行结果
4.3  调度算法
4.3.1  应用程序之间
4.3.2  作业及调度阶段之间
4.3.3  任务之间
4.4  容错及HA
4.4.1  Executor异常
4.4.2  Worker异常
4.4.3  Master异常
4.5  监控管理
4.5.1  UI监控
4.5.2  Metrics
4.5.3  REST
4.6  实例演示
4.6.1  计算年降水实例
4.6.2  HA配置实例
4.7  小结
第5章  Spark存储原理
5.1  存储分析
5.1.1  整体架构
5.1.2  存储级别
5.1.3  RDD存储调用
5.1.4  读数据过程
5.1.5  写数据过程
5.2  Shuffle分析
5.2.1  Shuffle简介
5.2.2  Shuffle的写操作
5.2.3  Shuffle的读操作
5.3  序列化和压缩
5.3.1  序列化
5.3.2	 压缩
5.4  共享变量
5.4.1  广播变量
5.4.2  累加器
5.5  实例演示
5.6  小结
第6章  Spark运行架构
6.1  运行架构总体介绍
6.1.1  总体介绍
6.1.2  重要类介绍
6.2  本地（Local）运行模式
6.2.1  运行模式介绍
6.2.2  实现原理
6.3  伪分布（Local-Cluster）运行模式
6.3.1  运行模式介绍
6.3.2  实现原理
6.4  独立（Standalone）运行模式
6.4.1  运行模式介绍
6.4.2  实现原理
6.5  YARN运行模式
6.5.1  YARN运行框架
6.5.2  YARN-Client运行模式介绍
6.5.3  YARN-Client 运行模式实现原理
6.5.4  YARN-Cluster运行模式介绍
6.5.5  YARN-Cluster 运行模式实现原理
6.5.6  YARN-Client与YARN-Cluster对比
6.6  Mesos运行模式
6.6.1  Mesos介绍
6.6.2  粗粒度运行模式介绍
6.6.3  粗粒度实现原理
6.6.4  细粒度运行模式介绍
6.6.5  细粒度实现原理
6.6.6  Mesos粗粒度和Mesos细粒度对比
6.7  实例演示
6.7.1  独立运行模式实例
6.7.2  YARN-Client实例
6.7.3  YARN-Cluster实例
6.8  小结
第三篇  组件篇
第7章  Spark SQL
7.1  Spark SQL简介
7.1.1  Spark SQL发展历史
7.1.2  DataFrame/Dataset介绍
7.2  Spark SQL运行原理
7.2.1  通用SQL执行原理
7.2.2  SparkSQL运行架构
7.2.3  SQLContext运行原理分析
7.2.4  HiveContext介绍
7.3  使用Hive-Console
7.3.1  编译Hive-Console
7.3.2  查看执行计划
7.3.3  应用Hive-Console
7.4  使用SQLConsole
7.4.1  启动HDFS和Spark Shell
7.4.2  与RDD交互操作
7.4.3  读取JSON格式数据
7.4.4  读取Parquet格式数据
7.4.5  缓存演示
7.4.6  DSL演示
7.5  使用Spark SQL CLI
7.5.1  配置并启动Spark SQL CLI
7.5.2  实战Spark SQL CLI
7.6  使用Thrift Server
7.6.1  配置并启动Thrift Server
7.6.2  基本操作
7.6.3  交易数据实例
7.6.4  使用IDEA开发实例
7.7  实例演示
7.7.1  销售数据分类实例
7.7.2  网店销售数据统计
7.8  小结
第8章  Spark Streaming
8.1  Spark Streaming简介
8.1.1  术语定义
8.1.2  Spark Streaming特点
8.2  Spark Streaming编程模型
8.2.1  DStream的输入源
8.2.2  DStream的操作
8.3  Spark Streaming运行架构
8.3.1  运行架构
8.3.2  消息通信
8.3.3  Receiver分发
8.3.4  容错性
8.4  Spark Streaming运行原理
8.4.1  启动流处理引擎
8.4.2  接收及存储流数据
8.4.3  数据处理
8.5  实例演示
8.5.1  流数据模拟器
8.5.2  销售数据统计实例
8.5.3  Spark Streaming+Kafka实例
8.6  小结
第9章  Spark MLlib
9.1  Spark MLlib简介
9.1.1  Spark MLlib介绍
9.1.2  Spark MLlib数据类型
9.1.3  Spark MLlib基本统计方法
9.1.4  预言模型标记语言
9.2  线性模型
9.2.1  数学公式
9.2.2  线性回归
9.2.3  线性支持向量机
9.2.4  逻辑回归
9.2.5  线性最小二乘法、Lasso和岭回归
9.2.6  流式线性回归
9.3  决策树
9.4  决策模型组合
9.4.1  随机森林
9.4.2  梯度提升决策树
9.5  朴素贝叶斯
9.6  协同过滤
9.7  聚类
9.7.1  K-means
9.7.2  高斯混合
9.7.3  快速迭代聚类
9.7.4  LDA
9.7.5  二分K-means
9.7.6  流式K-means
9.8  降维
9.8.1  奇异值分解降维
9.8.2  主成分分析降维
9.9  特征提取和变换
9.9.1  词频—逆文档频率
9.9.2  词向量化工具
9.9.3  标准化
9.9.4  范数化
9.10  频繁模式挖掘
9.10.1  频繁模式增长
9.10.2  关联规则挖掘
9.10.3  PrefixSpan
9.11  实例演示
9.11.1  K-means聚类算法实例
9.11.2  手机短信分类实例
9.12  小结
第10章  Spark GraphX
10.1  GraphX介绍
10.1.1  图计算
10.1.2  GraphX介绍
10.1.3  发展历程
10.2  GraphX实现分析
10.2.1  GraphX图数据模型
10.2.2  GraphX图数据存储
10.2.3  GraphX图切分策略
10.2.4  GraphX图操作
10.3  实例演示
10.3.1  图例演示
10.3.2  社区发现演示
10.4  小结
第11章  SparkR
11.1  概述
11.1.1  R语言介绍
11.1.2  SparkR介绍
11.2  SparkR与DataFrame
11.2.1  DataFrames介绍
11.2.2  与DataFrame的相关操作
11.3  编译安装SparkR
11.3.1  编译安装R语言
11.3.2  安装SparkR运行环境
11.3.3  安装SparkR
11.3.4  启动并验证安装
11.4  实例演示
11.5  小结
第12章  Alluxio
12.1  Alluxio简介
12.1.1  Alluxio介绍
12.1.2  Alluxio系统架构
12.1.3  HDFS与Alluxio
12.2  Alluxio编译部署
12.2.1  编译Alluxio
12.2.2  单机部署Alluxio
12.2.3  集群模式部署Alluxio
12.3  Alluxio命令行使用
12.3.1  接口说明
12.3.2  接口操作示例
12.4  实例演示
12.4.1  启动环境
12.4.2  Alluxio上运行Spark
12.4.3  Alluxio上运行MapReduce
12.5  小结
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>图解Spark：核心技术与案例实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark GraphX实战
序言 XI
致谢XIII
关于本书  XIV
关于封面插图 XVIII
第1部分　Spark和图
１　两项重要的技术：Spark和图 3
1.1　Spark：超越Hadoop MapReduce  4
1.1.1　模糊的大数据定义  6
1.1.2　Hadoop：Spark之前的世界  6
1.1.3　Spark：内存中的 MapReduce处理  7
1.2　图：挖掘关系中的含义 9
1.2.1　图的应用  11
1.2.2　图数据的类型  12
1.2.3　普通的关系型数据库在图方面的不足  14
1.3　把快如闪电的图处理放到一起：Spark GraphX  14
1.3.1　图的属性：增加丰富性  15
1.3.2　图的分区：当图变为大数据集时 17
1.3.3　GraphX允许选择：图并行还是数据并行  19
1.3.4　GraphX支持的各种数据处理方式  19
1.3.5　GraphX与其他图系统 21
1.3.6　图存储：分布式文件存储与图数据库  23
1.4　小结 23
2　GraphX快速入门 24
2.1　准备开始并准备数据  24
2.2　用Spark Shell做GraphX交互式查询 26
2.3　PageRank算法示例 29
2.4　小结 31
3　基础知识 32
3.1　Scala—Spark的原生编程语言  33
3.1.1　Scala的理念：简洁和表现力 33
3.1.2　函数式编程 34
3.1.3　类型推断  38
3.1.4　类的声明  39
3.1.5　map和 reduce  41
3.1.6　一切皆是“函数”  42
3.1.7　与 Java的互操作性  44
3.2　Spark  44
3.2.1　分布式内存数据： RDD 44
3.2.2　延迟求值  47
3.2.3　集群要求和术语解释  49
3.2.4　序列化  50
3.2.5　常用的 RDD操作  50
3.2.6　Spark和 SBT初步 54
3.3　图术语解释 55
3.3.1　基础 55
3.3.2　RDF图和属性图 58
3.3.3　邻接矩阵  59
3.3.4　图查询系统 59
3.4　小结 60
第2部分　连接顶点
４　GraphX 基础  65
4.1　顶点对象与边对象 65
4.2　mapping操作  71
4.2.1　简单的图转换  71
4.2.2　Map/Reduce  73
4.2.3　迭代的 Map/Reduce  77
4.3　序列化/反序列化 79
4.3.1　读 /写二进制格式的数据  79
4.3.2　JSON格式  81
4.3.3　Gephi可视化软件的 GEXF格式  85
4.4　图生成  86
4.4.1　确定的图  86
4.4.2　随机图  88
4.5　Pregel API 90
4.6　小结 96
5　内置图算法 97
5.1　找出重要的图节点：网页排名  98
5.1.1　PageRank算法解释  98
5.1.2　在 GraphX中使用 PageRank  99
5.1.3　个性化的 PageRank 102
5.2　衡量连通性：三角形数  103
5.2.1　三角形关系的用法  103
5.2.2　Slashdot朋友和反对者的用户关系示例  104
5.3　查找最少的跳跃：最短路径 106
5.4　找到孤岛人群：连通组件  107
5.4.1　预测社交圈子 108
5.5　受欢迎的回馈：增强连通组件  114
5.6　社区发现算法：标签传播  115
5.7　小结  117
6　其他有用的图算法118
6.1　你自己的GPS：有权值的最短路径  119
6.2　旅行推销员问题：贪心算法 124
6.3　路径规划工具：最小生成树 127
6.3.1　基于 Word2Vec的推导分类法和最小生成树 131
6.4　小结  135
7　机器学习 136
7.1　监督、无监督、半监督学习 137
7.2　影片推荐： SVDPlusPlus. 139
7.2.1　公式解释  146
7.3　在MLlib中使用GraphX 146
7.3.1　主题聚类：隐含狄利克雷分布  147
7.3.2　垃圾信息检测： LogisticRegressionWithSGD 156
7.3.3　使用幂迭代聚类进行图像分割（计算机视觉） 160
7.4　穷人（简化版）的训练数据：基于图的半监督学习  165
7.4.1　K近邻图构建 168
7.4.2　半监督学习标签传播算法  175
7.5　小结  180
第3部分　更多内容
8　缺失的算法  183
8.1　缺失的基本图操作  184
8.1.1　通用意义上的子图  184
8.1.2　图合并 185
8.2　读取RDF图文件 189
8.2.1　顶点匹配以及图构建 189
8.2.2　使用 IndexedRDD和 RDD HashMap来提升性能 191
8.3　穷人（简化版）的图同构：找到Wikipedia缺失的信息  197
8.4　全局聚类系数：连通性比较 202
8.5　小结  205
9　性能和监控  207
9.1　监控Spark应用  208
9.1.1　Spark如何运行应用  208
9.1.2　用 Spark监控来了解你的应用的运行时信息  211
9.1.3　history server  221
9.2　Spark配置  223
9.2.1　充分利用全部 CPU资源 226
9.3　Spark性能调优  227
9.3.1　用缓存和持久化来加速 Spark 227
9.3.2　checkpointing  230
9.3.3　通过序列化降低内存压力  232
9.4　图分区 233
9.5　小结  235
10　更多语言以及工具 237
10.1　在GraphX中使用除Scala外的其他语言  238
10.1.1　在 GraphX中使用 Java 7  238
10.1.2　在 GraphX中使用 Java 8  245
10.1.3　未来 GraphX是否会支持 Python或者 R 245
10.2　其他可视化工具：Apache Zeppelin 和 d3.js 245
10.3　类似一个数据库：Spark Job Server  248
10.3.1　示例：查询 Slashdot好友的分离程度  250
10.3.2　更多使用 Spark Job Server的例子  253
10.4　通过GraphFrames在Spark的图上使用SQL 254
10.4.1　GraphFrames和 GraphX的互操作性 255
10.4.2　使用 SQL进行便捷、高性能的操作 257
10.4.3　使用 Cypher语言的子集来进行顶点搜索  258
10.4.4　稍微复杂一些的 YAGO图同构搜索  260
10.5　小结  264
附录A　安装Spark  266
附录B　Gephi可视化软件  271
附录C　更多资源 275
附录D　本书中的Scala小贴士  278
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark GraphX实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析实战
前 言
第1章 Spark简介 1
1.1 初识Spark 1
1.2 Spark生态系统BDAS 3
1.3 Spark架构与运行逻辑 4
1.4 弹性分布式数据集 6
1.4.1 RDD简介 6
1.4.2 RDD算子分类 8
1.5 本章小结 17
第2章 Spark开发与环境配置 18
2.1 Spark应用开发环境配置 18
2.1.1 使用Intellij开发Spark程序 18
2.1.2 使用SparkShell进行交互式数据分析 23
2.2 远程调试Spark程序 24
2.3 Spark编译 26
2.4 配置Spark源码阅读环境 29
2.5 本章小结 29
第3章 BDAS简介 30
3.1 SQL on Spark 30
3.1.1 为什么使用Spark SQL 31
3.1.2 Spark SQL架构分析 32
3.2 Spark Streaming 35
3.2.1 Spark Streaming简介 35
3.2.2 Spark Streaming架构 38
3.2.3 Spark Streaming原理剖析 38
3.3 GraphX 45
3.3.1 GraphX简介 45
3.3.2 GraphX的使用简介 45
3.3.3 GraphX体系结构 48
3.4 MLlib 50
3.4.1 MLlib简介 50
3.4.2 MLlib中的聚类和分类 52
3.5 本章小结 57
第4章 Lamda架构日志分析流水线 58
4.1 日志分析概述 58
4.2 日志分析指标 61
4.3 Lamda架构 62
4.4 构建日志分析数据流水线 64
4.4.1 用Flume进行日志采集 64
4.4.2 用Kafka将日志汇总 68
4.4.3 用Spark Streaming进行实时日志分析 70
4.4.4 Spark SQL离线日志分析 75
4.4.5 用Flask将日志KPI可视化 78
4.5 本章小结 81
第5章 基于云平台和用户日志的推荐系统 82
5.1 Azure云平台简介 82
5.1.1 Azure网站模型 83
5.1.2 Azure数据存储 84
5.1.3 Azure Queue消息传递 84
5.2 系统架构 85
5.3 构建Node.js应用 86
5.3.1 创建Azure Web应用 87
5.3.2 构建本地Node.js网站 90
5.3.3 发布应用到云平台 90
5.4 数据收集与预处理 91
5.4.1 通过JS收集用户行为日志 92
5.4.2 用户实时行为回传到Azure Queue 94
5.5 Spark Streaming实时分析用户日志 96
5.5.1 构建Azure Queue的Spark Streaming Receiver 96
5.5.2 Spark Streaming实时处理Azure Queue日志 97
5.5.3 Spark Streaming数据存储于Azure Table 98
5.6 MLlib离线训练模型 99
5.6.1 加载训练数据 99
5.6.2 使用rating RDD训练ALS模型 100
5.6.3 使用ALS模型进行电影推荐 101
5.6.4 评估模型的均方差 101
5.7 本章小结 102
第6章 Twitter情感分析 103
6.1 系统架构 103
6.2 Twitter数据收集 104
6.2.1 设置 104
6.2.2 Spark Streaming接收并输出Tweet 109
6.3 数据预处理与Cassandra存储 111
6.3.1 添加SBT依赖 111
6.3.2 创建Cassandra Schema 112
6.3.3 数据存储于Cassandra 112
6.4 Spark Streaming热点Twitter分析 113
6.5 Spark Streaming在线情感分析 115
6.6 Spark SQL进行Twitter分析 118
6.6.1 读取Cassandra数据 118
6.6.2 查看JSON数据模式 118
6.6.3 Spark SQL分析Twitter 119
6.7 Twitter可视化 123
6.8 本章小结 125
第7章 热点新闻分析系统 126
7.1 新闻数据分析 126
7.2 系统架构 126
7.3 爬虫抓取网络信息 127
7.3.1 Scrapy简介 127
7.3.2 创建基于Scrapy的新闻爬虫 128
7.3.3 爬虫分布式化 133
7.4 新闻文本数据预处理 134
7.5 新闻聚类 135
7.5.1 数据转换为向量（向量空间模型VSM） 135
7.5.2 新闻聚类 136
7.5.3 词向量同义词查询 138
7.5.4 实时热点新闻分析 138
7.6 Spark Elastic Search构建全文检索引擎 139
7.6.1 部署Elastic Search 139
7.6.2 用Elastic Search索引MongoDB数据 141
7.6.3 通过Elastic Search检索数据 143
7.7 本章小结 145
第8章 构建分布式的协同过滤推荐系统 146
8.1 推荐系统简介 146
8.2 协同过滤介绍 147
8.2.1 基于用户的协同过滤算法User-based CF 148
8.2.2 基于项目的协同过滤算法Item-based CF 149
8.2.3 基于模型的协同过滤推荐Model-based CF 150
8.3 基于Spark的矩阵运算实现协同过滤算法 152
8.3.1 Spark中的矩阵类型 152
8.3.2 Spark中的矩阵运算 153
8.3.3 实现User-based协同过滤的示例 153
8.3.4 实现Item-based协同过滤的示例 154
8.3.5 基于奇异值分解实现Model-based协同过滤的示例 155
8.4 基于Spark的MLlib实现协同过滤算法 155
8.4.1 MLlib的推荐算法工具 155
8.4.2 MLlib协同过滤推荐示例 156
8.5 案例：使用MLlib协同过滤实现电影推荐 157
8.5.1 MovieLens数据集 157
8.5.2 确定ZUI佳的协同过滤模型参数 158
8.5.3 利用ZUI佳模型进行电影推荐 160
8.6 本章小结 161
第9章 基于Spark的社交网络分析 162
9.1 社交网络介绍 162
9.1.1 社交网络的类型 162
9.1.2 社交网络的相关概念 163
9.2 社交网络中社团挖掘算法 164
9.2.1 聚类分析和K均值算法简介 165
9.2.2 社团挖掘的衡量指标 165
9.2.3 基于谱聚类的社团挖掘算法 166
9.3 Spark中的K均值算法 168
9.3.1 Spark中与K均值有关的对象和方法 168
9.3.2 Spark下K均值算法示例 168
9.4 案例：基于Spark的Facebook社团挖掘 169
9.4.1 SNAP社交网络数据集介绍 169
9.4.2 基于Spark的社团挖掘实现 170
9.5 社交网络中的链路预测算法 172
9.5.1 分类学习简介 172
9.5.2 分类器的评价指标 173
9.5.3 基于Logistic回归的链路预测算法 174
9.6 Spark MLlib中的Logistic回归 174
9.6.1 分类器相关对象 174
9.6.2 模型验证对象 175
9.6.3 基于Spark的Logistic回归示例 175
9.7 案例：基于Spark的链路预测算法 177
9.7.1 SNAP符号社交网络Epinions数据集 177
9.7.2 基于Spark的链路预测算法 177
9.8 本章小结 179
第10章 基于Spark的大规模新闻主题分析 180
10.1 主题模型简介 180
10.2 主题模型LDA 181
10.2.1 LDA模型介绍 181
10.2.2 LDA的训练算法 183
10.3 Spark中的LDA模型 185
10.3.1 MLlib对LDA的支持 185
10.3.2 Spark中LDA模型训练示例 186
10.4 案例：Newsgroups新闻的主题分析 189
10.4.1 Newsgroups数据集介绍 190
10.4.2 交叉验证估计新闻的主题个数 190
10.4.3 基于主题模型的文本聚类算法 193
10.4.4 基于主题模型的文本分类算法 195
10.5 本章小结 196
第11章 构建分布式的搜索引擎 197
11.1 搜索引擎简介 197
11.2 搜索排序概述 198
11.3 查询无关模型PageRank 199
11.4 基于Spark的分布式PageRank实现 200
11.4.1 PageRank的MapReduce实现 200
11.4.2 Spark的分布式图模型GraphX 203
11.4.3 基于GraphX的PageRank实现 203
11.5 案例：GoogleWeb Graph的PageRank计算 204
11.6 查询相关模型Ranking SVM 206
11.7 Spark中支持向量机的实现 208
11.7.1 Spark中的支持向量机模型 208
11.7.2 使用Spark测试数据演示支持向量机的训练 209
11.8 案例：基于MSLR数据集的查询排序 211
11.8.1 Microsoft Learning to Rank数据集介绍 211
11.8.2 基于Spark的Ranking SVM实现 212
11.9 本章小结 213
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>绝佳提问
中文版序 重要的是不要停止提问/ Ⅰ
前    言 我们都渴望答案，但首先要学会提问/ Ⅴ
第一部分 寻回失落的探询能力
01 世界由问题创造，由创意改变/ 003
如果不去探询，你就会对改变充满恐惧。但如果你对探询、尝试和连接各种事物并不感到烦恼，那么改变就会使你更具有冒险精神。而如果你能将改变看成是一种冒险，那么你就能开启一段探询的旅程。探询者获得的每个“答案”都会引发他们提出新一波的问题。
◎ 如果他们能让一个人登上月球，为什么不能制作一只体面的假肢？
◎ 提出问题比给出答案更有价值吗？
◎ 为什么一切都以“为什么”开始？
◎ 怎样将“提问”转化为“行动”？
02 容忍笨问题，才能学聪明/ 033
小孩子的父母总是要不断去回答他们提出的问题。虽然我们口口声声说，佩服孩子的好奇心，但在某些时刻我们就是不欢迎他们提出的问题。但事实上，科学世界的许多领域都会被包含在“一个孩子提出的问题中”。
◎ 为什么我们提问的次数会急剧减少？
◎ 学校能基于问题来进行教学吗？
◎ 如果探询能力与生俱来，那么为什么我们还需要学习？
◎ 我们能自学质疑吗？
第二部分 绝佳提问点燃创新力
03 “为什么”，带着好奇心去发现生活/ 069
“为什么”阶段与发现和理解力有关。“发现”看似简单，好像只需要睁开双眼环顾四周就可以了，但很多时候，只有充满好奇心和天真的孩子才能发现那些别人错过的东西。而一个天真的孩子的问题，可能会改变商业游戏的规则。
◎ 为什么照片不能即时成像？
◎ 为什么退一步反而会帮助我们前进？
◎ 为什么要付费与海豚一起游泳？
◎ 为什么乔治•卡林能看到被我们所忽视的事物？
04 “如果”，放飞你的想象力/ 101
在“如果”阶段，想象力开始发挥作用。长久以来，如果你都沉浸在一个想要解决的难题或问题中，那么你的大脑将会想出一系列可能的解决方案，而这些可能的解决
方案则会最终通向答案。所以，“如果”问题拥有无穷的力量，它们是创新的种子。
◎ 如果我们可以绘制音乐DNA图谱，那会怎么样？
◎ 如果你的大脑是一片枝繁叶茂的森林，那会怎么样？
◎ 如果只用圆点和破折号就能将这个世界分类，那会怎么样？
◎ 如果你带着一个问题入眠，那么当你醒来时会得到答案吗？
05 “怎样”，让看似不切实际的想法落地/ 121
很多时候，除非我们的想法已经非常成熟、臻于完美了，否则我们不会立刻尝试，不会分享给他人以获取反馈，也不会考虑其可行性。但是，过分思考和过度准备并不能自然而然地促使人们去探索和创造，你应该快速勇敢地从“如果”迈向“怎样”。
◎ 怎样才能描绘出自己的想法？
◎ 怎样搭建一座不倒塌的高塔？
◎ 怎样学会去爱一只受伤的脚？
◎ 怎样才能摆脱电源线的束缚？
第三部分 探询改变商业，探询改变生活
06 最具创意的领袖，专家级的探询者/ 145
当今商业领域的竞争愈加激烈，为应对这一变化，很多公司的领导者正逐渐意识到，如果他们仅仅提出一些不重要的问题，就无法推进他们的日常工作取得进展，无法提高他们所处的地位，也无法提升他们的品牌竞争力。而如果要创新，他们就必须提出更多更有价值的问题。
◎ 如果我们的公司没有存在过，那会怎么样？
◎ 有人会愿意追随一位“不靠谱”的领导吗？
◎ 企业的使命宣言都应该包括一些有关使命的问题吗？
◎ 怎样才能创建一种探询文化？
07 拥抱未知，追逐自己的好奇心/ 189
当今世界，到处都是奇怪的难题，当然也有很多美丽问题。这就意味着，最棘手的问题往往就隐藏在某些看似普通但实际上极具价值的问题中。而如果我们发现了这些最棘手的问题，并不断围绕其进行思考，我们就可能会更清晰地看到问题的本质。
◎ 你为什么要回避探询？
◎ 如果你做出一点小改变，那会怎么样？
◎ 如果你不会失败，那会怎么样？
◎ 我们如何才能突破限制，绘出新篇章？
译者后记/  233
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>绝佳提问
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark:原理、机制及应用
前言
第一篇概念篇
第1章Spark概述
1.1Spark初见
1.1.1Spark的发展史及近况
1.1.2Spark的特点
1.1.3Spark的作用
1.1.4Spark的体系结构
1.1.5Spark的发展趋势
1.2Spark框架
1.2.1批处理框架
1.2.2流处理框架
1.3Spark的生态系统
1.4Spark的数据存储
1.5本章小结
第2章Spark环境配置
2.1Spark运行环境配置
2.1.1先决条件
2.1.2下载与运行Spark
2.1.3使用交互式Shell
2.1.4搭建Spark Standalone集群
2.2Spark开发环境配置
2.2.1Spark独立应用程序
2.2.2构建IDE开发环境
2.3Spark编译环境配置
2.3.1使用Maven编译项目源码
2.3.2使用IDEA搭建源码编译与阅读环境
2.4本章小结
第二篇开发篇
第3章Spark核心开发
3.1Spark编程模型概述
3.2SparkContext
3.2.1SparkContext的作用
3.2.2SparkContext的创建
3.2.3使用Shell
3.2.4应用实践
3.3RDD简介
3.3.1RDD创建
3.3.2RDD转换操作
3.3.3RDD动作操作
3.3.4RDD惰性计算
3.3.5RDD持久化
3.3.6RDD检查点
3.4共享变量
3.4.1广播变量
3.4.2累加器
3.5Spark核心开发实践
3.5.1单值型Trasnformation算子
3.5.2键值对型Transformation算子
3.5.3Action算子
3.6本章小结
第4章Spark四大应用技术框架
4.1Spark SQL
4.1.1Spark SQL入门
4.1.2数据源
4.1.3性能调优
4.1.4分布式SQL引擎
4.1.5Shark迁移至SparkSQL指南
4.1.6Hive的兼容性
4.1.7Spark SQL数据类型
4.2Spark Streaming
4.2.1Spark Streaming简介
4.2.2入门实例
4.2.3基本概念
4.3Spark GraphX
4.3.1Spark GraphX简介
4.3.2属性图
4.3.3图操作
4.3.4Pregel API
4.3.5图构造器
4.3.6顶点与边相关RDD
4.3.7最优化表示
4.3.8图算法
4.3.9Ex锄ple
4.4Spark Mllib
4.4.1Spark Mllib简介
4.4.2数据类型
4.4.3基本统计分析
4.4.4分类与回归
4.4.5协同过滤
4.4.6聚类
4.4.7降维
4.4.8特征提取与转换
4.4.9频繁模式挖掘
4.4.10最优化算法
4.4.11导出PMMl模式
4.5SparkR
4.5.1SparkR DataFrame
4.5.2DataFrame的相关操作
4.5.3从SparkR运行SQL查询
第5章Spark系统配置与调优
5.1Spark运行监控
5.2Spark配置参数
5.2.1应用属性
5.2.2运行环境属性
5.2.3Shuffle操作属性
5.2.4压缩与序列化属性
5.2.5数据序列化
5.3内存调优
5.3.1调整数据结构
5.3.2序列化RDD存储
5.3.3GC
5.4其他调优
5.4.1并行度
5.4.2Reduce任务
5.4.3广播变量
5.4.4数据本地化
5.4.5网络通信调优
5.4.6磁盘空间优化
5.4.7任务执行速度“倾斜”
5.5本童小结
第三篇机制篇
第6章RDD内部结构
6.1RDD接口
6.2分区
6.2.1分区接口
6.2.2分区个数
6.2.3分区内部的记录个数
6.3依赖关系
6.3.1依赖与RDD
6.3.2依赖分类
6.3.3窄依赖
6.3.4Shuffle依赖
6.3.5依赖与容错机制
6.3.6依赖与并行计算
6.4计算函数
6.4.1compute方法
6.4.2iterator方法
6.5分区器
6.5.1哈希分区器
6.5.2范围分区器
6.5.3默认分区器
6.6持久化
6.7检查点
6.8本章小结
第7章Spark调度机制
7.1调度基础
7.1.1基本概念
7.1.2通信框架
7.2集群资源调度
7.2.1集群部署图
7.2.2集群资源注册
7.2.3集群资源申请与分配
7.3DAG调度
7.3.1DAG调度通信机制
7.3.2作业处理流程
7.3.3阶段划分
7.4任务调度
7.4.1任务分类与执行
7.4.2任务划分与提交
7.4.3任务调度算法
7.4.4任务调度相关类
7.4.5任务分配
7.4.6任务接收与执行
7.5本章小结
第8章Shuffle过程
8.1与Hadoop Shuffle过程的区别
8.1.1MR模型的Shuffle过程
8.1.2聚合器
8.1.3哈希Shuffle与排序Shuffle
8.1.4Spark的Shufne过程
8.2Shume写过程
8.2.1哈希Shuffle写过程
8.2.2排序Shuffle写过程
8.3Shume读过程
8.4本章小结
第四篇应用篇
第9章视频娱乐领域
9.1腾讯公司在Hadoop和Spark平台上的应用
9.1.1公司背景特点
9.1.2业务需求
9.1.3解决方案
9.1.4方案效果
9.1.5小结
9.2Spotify公司在Hadoop和Spark平台AlS算法的运行时间对比
9.2.1公司背景特点
9.2.2业务需求
9.2.3解决方案
9.2.4方案效果
9.2.5小结
9.3本章小结
第10章电商领域
10.1淘 宝公司在Spark平台上对GraphX与Bagel的运行效果对比
10.1.1公司背景特点
10.1.2业务需求
10.1.3解决方案
10.1.4方案效果
10.1.5小结
10.2Yahoo！关于Hive与Shark的应用
10.2.1公司背景特点
10.2.2业务需求
10.2.3解决方案
10.2.4方案效果
10.2.5小结
10.3本章小结
第11章电信领域
11.1Telefonica应用Spark和Cassandra方案解决多用户事务查询
11.1.1公司背景特点
11.1.2业务需求
11.1.3解决方案
11.1.4方案效果
11.1.5小结
11.2NTTDATA对Spark on YARN架构各项性能测试分析
11.2.1公司背景特点
11.2.2业务需求
11.2.3解决方案
11.2.4方案效果
11.2.5小结
11.3本章小结
第12章零售领域
12.1Euclid Analysis基于Spark的地理位置分析服务
12.1.1公司背景特点
12.1.2业务需求
12.1.3解决方案
12.1.4方案效果
12.1.5小结
12.2Graphflow应用Spark Mllib进行实时个性化推荐
12.2.1公司背景特点
12.2.2业务需求
12.2.3解决方案
12.2.4方案效果
12.2.5小结
12.3本章小结
第13章其他领域
13.1Uber基于Spark的私家车搭乘服务
13.1.1公司背景特点
13.1.2业务需求
13.1.3解决方案
13.1.4方案效果
13.1.5小结
13.2PubMatic应用Spark提供广告服务
13.2.1公司背景特点
13.2.2业务需求
13.2.3解决方案
13.2.4方案效果
13.2.5小结
13.3本章小结
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark:原理、机制及应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度实践spark机器学习
前言
第1章　了解机器学习 1
1.1　机器学习的定义 1
1.2　大数据与机器学习 2
1.3　机器学习、人工智能及深度学习 2
1.4　机器学习的基本任务 3
1.5　如何选择合适算法 4
1.6　Spark在机器学习方面的优势 5
1.7　小结 5
第2章　构建Spark机器学习系统 6
2.1　机器学习系统架构 6
2.2　启动集群 7
2.3　加载数据 9
2.4　探索数据 10
2.4.1　数据统计信息 10
2.4.2　数据质量分析 11
2.4.3　数据特征分析 12
2.4.4　数据的可视化 17
2.5　数据预处理 19
2.5.1　数据清理 20
2.5.2　数据变换 21
2.5.3　数据集成 22
2.5.4　数据归约 23
2.6　构建模型 25
2.7　模型评估 26
2.8　组装 30
2.9　模型选择或调优 30
2.9.1　交叉验证 31
2.9.2　训练–验证切分 32
2.10　保存模型 32
2.11　小结 33
第3章　ML Pipeline原理与实战 34
3.1　Pipeline简介 34
3.2　DataFrame 35
3.3　Pipeline组件 36
3.4　Pipeline原理 37
3.5　Pipeline实例 38
3.5.1　使用Estimator、Transformer和Param的实例 38
3.5.2　ML使用Pipeline的实例 40
3.6　小结 41
第4章　特征提取、转换和选择 42
4.1　特征提取 42
4.1.1　词频—逆向文件
频率（TF-IDF） 42
4.1.2　Word2Vec 43
4.1.3　计数向量器 44
4.2　特征转换 45
4.2.1　分词器 45
4.2.2　移除停用词 46
4.2.3　n-gram 47
4.2.4　二值化 48
4.2.5　主成分分析 48
4.2.6　多项式展开 50
4.2.7　离散余弦变换 50
4.2.8　字符串—索引变换 51
4.2.9　 索引—字符串变换 53
4.2.10　独热编码 54
4.2.11　向量—索引变换 57
4.2.12　交互式 58
4.2.13　正则化 59
4.2.14　规范化 60
4.2.15　最大值—最小值缩放 60
4.2.16　最大值—绝对值缩放 61
4.2.17　离散化重组 62
4.2.18　元素乘积 63
4.2.19　SQL转换器 64
4.2.20　向量汇编 65
4.2.21　分位数离散化 66
4.3　特征选择 67
4.3.1　向量机 67
4.3.2　R公式 69
4.3.3　卡方特征选择 70
4.4　小结 71
第5章　模型选择和优化 72
5.1　模型选择 72
5.2　交叉验证 73
5.3　训练验证拆分法 75
5.4　自定义模型选择 76
5.5　小结 78
第6章　Spark MLlib基础 79
6.1　Spark MLlib简介 80
6.2　Spark MLlib架构 81
6.3　数据类型 82
6.4　基础统计 84
6.4.1　摘要统计 84
6.4.2　相关性 84
6.4.3　假设检验 85
6.4.4　随机数据生成 85
6.5　RDD、Dataframe和Dataset 86
6.5.1　RDD 86
6.5.2　DatasetDataFrame 87
6.5.3　相互转换 88
6.6　小结 89
第7章　构建Spark ML推荐模型 90
7.1　推荐模型简介 91
7.2　数据加载 92
7.3　数据探索 94
7.4　训练模型 94
7.5　组装 95
7.6　评估模型 96
7.7　模型优化 96
7.8　小结 98
第8章　构建Spark ML分类模型 99
8.1　分类模型简介 99
8.1.1　线性模型 100
8.1.2　决策树模型 101
8.1.3　朴素贝叶斯模型 102
8.2　数据加载 102
8.3　数据探索 103
8.4　数据预处理 104
8.5　组装 109
8.6　模型优化 110
8.7　小结 113
第9章　构建Spark ML回归模型 114
9.1　回归模型简介 115
9.2　数据加载 115
9.3　探索特征分布 117
9.4　数据预处理 120
9.4.1　特征选择 121
9.4.2　特征转换 121
9.5　组装 122
9.6　模型优化 124
9.7　小结 126
第10章　构建Spark ML聚类模型 127
10.1　K-means模型简介 128
10.2　数据加载 129
10.3　探索特征的相关性 129
10.4　数据预处理 131
10.5　组装 132
10.6　模型优化 134
10.7　小结 136
第11章　PySpark 决策树模型 137
11.1　PySpark 简介 138
11.2　决策树简介 139
11.3　数据加载 140
11.3.1　原数据集初探 140
11.3.2　PySpark的启动 142
11.3.3　基本函数 142
11.4　数据探索 143
11.5　数据预处理 143
11.6　创建决策树模型 145
11.7　训练模型进行预测 146
11.8　模型优化 149
11.8.1　特征值的优化 149
11.8.2　交叉验证和网格参数 152
11.9　脚本方式运行 154
11.9.1　在脚本中添加配置信息 154
11.9.2　运行脚本程序 154
11.10　小结 154
第12章　SparkR朴素贝叶斯模型 155
12.1　SparkR简介 156
12.2　获取数据 157
12.2.1　SparkDataFrame数据结构
说明 157
12.2.2　创建SparkDataFrame 157
12.2.3　SparkDataFrame的常用操作 160
12.3　朴素贝叶斯分类器 162
12.3.1　数据探查 162
12.3.2　对原始数据集进行转换 163
12.3.3　查看不同船舱的生还率差异 163
12.3.4　转换成SparkDataFrame格式的数据 165
12.3.5　模型概要 165
12.3.6　预测 165
12.3.7　评估模型 166
12.4　小结 167
第13章　使用Spark Streaming构建在线学习模型 168
13.1　Spark Streaming简介 168
13.1.1　Spark Streaming常用术语 169
13.1.2　Spark Streaming处理流程 169
13.2　Dstream操作
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深度实践spark机器学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python+Spark 2.0+Hadoop机器学习与大数据实战
目   录


第1章  Python Spark机器学习与Hadoop大数据     1
1.1  机器学习的介绍   2
1.2 Spark的介绍 5
1.3 Spark数据处理 RDD、DataFrame、Spark SQL  7
1.4  使用Python开发 Spark机器学习与大数据应用       8
1.5 Python Spark 机器学习         9
1.6 Spark ML Pipeline机器学习流程介绍  10
1.7 Spark 2.0的介绍    12
1.8  大数据定义   13
1.9 Hadoop 简介          14
1.10 Hadoop HDFS分布式文件系统  14
1.11 Hadoop MapReduce的介绍        17
1.12 结论      18
第2章  VirtualBox虚拟机软件的安装        19
2.1 VirtualBox的下载和安装      20
2.2  设置VirtualBox存储文件夹          23
2.3  在VirtualBox创建虚拟机     25
2.4  结论        29
第3章  Ubuntu Linux 操作系统的安装      30
3.1 Ubuntu Linux 操作系统的安装    31
3.2  在Virtual设置Ubuntu虚拟光盘文件 33
3.3  开始安装Ubuntu  35
3.4  启动Ubuntu  40
3.5  安装增强功能        41
3.6  设置默认输入法   45
3.7  设置“终端”程序        48
3.8  设置“终端”程序为白底黑字   49
3.9  设置共享剪贴板   50
3.10 设置最佳下载服务器 52
3.11 结论      56
第4章  Hadoop Single Node Cluster的安装        57
4.1  安装JDK         58
4.2  设置SSH无密码登录    61
4.3  下载安装Hadoop  64
4.4  设置Hadoop环境变量 67
4.5  修改Hadoop配置设置文件 69
4.6  创建并格式化HDFS目录     73
4.7  启动Hadoop  74
4.8  打开HadoopResource-Manager Web界面 76
4.9 NameNode HDFS Web界面   78
4.10 结论      79
第5章  Hadoop Multi Node Cluster的安装         80
5.1  把Single NodeCluster复制到data1    83
5.2  设置VirtualBox网卡     84
5.3  设置data1服务器         87
5.4  复制data1服务器到data2、data3、master     94
5.5  设置data2服务器         97
5.6  设置data3服务器         100
5.7  设置master服务器      102
5.8 master连接到data1、data2、data3 创建HDFS目录      107
5.9  创建并格式化NameNodeHDFS目录  110
5.10 启动Hadoop Multi Node Cluster         112
5.11 打开Hadoop ResourceManager Web界面         114
5.12 打开NameNode Web界面 115
5.13 停止Hadoop Multi Node Cluster         116
5.14 结论      116
第 6 章  Hadoop HDFS命令        117
6.1  启动HadoopMulti-Node Cluster  118
6.2  创建与查看HDFS目录 120
6.3  从本地计算机复制文件到HDFS  122
6.4  将HDFS上的文件复制到本地计算机 127
6.5  复制与删除HDFS文件 129
6.6  在Hadoop HDFSWeb用户界面浏览HDFS  131
6.7  结论        134
第7章  Hadoop MapReduce         135
7.1  简单介绍WordCount.java     136
7.2  编辑WordCount.java     137
7.3  编译WordCount.java     141
7.4  创建测试文本文件        143
7.5  运行WordCount.java     145
7.6  查看运行结果        146
7.7  结论        147
第8章  Python Spark的介绍与安装   148
8.1 Scala的介绍与安装       150
8.2  安装Spark      153
8.3  启动pyspark交互式界面     156
8.4  设置pyspark显示信息 157
8.5  创建测试用的文本文件        159
8.6  本地运行pyspark程序 161
8.7  在Hadoop YARN运行pyspark      163
8.8  构建SparkStandalone Cluster运行环境      165
8.9  在SparkStandalone运行pyspark         171
8.10 Spark Web UI界面        173
8.11 结论      175
第9章  在 IPythonNotebook 运行 Python Spark 程序   176
9.1  安装Anaconda       177
9.2  在IPythonNotebook使用Spark   180
9.3  打开IPythonNotebook笔记本     184
9.4  插入程序单元格   185
9.5  加入注释与设置程序代码说明标题   186
9.6  关闭IPythonNotebook 188
9.7  使用IPythonNotebook在Hadoop YARN-client模式运行   189
9.8  使用IPythonNotebook在Spark Stand Alone模式运行       192
9.9  整理在不同的模式运行IPythonNotebook的命令     194
9.9.1 在 Local 启动 IPython Notebook     195
9.9.2 在Hadoop YARN-client 模式启动 IPython Notebook       195
9.9.3 在Spark Stand Alone 模式启动 IPython Notebook 195
9.10 结论      196
第10章  Python Spark RDD 197
10.1 RDD的特性 198
10.2 开启IPython Notebook        199
10.3 基本RDD“转换”运算     201
10.4 多个RDD“转换”运算     206
10.5 基本“动作”运算      208
10.6 RDD Key-Value 基本“转换”运算   209
10.7 多个RDD Key-Value“转换”运算     212
10.8 Key-Value“动作”运算      215
10.9 Broadcast 广播变量   217
10.10 accumulator累加器  220
10.11 RDD Persistence持久化   221
10.12 使用Spark创建WordCount      223
10.13 Spark WordCount详细解说       226
10.14 结论   228
第11章  Python Spark的集成开发环境     229
11.1 下载与安装eclipse Scala IDE      232
11.2 安装PyDev  235
11.3 设置字符串替代变量 240
11.4 PyDev 设置 Python 链接库       243
11.5 PyDev设置anaconda2链接库路径   245
11.6 PyDev设置Spark Python链接库         247
11.7 PyDev设置环境变量  248
11.8 新建PyDev项目 251
11.9 加入WordCount.py程序     253
11.10 输入WordCount.py程序  254
11.11 创建测试文件并上传至HDFS目录 257
11.12 使用spark-submit执行WordCount程序         259
11.13 在Hadoop YARN-client上运行WordCount程序      261
11.14 在Spark Standalone Cluster上运行WordCount程序      264
11.15 在eclipse外部工具运行Python Spark程序    267
11.16 在eclipse运行spark-submit YARN-client          273
11.17 在eclipse运行spark-submit Standalone 277
11.18 结论   280
第12章  Python Spark创建推荐引擎 281
12.1 推荐算法介绍      282
12.2 “推荐引擎”大数据分析使用场景 282
12.3 ALS推荐算法的介绍   283
12.4 如何搜索数据      285
12.5 启动IPython Notebook        289
12.6 如何准备数据      290
12.7 如何训练模型      294
12.8 如何使用模型进行推荐      295
12.9 显示推荐的电影名称 297
12.10 创建Recommend项目      299
12.11 运行RecommendTrain.py 推荐程序代码        302
12.12 创建Recommend.py推荐程序代码         304
12.13 在eclipse运行Recommend.py         307
12.14 结论   310
第13章  Python Spark MLlib决策树二元分类   311
13.1 决策树介绍 312
13.2 “StumbleUpon Evergreen”大数据问题  313
13.2.1 Kaggle网站介绍       313
13.2.2 “StumbleUpon Evergreen”大数据问题场景分析        313
13.3 决策树二元分类机器学习 314
13.4 如何搜集数据      315
13.4.1 StumbleUpon数据内容    315
13.4.2 下载 StumbleUpon 数据         316
13.4.3 用LibreOffice Calc 电子表格查看train.tsv    319
13.4.4 复制到项目目录       322
13.5  使用IPython Notebook示范       323
13.6 如何进行数据准备      324
13.6.1 导入并转换数据       324
13.6.2 提取 feature 特征字段  327
13.6.3 提取分类特征字段  328
13.6.4 提取数值特征字段  331
13.6.5 返回特征字段  331
13.6.6 提取 label 标签字段       331
13.6.7 建立训练评估所需的数据       332
13.6.8 以随机方式将数据分为 3 部分并返回         333
13.6.9 编写 PrepareData(sc) 函数    333
13.7 如何训练模型      334
13.8 如何使用模型进行预测      335
13.9 如何评估模型的准确率      338
13.9.1 使用 AUC 评估二元分类模型        338
13.9.2 计算 AUC 339
13.10 模型的训练参数如何影响准确率   341
13.10.1 建立 trainEvaluateModel       341
13.10.2 评估impurity参数 343
13.10.3 训练评估的结果以图表显示         344
13.10.4 编写 evalParameter       347
13.10.5 使用 evalParameter 评估 maxDepth 参数        347
13.10.6 使用 evalParameter 评估 maxBins 参数  348
13.11 如何找出准确率最高的参数组合   349
13.12 如何确认是否过度训练   352
13.13 编写RunDecisionTreeBinary.py程序        352
13.14 开始输入RunDecisionTreeBinary.py程序        353
13.15 运行RunDecisionTreeBinary.py         355
13.15.1 执行参数评估         355
13.15.2 所有参数训练评估找出最好的参数组合    355
13.15.3 运行 RunDecisionTreeBinary.py 不要输入参数  357
13.16 查看DecisionTree的分类规则          358
13.17 结论   360
第14章  Python Spark MLlib 逻辑回归二元分类       361
14.1 逻辑回归分析介绍      362
14.2 RunLogisticRegression WithSGDBinary.py程序说明 363
14.3 运行RunLogisticRegression WithSGDBinary.py进行参数评估          367
14.4 找出最佳参数组合      370
14.5 修改程序使用参数进行预测      370
14.6 结论      372
第15章  Python Spark MLlib支持向量机SVM二元分类  373
15.1 支持向量机SVM算法的基本概念    374
15.2 运行SVMWithSGD.py进行参数评估          376
15.3 运行SVMWithSGD.py 训练评估参数并找出最佳参数组合    378
15.4 运行SVMWithSGD.py 使用最佳参数进行预测        379
15.5 结论      381
第16章  Python Spark MLlib朴素贝叶斯二元分类   382
16.1 朴素贝叶斯分析原理的介绍      383
16.2 RunNaiveBayesBinary.py程序说明     384
16.3 运行NaiveBayes.py进行参数评估    386
16.4 运行训练评估并找出最好的参数组合      387
16.5 修改RunNaiveBayesBinary.py 直接使用最佳参数进行预测  388
16.6 结论      390
第17章  Python Spark MLlib决策树多元分类   391
17.1 “森林覆盖植被”大数据问题分析场景 392
17.2 UCI Covertype数据集介绍 393
17.3 下载与查看数据 394
17.4 修改PrepareData() 数据准备   396
17.5 修改trainModel 训练模型程序         398
17.6 使用训练完成的模型预测数据 399
17.7 运行RunDecisionTreeMulti.py 进行参数评估  401
17.8 运行RunDecisionTreeMulti.py 训练评估参数并找出最好的参数组合  403
17.9 运行RunDecisionTreeMulti.py 不进行训练评估      404
17.10 结论   406
第18章  Python Spark MLlib决策树回归分析   407
18.1 Bike Sharing大数据问题分析     408
18.2 Bike Sharing数据集     409
18.3 下载与查看数据 409
18.4 修改 PrepareData() 数据准备  412
18.5 修改DecisionTree.trainRegressor训练模型      415
18.6 以 RMSE 评估模型准确率         416
18.7 训练评估找出最好的参数组合 417
18.8 使用训练完成的模型预测数据 417
18.9 运行RunDecisionTreeMulti.py进行参数评估   419
18.10 运行RunDecisionTreeMulti.py训练评估参数并找出最好的参数组合 421
18.11 运行RunDecisionTreeMulti.py 不进行训练评估    422
18.12 结论   424
第19章  Python Spark SQL、DataFrame、RDD数据统计与可视化         425
19.1 RDD、DataFrame、Spark SQL 比较  426
19.2 创建RDD、DataFrame与Spark SQL 427
19.2.1 在 local 模式运行 IPython Notebook    427
19.2.2 创建RDD  427
19.2.3 创建DataFrame        428
19.2.4 设置 IPython Notebook 字体 430
19.2.5 为DataFrame 创建别名 431
19.2.6 开始使用 Spark SQL         431
19.3 SELECT显示部分字段          434
19.3.1 使用 RDD 选取显示部分字段       434
19.3.2 使用 DataFrames 选取显示字段  434
19.3.3 使用 Spark SQL 选取显示字段       435
19.4 增加计算字段      436
19.4.1 使用 RDD 增加计算字段       436
19.4.2 使用 DataFrames 增加计算字段  436
19.4.3 使用 Spark SQL 增加计算字段       437
19.5 筛选数据      438
19.5.1 使用 RDD 筛选数据       438
19.5.2 使用 DataFrames 筛选数据  438
19.5.3 使用 Spark SQL 筛选数据       439
19.6 按单个字段给数据排序      439
19.6.1 RDD 按单个字段给数据排序          439
19.6.2 使用 Spark SQL排序        440
19.6.3 使用 DataFrames按升序给数据排序   441
19.6.4 使用 DataFrames按降序给数据排序   442
19.7 按多个字段给数据排序      442
19.7.1 RDD 按多个字段给数据排序          442
19.7.2 Spark SQL 按多个字段给数据排序         443
19.7.3 DataFrames 按多个字段给数据排序    443
19.8 显示不重复的数据      444
19.8.1 RDD 显示不重复的数据          444
19.8.2 Spark SQL 显示不重复的数据         445
19.8.3 Dataframes显示不重复的数据      445
19.9 分组统计数据      446
19.9.1 RDD 分组统计数据          446
19.9.2 Spark SQL分组统计数据 447
19.9.3 Dataframes分组统计数据      448
19.10 Join 联接数据   450
19.10.1 创建 ZipCode 450
19.10.2 创建 zipcode_tab  452
19.10.3 Spark SQL 联接 zipcode_table 数据表         454
19.10.4 DataFrame user_df 联接 zipcode_df   455
19.11 使用 Pandas DataFrames 绘图       457
19.11.1 按照不同的州统计并以直方图显示    457
19.11.2 按照不同的职业统计人数并以圆饼图显示         459
19.12 结论   461
第20章  Spark ML Pipeline 机器学习流程二元分类         462
20.1 数据准备      464
20.1.1 在 local 模式执行 IPython Notebook    464
20.1.2 编写 DataFrames UDF 用户自定义函数       466
20.1.3 将数据分成 train_df 与 test_df    468
20.2 机器学习pipeline流程的组件   468
20.2.1 StringIndexer     468
20.2.2 OneHotEncoder         470
20.2.3 VectorAssembler       472
20.2.4 使用 DecisionTreeClassi?er 二元分类  474
20.3 建立机器学习pipeline流程        475
20.4 使用pipeline进行数据处理与训练   476
20.5 使用pipelineModel 进行预测    477
20.6 评估模型的准确率      478
20.7 使用TrainValidation进行训练验证找出最佳模型    479
20.8 使用crossValidation交叉验证找出最佳模型   481
20.9 使用随机森林 RandomForestClassi?er分类器         483
20.10 结论   485
第21章  Spark ML Pipeline 机器学习流程多元分类         486
21.1 数据准备      487
21.1.1 读取文本文件  488
21.1.2  创建 DataFrame      489
21.1.3 转换为 double 490
21.2 建立机器学习pipeline流程        492
21.3 使用dt_pipeline进行数据处理与训练      493
21.4 使用pipelineModel 进行预测    493
21.5 评估模型的准确率      495
21.4 使用TrainValidation进行训练验证找出最佳模型    496
21.7 结论      498
第22章  Spark ML Pipeline 机器学习流程回归分析         499
22.1 数据准备      501
22.1.1 在local 模式执行 IPython Notebook     501
22.1.2 将数据分成 train_df 与 test_df    504
22.2 建立机器学习pipeline流程        504
22.3 使用dt_pipeline进行数据处理与训练      506
22.4 使用pipelineModel 进行预测    506
22.5 评估模型的准确率      507
22.6 使用TrainValidation进行训练验证找出最佳模型    508
22.7 使用crossValidation进行交叉验证找出最佳模型   510
22.8 使用GBT Regression   511
22.9 结论      513
附录A  本书范例程序下载与安装说明      514
A.1  下载范例程序        515
A.2  打开本书IPythonNotebook范例程序         516
A.3  打开 eclipsePythonProject 范例程序         518
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Python+Spark 2.0+Hadoop机器学习与大数据实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实时大数据分析
目 录
第1章 大数据技术前景及分析平台 1
1.1 大数据的概念 1
1.2 大数据的维度范式 2
1.3 大数据生态系统 3
1.4 大数据基础设施 4
1.5 大数据生态系统组件 5
1.5.1 构建业务解决方案 8
1.5.2 数据集处理 8
1.5.3 解决方案实施 8
1.5.4 呈现 9
1.6 分布式批处理 9
1.7 分布式数据库（NoSQL） 13
1.7.1 NoSQL数据库的优势 15
1.7.2 选择NoSQL数据库 16
1.8 实时处理 16
1.8.1 电信或移动通信场景 17
1.8.2 运输和物流 17
1.8.3 互联的车辆 18
1.8.4 金融部门 18
1.9 本章小结 18
第2章 熟悉Storm 19
2.1 Storm概述 19
2.2 Storm的发展 20
2.3 Storm的抽象概念 22
2.3.1 流 22
2.3.2 拓扑 22
2.3.3 Spout 23
2.3.4 Bolt 23
2.3.5 任务 24
2.3.6 工作者 25
2.4 Storm的架构及其组件 25
2.4.1 Zookeeper集群 25
2.4.2 Storm集群 25
2.5 如何以及何时使用Storm 27
2.6 Storm的内部特性 32
2.6.1 Storm的并行性 32
2.6.2 Storm的内部消息处理 34
2.7 本章小结 36
第3章 用Storm处理数据...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>实时大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据商业实战三部曲
上篇 内核解密
第1章 电光石火间体验Spark 2.2开发实战... 2
1.1 通过RDD实战电影点评系统入门及源码阅读... 2
1.1.1 Spark核心概念图解... 2
1.1.2 通过RDD实战电影点评系统案例... 4
1.2 通过DataFrame和DataSet实战电影点评系统... 7
1.2.1 通过DataFrame实战电影点评系统案例... 7
1.2.2 通过DataSet实战电影点评系统案例... 10
1.3 Spark 2.2源码阅读环境搭建及源码阅读体验... 11
第2章 Spark 2.2技术及原理... 14
2.1 Spark 2.2综述... 14
2.1.1 连续应用程序... 14
2.1.2 新的API 15
2.2 Spark 2.2 Core. 16
2.2.1 第二代Tungsten引擎... 16
2.2.2 SparkSession. 16
2.2.3 累加器API 17
2.3 Spark 2.2 SQL. 19
2.3.1 Spark SQL. 20
2.3.2 DataFrame和Dataset API 20
2.3.3 Timed Window.. 21
2.4 Spark 2.2 Streaming. 21
2.4.1 Structured Streaming. 21
2.4.2 增量输出模式... 23
2.5 Spark 2.2 MLlib. 27
2.5.1 基于DataFrame的Machine Learning API 28
2.5.2 R的分布式算法... 28
2.6 Spark 2.2 GraphX.. 29
第3章 Spark的灵魂：RDD和DataSet 30
3.1 为什么说RDD和DataSet是Spark的灵魂... 30
3.1.1 RDD的定义及五大特性剖析... 30
3.1.2 DataSet的定义及内部机制剖析... 34
3.2 RDD弹性特性七个方面解析... 36
3.3 RDD依赖关系... 43
3.3.1 窄依赖解析... 43
3.3.2 宽依赖解析... 45
3.4 解析Spark中的DAG逻辑视图... 46
3.4.1 DAG生成的机制... 46
3.4.2 DAG逻辑视图解析... 47
3.5 RDD内部的计算机制... 49
3.5.1 Task解析... 49
3.5.2 计算过程深度解析... 49
3.6 Spark RDD容错原理及其四大核心要点解析... 57
3.6.1 Spark RDD容错原理... 57
3.6.2 RDD容错的四大核心要点... 57
3.7 Spark RDD中Runtime流程解析... 59
3.7.1 Runtime架构图... 59
3.7.2 生命周期... 60
3.8 通过WordCount实战解析Spark RDD内部机制... 70
3.8.1 Spark WordCount动手实践... 70
3.8.2 解析RDD生成的内部机制... 72
3.9 基于DataSet的代码到底是如何一步步转化成为RDD的... 78
第4章 Spark Driver启动内幕剖析... 81
4.1 Spark Driver Program剖析... 81
4.1.1 Spark Driver Program.. 81
4.1.2 SparkContext深度剖析... 81
4.1.3 SparkContext源码解析... 82
4.2 DAGScheduler解析... 96
4.2.1 DAG的定义... 96
4.2.2 DAG的实例化... 97
4.2.3 DAGScheduler划分Stage的原理... 98
4.2.4 DAGScheduler划分Stage的具体算法... 99
4.2.5 Stage内部Task获取位置的算法... 113
4.3 TaskScheduler解析... 116
4.3.1 TaskScheduler原理剖析... 116
4.3.2 TaskScheduler源码解析... 117
4.4 SchedulerBackend解析... 132
4.4.1 SchedulerBackend原理剖析... 132
4.4.2 SchedulerBackend源码解析... 132
4.4.3 Spark程序的注册机制... 133
4.4.4 Spark程序对计算资源Executor的管理... 134
4.5 打通Spark系统运行内幕机制循环流程... 135
4.6 本章总结... 145
第5章 Spark集群启动原理和源码详解... 146
5.1 Master启动原理和源码详解... 146
5.1.1 Master启动的原理详解... 146
5.1.2 Master启动的源码详解... 147
5.1.3 Master HA双机切换... 157
5.1.4 Master的注册机制和状态管理解密... 163
5.2 Worker启动原理和源码详解... 170
5.2.1 Worker启动的原理流程... 170
5.2.2 Worker启动的源码详解... 174
5.3 ExecutorBackend启动原理和源码详解... 178
5.3.1 ExecutorBackend接口与Executor的关系... 178
5.3.2 ExecutorBackend的不同实现... 179
5.3.3 ExecutorBackend中的通信... 181
5.3.4 ExecutorBackend的异常处理... 183
5.4 Executor中任务的执行... 184
5.4.1 Executor中任务的加载... 184
5.4.2 Executor中的任务线程池... 185
5.4.3 任务执行失败处理... 186
5.4.4 揭秘TaskRunner 188
5.5 Executor执行结果的处理方式... 189
5.6 本章总结... 197
第6章 Spark Application提交给集群的原理和源码详解... 198
6.1 Spark Application到底是如何提交给集群的... 198
6.1.1 Application提交参数配置详解... 198
6.1.2 Application提交给集群原理详解... 199
6.1.3 Application提交给集群源码详解... 201
6.2 Spark Application是如何向集群申请资源的... 211
6.2.1 Application申请资源的两种类型详解... 211
6.2.2 Application申请资源的源码详解... 213
6.3 从Application提交的角度重新审视Driver 219
6.3.1 Driver到底是什么时候产生的... 220
6.3.2 Driver和Master交互原理解析... 238
6.3.3 Driver和Master交互源码详解... 244
6.4 从Application提交的角度重新审视Executor 249
6.4.1 Executor到底是什么时候启动的... 249
6.4.2 Executor如何把结果交给Application. 254
6.5 Spark 1.6 RPC内幕解密：运行机制、源码详解、Netty与Akka等... 254
6.6 本章总结... 267
第7章 Shuffle原理和源码详解... 268
7.1 概述... 268
7.2 Shuffle的框架... 269
7.2.1 Shuffle的框架演进... 269
7.2.2 Shuffle的框架内核... 270
7.2.3 Shuffle框架的源码解析... 272
7.2.4 Shuffle数据读写的源码解析... 275
7.3 Hash Based Shuffle. 281
7.3.1 概述... 281
7.3.2 Hash Based Shuffle内核... 282
7.3.3 Hash Based Shuffle数据读写的源码解析... 285
7.4 Sorted Based Shuffle. 290
7.4.1 概述... 292
7.4.2 Sorted Based Shuffle内核... 293
7.4.3 Sorted Based Shuffle数据读写的源码解析... 294
7.5 Tungsten Sorted Based Shuffle. 302
7.5.1 概述... 302
7.5.2 Tungsten Sorted Based Shuffle内核... 302
7.5.3 Tungsten Sorted Based Shuffle数据读写的源码解析... 303
7.6 Shuffle与Storage 模块间的交互... 309
7.6.1 Shuffle注册的交互... 310
7.6.2 Shuffle写数据的交互... 314
7.6.3 Shuffle读数据的交互... 315
7.6.4 BlockManager架构原理、运行流程图和源码解密... 315
7.6.5 BlockManager解密进阶：BlockManager初始化和注册解密、BlockManager- Master工作解密、BlockTransferService解密、本地数据读写解密、远程数据读写解密... 324
7.7 本章总结... 341
第8章 Job工作原理和源码详解... 342
8.1 Job到底在什么时候产生... 342
8.1.1 触发Job的原理和源码解析... 342
8.1.2 触发Job的算子案例... 344
8.2 Stage划分内幕... 345
8.2.1 Stage划分原理详解... 345
8.2.2 Stage划分源码详解... 346
8.3 Task全生命周期详解... 346
8.3.1 Task的生命过程详解... 347
8.3.2Task在Driver和Executor中交互的全生命周期原理和源码详解... 348
8.4 ShuffleMapTask和ResultTask处理结果是如何被Driver管理的... 364
8.4.1 ShuffleMapTask执行结果和Driver的交互原理及源码详解... 364
8.4.2ResultTask执行结果与Driver的交互原理及源码详解... 370
第9章 Spark中Cache和checkpoint原理和源码详解... 372
9.1 Spark中Cache原理和源码详解... 372
9.1.1 Spark中Cache原理详解... 372
9.1.2 Spark中Cache源码详解... 372
9.2 Spark中checkpoint原理和源码详解... 381
9.2.1 Spark中checkpoint原理详解... 381
9.2.2 Spark中checkpoint源码详解... 381
第10章 Spark中Broadcast和Accumulator原理和源码详解... 391
10.1 Spark中Broadcast原理和源码详解... 391
10.1.1 Spark中Broadcast原理详解... 391
10.1.2 Spark中Broadcast源码详解... 393
10.2 Spark中Accumulator原理和源码详解... 396
10.2.1 Spark中Accumulator原理详解... 396
10.2.2 Spark中Accumulator源码详解... 396
第11章 Spark与大数据其他经典组件整合原理与实战... 399
11.1 Spark组件综合应用... 399
11.2 Spark与Alluxio整合原理与实战... 400
11.2.1 Spark与Alluxio整合原理... 400
11.2.2 Spark与Alluxio整合实战... 401
11.3 Spark与Job Server整合原理与实战... 403
11.3.1 Spark与Job Server整合原理... 403
11.3.2 Spark与Job Server整合实战... 404
11.4 Spark与Redis整合原理与实战... 406
11.4.1 Spark与Redis整合原理... 406
11.4.2 Spark与Redis整合实战... 407
中篇 商业案例
第12章 Spark商业案例之大数据电影点评系统应用案例... 412
12.1 通过RDD实现分析电影的用户行为信息... 412
12.1.1 搭建IDEA开发环境... 412
12.1.2 大数据电影点评系统中电影数据说明... 425
12.1.3 电影点评系统用户行为分析统计实战... 428
12.2 通过RDD实现电影流行度分析... 431
12.3 通过RDD分析各种类型的喜爱电影TopN及性能优化技巧... 433
12.4 通过RDD分析电影点评系统仿微信等用户群分析及广播
背后机制解密... 436
12.5 通过RDD分析电影点评系统实现Java和Scala版本的二次排序系统... 439
12.5.1 二次排序自定义Key值类实现（Java）... 440
12.5.2 电影点评系统二次排序功能实现（Java）... 442
12.5.3 二次排序自定义Key值类实现（Scala）... 445
12.5.4 电影点评系统二次排序功能实现（Scala）... 446
12.6 通过Spark SQL中的SQL语句实现电影点评系统用户行为分析... 447
12.7 通过Spark SQL下的两种不同方式实现口碑佳电影分析... 451
12.8 通过Spark SQL下的两种不同方式实现流行电影分析... 456
12.9 通过DataFrame分析受男性和女性喜爱电影TopN.. 457
12.10 纯粹通过DataFrame分析电影点评系统仿微信、淘宝等用户群... 460
12.11 纯粹通过DataSet对电影点评系统进行流行度和不同年龄阶段兴趣分析等... 462
12.11.1 通过DataSet实现某特定电影观看者中男性和女性不同年龄的人数... 463
12.11.2 通过DataSet方式计算所有电影中平均得分高
（口碑好）的电影TopN.. 464
12.11.3 通过DataSet方式计算所有电影中粉丝或者观看人数多（流行电影）的电影TopN 465
12.11.4 纯粹通过DataSet的方式实现所有电影中受男性、女性喜爱的
电影Top10. 466
12.11.5纯粹通过DataSet的方式实现所有电影中微信核心目标
用户喜爱电影TopN分析... 467
12.11.6 纯粹通过DataSet的方式实现所有电影中淘宝核心目标用户喜爱电影TopN分析 469
12.12 大数据电影点评系统应用案例涉及的核心知识点原理、源码及案例代码... 470
12.12.1 知识点：广播变量Broadcast内幕机制... 470
12.12.2 知识点：SQL全局临时视图及临时视图... 473
12.12.3 大数据电影点评系统应用案例完整代码... 474
12.13 本章总结... 496
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据商业实战三部曲
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>一路梦想
"阿根廷/1
阿根廷，梦的诞生/2
智利&玻利维亚/16
智利，在海洋与山脉之间/17
玻利维亚，如此靠近天空的地方/23
秘鲁/32
秘鲁，太阳帝国/33
厄瓜多尔/55
厄瓜多尔，世界的中央/56
亚马逊河&巴西/83
亚马逊河，疾风/84
巴西，红土地/107
委内瑞拉&特立尼达和多巴哥/112
委内瑞拉，繁星/113
哥伦比亚/129
哥伦比亚，不一样的国度/130
巴拿马&哥斯达黎加/144
巴拿马，被海洋环绕的国家/145
哥斯达黎加，这才是生活！/151
尼加拉瓜&洪都拉斯&萨尔瓦多/161
尼加拉瓜，时间不等人/162
洪都拉斯，通往玛雅世界的入口/169
萨尔瓦多，小国的大体验/173
危地马拉&伯利兹/178
危地马拉，多彩的国度/179
伯利兹，海盗的宝藏/183
墨西哥&古巴/188
墨西哥&古巴，玛雅和阿兹特克世界/189
美国&加拿大/207
美国&加拿大，重新开始/208
加拿大，和谐/232
阿拉斯加/268
阿拉斯加，最后一站/269
回家吧/286
真的非常非常感谢！/289"
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>一路梦想
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>创意天才的思维方法
第1章 重新审视思考
第2章 想象力教育
第3章 观察
第4章 形象化
第5章 抽象化
第6章 识别模式
第7章 形成模式
第8章 类比
第9章 身体思维
第10章 移情
第11章 立体思维
第12章 建模
第13章 游戏
第14章 转换
第15章 综合
第16章 综合教育
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>创意天才的思维方法
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>運動改造大腦
【推薦序】  讓孩子贏在體育課　 ◎洪蘭
【推薦序】  別懷疑，就是這麼簡單　◎魏國珍
【推薦序】  快站起來體驗ＩＱ、ＥＱ驚人的改變！　◎張金郎
【推薦序】  給頑固的頭腦一次機會　◎蔡淳娟
【引言】重新建立身心連結

1. 歡迎加入革命：一則關於運動和大腦的個案研究
2. 學習：增長你的腦細胞
3. 壓力：最艱鉅的挑戰
4. 焦慮：沒什麼好恐慌的
5. 憂鬱：讓心情起飛
6. 注意力不足：遠離分心障礙
7. 成癮：拿回自己的主導權
8  荷爾蒙變化：對女性大腦健康的影響
8. 老化：一條智慧之道
9. 訓練計畫：塑造你的大腦
【後記】 讓靈光繼續綻放
【名詞解釋】
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>運動改造大腦
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark SQL内核剖析
第 1 章 Spark SQL 背景  1
1.1    大数据与 Spark 系统  1
1.2    关系模型与 SQL 语言  3
1.3    Spark SQL 发展历程  4
1.4    本章小结  5
第 2 章 Spark 基础知识介绍  6
2.1    RDD 编程模型  6
2.2    DataFrame 与 Dataset  9
2.3    本章小结  10
第 3 章 Spark SQL 执行全过程概述  11
3.1    从 SQL 到 RDD：一个简单的案例  11
3.2    重要概念  14
3.2.1  InternalRow 体系  14
3.2.2  TreeNode 体系  15
3.2.3  Expression 体系  17
3.3    内部数据类型系统  20
3.4    本章小结  21
第 4 章 Spark SQL 编译器 Parser  22
4.1  DSL 工具之 ANTLR 简介  22
4.1.1    基于 ANTLR 4 的计算器  23
4.1.2    访问者模式  25
4.2  SparkSqlParser 之 AstBuilder  28
4.3    常见 SQL 生成的抽象语法树概览  30
4.4    本章小结  33
第 5 章 Spark SQL 逻辑计划（LogicalPlan）  34
5.1    Spark SQL 逻辑计划概述  34
5.2  LogicalPlan 简介  35
5.2.1  QueryPlan 概述  35
5.2.2  LogicalPlan 基本操作与分类  37
5.2.3  LeafNode 类型的 LogicalPlan  38
5.2.4  UnaryNode 类型的 LogicalPlan  39
5.2.5  BinaryNode 类型的 LogicalPlan  40
5.2.6  其他类型的 LogicalPlan  41
5.3  AstBuilder 机制：Unresolved LogicalPlan 生成  41
5.4  Analyzer 机制：Analyzed LogicalPlan 生成  46
5.4.1  Catalog 体系分析  46
5.4.2    Rule 体系  48
5.4.3    Analyzed LogicalPlan 生成过程  50
5.5  Spark SQL 优化器 Optimizer  56
5.5.1  Optimizer 概述  56
5.5.2  Optimizer 规则体系  57
5.5.3  Optimized LogicalPlan 的生成过程  62
5.6    本章小结  64
第 6 章 Spark SQL 物理计划（PhysicalPlan）  66
6.1    Spark SQL 物理计划概述  66
6.2  SparkPlan 简介  67
6.2.1  LeafExecNode 类型  68
6.2.2  UnaryExecNode 类型  69
6.2.3  BinaryExecNode 类型  70
6.2.4  其他类型的 SparkPlan  70
6.3  Metadata 与 Metrics 体系  71
6.4  Partitioning 与 Ordering 体系  72
6.4.1  Distribution 与 Partitioning 的概念  72
6.4.2    SparkPlan 的常用分区排序操作  76
6.5  SparkPlan 生成  77
6.5.1    物理计划 Strategy 体系  79
6.5.2    常见 Strategy 分析  81
6.6    执行前的准备  83
6.6.1  PlanSubqueries 规则  84
6.6.2  EnsureRequirements 规则  85
6.7    本章小结  89
第 7 章 Spark SQL 之 Aggregation 实现  90
7.1  Aggregation 执行概述  90
7.1.1    文法定义  90
7.1.2  聚合语句 Unresolved LogicalPlan 生成  92
7.1.3    从逻辑算子树到物理算子树  93
7.2  聚合函数（AggregateFunction）  97
7.2.1    聚合缓冲区与聚合模式（AggregateMode）  97
7.2.2  DeclarativeAggregate 聚合函数  100
7.2.3  ImperativeAggregate 聚合函数  101
7.2.4  TypedImperativeAggregate 聚合函数  101
7.3    聚合执行  102
7.3.1  执行框架 AggregationIterator  103
7.3.2  基于排序的聚合算子 SortAggregateExec  104
7.3.3  基于 Hash 的聚合算子 HashAggregateExec  105
7.4    窗口（Window）函数  108
7.4.1    窗口函数定义与简介  109
7.4.2    窗口函数相关表达式  111
7.4.3    窗口函数的逻辑计划阶段与物理计划阶段  113
7.4.4    窗口函数的执行  117
7.5    多维分析  120
7.5.1    OLAP 多维分析背景  120
7.5.2  Spark SQL 多维查询  121
7.5.3  多维分析 LogicalPlan 阶段  123
7.5.4  多维分析 PhysicalPlan 与执行  126
7.6    本章小结  128
第 8 章 Spark SQL 之 Join 实现  129
8.1    Join 查询概述  129
8.2    文法定义与抽象语法树  130
8.3    Join 查询逻辑计划  133
8.3.1  从 AST 到 Unresolved LogicalPlan  133
8.3.2  从 Unresolve LogicalPlan 到 Analyzed LogicalPlan  136
8.3.3  从 Analyzed LogicalPlan 到 Optimized LogicalPlan  137
8.4    Join 查询物理计划  140
8.4.1    Join 物理计划的生成  140
8.4.2    Join 物理计划的选取  141
8.5    Join 查询执行  143
8.5.1    Join 执行基本框架  143
8.5.2  BroadcastJoinExec 执行机制  144
8.5.3  ShuffledHashJoinExec 执行机制  145
8.5.4  SortMergeJoinExec 执行机制  148
8.6    本章小结  155
第 9 章 Tungsten 技术实现  156
9.1    内存管理与二进制处理  156
9.1.1    Spark 内存管理基础  156
9.1.2    Tungsten 内存管理优化基础  174
9.1.3    Tungsten 内存优化应用  179
9.2  缓存敏感计算（Cache-aware computation）  185
9.3  动态代码生成（Code generation）  188
9.3.1    漫谈代码生成  188
9.3.2    Janino 编译器实践  190
9.3.3    基本（表达式）代码生成  191
9.3.4    全阶段代码生成（WholeStageCodegen）  196
9.4    本章小结  211
第 10 章 Spark SQL 连接 Hive  212
10.1    Spark SQL 连接 Hive 概述  212
10.2    Hive 相关的规则和策略  213
10.2.1  HiveSessionCatalog 体系  213
10.2.2  Analyzer 之 Hive-Specific 分析规则  216
10.2.3  SparkPlanner 之 Hive-Specific 转换策略  217
10.2.4    Hive 相关的任务执行  218
10.3  Spark SQL 与 Hive 数据类型  219
10.3.1    Hive 数据类型与 SerDe 框架  219
10.3.2  DataTypeToInspector 与 Data Wrapping  220
10.3.3  InspectorToDataType 与 Data Unwrapping  221
10.4    Hive UDF 管理机制  223
10.5  Spark Thrift Server 实现  225
10.5.1  Service 体系  227
10.5.2  Operation 与 OperationManager  228
10.5.3  Session 与 SessionManager  232
10.5.4  Authentication 安全认证管理  234
10.5.5  Spark Thrift Server 执行流程  235
10.6    本章小结  239
第 11 章 Spark SQL 开发与实践  240
11.1    腾讯大数据平台（TDW）简介  240
11.2    腾讯大数据平台 SQL 引擎（TDW-SQL-Engine）  241
11.2.1    SQL-Engine 背景与演化历程  241
11.2.2    SQL-Engine 整体架构  242
11.3    TDW-Spark SQL 开发与优化  244
11.3.1    业务运行支撑框架  244
11.3.2    新功能开发案例  248
11.3.3    性能优化开发案例  256
11.4    业务实践经验与教训  261
11.4.1    Spark SQL 集群管理的经验  261
11.4.2    Spark SQL 业务层面调优  263
11.4.3    SQL 写法的“陷阱”  268
11.5  本章小结  271
总结      272
参考文献    273
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark SQL内核剖析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>闪光的氰化物
第一部　罗斯玛丽
第一章　爱丽丝·马勒
第二章　鲁丝·莱辛
第三章　安东尼·布朗
第四章　斯蒂芬·法拉戴
第五章　亚历山德拉·法拉戴
第六章　乔治·巴顿
第二部　万灵节
第三部　爱丽丝
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>闪光的氰化物
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>卖创意
推荐序  一枚小火花
前言 开启“创意城堡”的金钥匙
第1章　幻想工程：让创意照进现实
是谁把《星球大战》、《汽车总动员》的奇幻场景搬进现实？是谁设计了梦幻城堡、明日世界等让全世界游客魂牵梦绕的游乐天堂？那些为数十亿家庭创造绝妙娱乐时光的人，到底拥有怎样的魔法？
开启创意之路的三句箴言
创意是团队事业，而非个人业务
“米奇奥斯卡”和“高飞奖”：好创意≠好体验
第2章 米奇十诫：打造极致体验的十条创意法则
幻想工程师将电影《汽车总动员》中的场景都变成逼真的三维实体，甚至让孩子惊叹，这就是拍电影的地方。他们如何化虚为实？项目众多，“夺宝奇兵” 凭什么每天让游客为它排起长龙？作为全世界最著名的公园之一，动物王国在开园之初的游客体验情况却不理想。问题在哪里？怎样解决？
戒律 1　了解受众需要并想要什么
戒律 2　穿顾客的鞋子：通过感同身受完善细节
戒律 3　给创意一个内生逻辑
戒律 4　创造一根“维也纳香肠”
戒律 5　视觉素养交流：利用所有非语言工具
戒律 6　明确唤醒点，避免瞌睡点
戒律 7　一次一个：画出清晰的故事线
戒律 8　避免复杂体验：保持设定和故事的一致性
戒律9 　一分教育要携带十分快乐
戒律 10　高级体验离不开高效维护

第3章 如何成为一名幻想工程师
如今备受崇敬的幻想工程师多年前可能只是迪士尼餐厅里刷碗的，或是乐园里开单轨小火车的。迪士尼把怀揣幻想工程梦的新员工放到底层用意何在？每年都有部分大学生通过“幻想国”大赛进入幻想工程部，这到底是怎样一场比赛？赛前应作何准备？
故事是一切的起点
找到能在闹钟之前叫醒你的东西
站在导师的肩膀上
把你的名字摆在团队后面
上前线
建立海量知识储备库
秃鹰与海绵
跨出舒适区：另类思维+冒险一搏
任何值得做的事都值得做到最好
进入“蓝天”
第4章　特殊的信
什么样的经历为16岁辍学的芬坦·伯克铺就了通往迪士尼事业的道路？什么样的魔力吸引了幻想工程师，让他们在迪士尼一干就是二十年、三十年，甚至半个世纪？什么样的礼物改变了首席建筑师伊丽莎白·埃兰德森的一生，让她用大半辈子的时间来追求自己的幻想工程之梦？
幻想工程师的创意之道
特别来信：“我”多希望自己一早就明白
结语　世界上最可怕的事
致谢
关于《敢想！敢做!》
幻想工程师名单
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>卖创意
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Scala程序设计（第2版）
序　　xv
前言　　xvii
第1章　零到六十：Scala简介　　1
1.1　为什么选择Scala　　1
1.1.1　富有魅力的Scala　　2
1.1.2　关于Java 8　　3
1.2　安装Scala　　3
1.2.1　使用SBT　　5
1.2.2　执行Scala命令行工具　　6
1.2.3　在IDE中运行Scala REPL　　8
1.3　使用Scala　　8
1.4　并发　　17
1.5　本章回顾与下一章提要　　27
第2章　更简洁，更强大　　28
2.1　分号　　28
2.2　变量声明　　29
2.3　Range　　31
2.4　偏函数　　32
2.5　方法声明　　33
2.5.1　方法默认值和命名参数列表　　33
2.5.2　方法具有多个参数列表　　34
2.5.3　Future简介　　35
2.5.4　嵌套方法的定义与递归　　38
2.6　推断类型信息　　40
2.7　保留字　　44
2.8　字面量　　46
2.8.1　整数字面量　　46
2.8.2　浮点数字面量　　47
2.8.3　布尔型字面量　　48
2.8.4　字符字面量　　48
2.8.5　字符串字面量　　48
2.8.6　符号字面量　　50
2.8.7　函数字面量　　50
2.8.8　元组字面量　　50
2.9　Option、Some 和None：避免使用null　　52
2.10　封闭类的继承　　53
2.11　用文件和名空间组织代码　　54
2.12　导入类型及其成员　　55
2.12.1　导入是相对的　　56
2.12.2　包对象　　57
2.13　抽象类型与参数化类型　　57
2.14　本章回顾与下一章提要　　59
第3章　要点详解　　60
3.1　操作符重载?　　60
3.2　无参数方法　　63
3.3　优先级规则　　64
3.4　领域特定语言　　65
3.5　Scala中的if语句　　66
3.6　Scala中的for推导式　　67
3.6.1　for循环　　67
3.6.2　生成器表达式　　67
3.6.3　保护式：筛选元素　　67
3.6.4　Yielding　　68
3.6.5　扩展作用域与值定义　　69
3.7　其他循环结构　　70
3.7.1　Scala的while循环　　71
3.7.2　Scala中的do-while循环　　71
3.8　条件操作符　　71
3.9　使用try、catch和final子句　　72
3.10　名字调用和值调用　　75
3.11　惰性赋值　　78
3.12　枚举　　79
3.13　可插入字符串　　81
3.14　Trait：Scala语言的接口和“混入”　　83
3.15　本章回顾与下一章提要　　85
第4章　模式匹配　　86
4.1　简单匹配　　86
4.2　match中的值、变量和类型　　87
4.3　序列的匹配　　90
4.4　元组的匹配　　94
4.5　case中的guard语句　　94
4.6　case类的匹配　　95
4.6.1　unapply方法　　96
4.6.2　unapplySeq方法　　100
4.7　可变参数列表的匹配　　101
4.8　正则表达式的匹配　　103
4.9　再谈case语句的变量绑定　　104
4.10　再谈类型匹配　　104
4.11　封闭继承层级与全覆盖匹配　　105
4.12　模式匹配的其他用法　　107
4.13　总结关于模式匹配的评价　　111
4.14　本章回顾与下一章提要　　111
第5章　隐式详解　　112
5.1　隐式参数　　112
5.2　隐式参数适用的场景　　115
5.2.1　执行上下文　　115
5.2.2　功能控制　　115
5.2.3　限定可用实例　　116
5.2.4　隐式证据　　120
5.2.5　绕开类型擦除带来的限制　　122
5.2.6　改善报错信息　　124
5.2.7　虚类型　　124
5.2.8　隐式参数遵循的规则　　127
5.3　隐式转换　　128
5.3.1　构建独有的字符串插入器　　132
5.3.2　表达式问题　　134
5.4　类型类模式　　135
5.5　隐式所导致的技术问题　　137
5.6　隐式解析规则　　139
5.7　Scala内置的各种隐式　　139
5.8　合理使用隐式　　146
5.9　本章回顾与下一章提要　　146
第6章　Scala函数式编程　　147
6.1　什么是函数式编程　　148
6.1.1　数学中的函数　　148
6.1.2　不可变变量　　149
6.2　Scala中的函数式编程　　151
6.2.1　匿名函数、Lambda与闭包　　152
6.2.2　内部与外部的纯粹性　　154
6.3　递归　　154
6.4　尾部调用和尾部调用优化　　155
6.5　偏应用函数与偏函数　　157
6.6　Curry化与函数的其他转换　　158
6.7　函数式编程的数据结构　　162
6.7.1　序列　　162
6.7.2　映射表　　166
6.7.3　集合　　168
6.8　遍历、映射、过滤、折叠与归约　　168
6.8.1　遍历　　169
6.8.2　映射　　170
6.8.3　扁平映射　　172
6.8.4　过滤　　173
6.8.5　折叠与归约　　174
6.9　向左遍历与向右遍历　　178
6.10　组合器：软件最佳组件抽象　　183
6.11　关于复制　　186
6.12　本章回顾与下一章提要　　188
第7章　深入学习for推导式　　189
7.1　内容回顾：for推导式组成元素　　189
7.2　for推导式：内部机制　　192
7.3　for推导式的转化规则　　194
7.4　Option以及其他的一些容器类型　　197
7.4.1　Option容器　　197
7.4.2　Either：Option类型的逻辑扩展　　200
7.4.3　Try类型　　205
7.4.4　Scalaz提供的Validation类　　206
7.5　本章回顾与下一章提要　　209
第8章　Scala面向对象编程　　210
8.1　类与对象初步　　211
8.2　引用与值类型　　213
8.3　价值类　　214
8.4　父类　　217
8.5　Scala的构造器　　217
8.6　类的字段　　221
8.6.1　统一访问原则　　223
8.6.2　一元方法　　224
8.7　验证输入　　224
8.8　调用父类构造器（与良好的面向对象设计）　　226
8.9　嵌套类型　　230
8.10　本章回顾与下一章提要　　232
第9章　特征　　 233
9.1　Java 8中的接口　　233
9.2　混入trait　　234
9.3　可堆叠的特征　　238
9.4　构造trait　　243
9.5　选择类还是trait　　244
9.6　本章回顾与下一章提要　　245
第10章　Scala对象系统（I）　　246
10.1　参数化类型：继承转化　　246
10.1.1　Hood下的函数　　247
10.1.2　可变类型的变异　　250
10.1.3　Scala和Java中的变异　　252
10.2　Scala的类型层次结构　　253
10.3　闲话Nothing（以及Null）　　254
10.4　Product、case类和元组　　258
10.5　Predef对象　　260
10.5.1　隐式转换　　260
10.5.2　类型定义　　262
10.5.3　条件检查方法　　263
10.5.4　输入输出方法　　263
10.5.5　杂项方法　　265
10.6　对象的相等　　265
10.6.1　equals方法　　266
10.6.2　== 和!=方法　　266
10.6.3　eq 和ne方法　　267
10.6.4　数组相等和sameElements方法　　267
10.7　本章回顾与下一章提要　　268
第11章　Scala对象系统（II）　　269
11.1　覆写类成员和trait成员　　269
11.2　尝试覆写final声明　　272
11.3　覆写抽象方法和具体方法　　272
11.4　覆写抽象字段和具体字段　　274
11.5　覆写抽象类型　　280
11.6　无须区分访问方法和字段：统一访问原则　　280
11.7　对象层次结构的线性化算法　　282
11.8　本章回顾与下一章提要　　287
第12章　Scala集合库　　288
12.1　通用、可变、不可变、并发以及并行集合　　288
12.1.1　scala.collection包　　289
12.1.2　collection.concurrent包　　290
12.1.3　collection.convert包　　291
12.1.4　collection.generic包　　291
12.1.5　collection.immutable包　　291
12.1.6　scala.collection.mutable包　　292
12.1.7　scala.collection.parallel包　　294
12.2　选择集合　　295
12.3　集合库的设计惯例　　296
12.3.1　Builder　　296
12.3.2　CanBuildFrom　　297
12.3.3　Like特征　　298
12.4　值类型的特化　　298
12.5　本章回顾与下一章提要　　300
第13章　可见性规则　　301
13.1　默认可见性：公有可见性　　301
13.2　可见性关键字　　302
13.3　Public可见性　　303
13.4　Protected可见性　　304
13.5　Private可见性　　305
13.6　作用域内私有和作用域内受保护可见性　　306
13.7　对可见性的想法　　312
13.8　本章回顾与下一章提要　　313
第14章　Scala类型系统（I）　　314
14.1　参数化类型　　315
14.1.1　变异标记　　315
14.1.2　类型构造器　　315
14.1.3　类型参数的名称　　315
14.2　类型边界　　315
14.2.1　类型边界上限　　316
14.2.2　类型边界下限　　316
14.3　上下文边界　　320
14.4　视图边界　　320
14.5　理解抽象类型　　322
14.6　自类型标记　　325
14.7　结构化类型　　329
14.8　复合类型　　332
14.9　存在类型　　334
14.10　本章回顾与下一章提要　　335
第15章　Scala类型系统（II）　　336
15.1　路径相关类型　　336
15.1.1　C.this　　337
15.1.2　C.super　　337
15.1.3　path.x　　338
15.2　依赖方法类型　　339
15.3　类型投影　　340
15.4　值的类型　　343
15.4.1　元组类型　　343
15.4.2　函数类型　　343
15.4.3　中缀类型　　343
15.5　Higher-Kinded类型　　344
15.6　类型Lambda　　348
15.7　自递归类型：F-Bounded多态　　350
15.8　本章回顾与下一章提要　　351
第16章　高级函数式编程　　352
16.1　代数数据类型　　352
16.1.1　加法类型与乘法类型　　352
16.1.2　代数数据类型的属性　　354
16.1.3　代数数据类型的最后思考　　355
16.2　范畴理论　　355
16.2.1　关于范畴　　356
16.2.2　Functor范畴　　356
16.2.3　Monad范畴　　360
16.2.4　Monad的重要性　　362
16.3　本章回顾与下一章提要　　363
第17章　并发工具　　365
17.1　scala.sys.process包　　365
17.2　Future类型　　367
17.3　利用Actor模型构造稳固且可扩展的并发应用　　 371
17.4　Akka：为Scala设计的Actor系统　　372
17.5　Pickling和Spores　　383
17.6　反应式编程　　384
17.7　本章回顾与下一章提要　　385
第18章　Scala与大数据　　386
18.1　大数据简史　　386
18.2　用Scala改善MapReduce　　387
18.3　超越MapReduce　　392
18.4　数学范畴　　393
18.5　Scala数据工具列表　　394
18.6　本章回顾与下一章提要　　394
第19章　Scala动态调用　　396
19.1　一个较为激进的示例：Ruby on Rails框架中的ActiveRecord库　　396
19.2　使用动态特征实现Scala 中的动态调用　　397
19.3　关于DSL的一些思考　　402
19.4　本章回顾与下一章提要　　402
第20章　Scala的领域特定语言　　403
20.1　DSL 示例：Scala中XML和JSON DSL　　404
20.2　内部DSL　　406
20.3　包含解析组合子的外部DSL　　410
20.3.1　关于解析组合子　　410
20.3.2　计算工资单的外部DSL　　410
20.4　内部DSL与外部DSL：最后的思考　　413
20.5　本章回顾与下一章提要　　413
第21章　Scala工具和库　　414
21.1　命令行工具　　414
21.1.1　命令行工具：scalac　　414
21.1.2　Scala命令行工具　　418
21.1.3　scalap和javap命令行工具　　421
21.1.4　scaladoc 命令行工具　　422
21.1.5　fsc命令行工具　　422
21.2　构建工具　　422
21.2.1　SBT：Scala标准构建工具　　423
21.2.2　其他构建工具　　425
21.3　与IDE或文本编辑器集成　　425
21.4　在Scala中应用测试驱动开发　　426
21.5　第三方库　　427
21.6　本章回顾与下一章提要　　429
第22章　与Java的互操作　　430
22.1　在Scala代码中使用Java名称　　430
22.2　Java泛型与Scala泛型　　430
22.3　JavaBean的性质　　432
22.4　AnyVal类型与Java原生类型　　433
22.5　Java代码中的Scala名称　　433
22.6　本章回顾与下一章提要　　434
第23章　应用程序设计　　435
23.1　回顾之前的内容　　435
23.2　注解　　437
23.3　Trait即模块　　441
23.4　设计模式　　442
23.4.1　构造型模式　　443
23.4.2　结构型模式　　443
23.4.3　行为型模式　　444
23.5　契约式设计带来更好的设计　　446
23.6　帕特农神庙架构　　448
23.7　本章回顾与下一章提要　　453
第24章　元编程：宏与反射　　454
24.1　用于理解类型的工具　　455
24.2　运行时反射　　455
24.2.1　类型反射　　455
24.2.2　ClassTag、TypeTag与Manifest　　457
24.3　Scala 的高级运行时反射API　　458
24.4　宏　　461
24.4.1　宏的示例：强制不变性　　463
24.4.2　关于宏的最后思考　　466
24.5　本章回顾与下一章提要　　466
附录A　参考文献　　468
作者简介　　473
关于封面　　473
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Scala程序设计（第2版）
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>我把自闭症儿子养育成天才
引言
一英寸还是一万英里
一个男婴
事情不大对
零散的技能
诊断的阴影
一个结束，一个开始
彩虹
突破
退步
新的常态
让它闪光
通向宇宙的窗口
一杯鸡汤
幼儿园高手
三封信
软糖豆
男孩的洞穴
我是谁？
星星救了他
草莓烘饼和行星
两只馅饼
玩的机会
梦想成真
困难时期
嫉妒的天使
粗字体和加下划线
跳一级还是七级
原创理论
离家者之家
幸运硬币
感恩节
过山车
第一份暑期工作
庆祝
附言
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>我把自闭症儿子养育成天才
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>动起来更聪明
推荐序 动起来
丁世忠
安踏体育用品有限公司董事局主席兼CEO
第1章大脑真的会长大
最聪明的学生
谁对胖孩子负责
	运动实验室：运动会让神奇的“脑细胞肥料”变多 ?
	运动 Tips：多运动，更聪明
第2章创造奇迹的“魔法师”
害羞的旁观者杰西
“内啡肽”旋风	 						运动实验室：员工爱运动，公司倒不了？
	运动 Tips：多运动，少压力
第3章赶走“不高兴”
我的大脑重启了
容易害羞的艾米
勇气补给站
	运动实验室：运动能够减轻焦虑吗？
	运动 Tips：做个小小营养师，争当不挑食的好孩子
第4章把大脑叫醒
女医生格蕾丝
比尔50多岁了	 						运动实验室：微笑实验
	运动 Tips：花样运动益处多
第5章分心不是我的错
我不是个笨小孩
“淘气包”杰克逊
	运动实验室：身体好，注意力也会更集中？
	运动 Tips：每天锻炼 1 小时
第6章做自己的小主人
神奇的“传送带”
运动会不会让人上瘾呢
	运动实验室：独自运动是个坏主意吗？
	运动 Tips：再坚持一下
第7章女生们，一起动起来
佩蒂的好办法
最快乐的新妈妈
	运动实验室：怀宝宝时，动还是不动？
	运动 Tips：身体灵活很重要
第8章一辈子的好朋友
我的妈妈
把大脑里的“魔鬼”赶出去
	运动实验室：运动能让海马体好好“工作”？
	运动 Tips：动一动，让骨骼更强壮
后记  让灵光持续绽放

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>动起来更聪明
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>媒介效果研究概论-第四版
前言
第一章 媒介效果研究的科学方式
认知的方式
科学研究的目标
如何达到科学研究设定的目标
小结
关键词与关键概念
参考文献
第二章 媒介效果研究的科学方法
媒介内容分析
抽样调查
建立因果关系
实验方法
关于研究方法的争论
小结
关键词与关键概念
参考文献
第三章 媒介效果研究简史
奠基
1929-1932年——佩恩基金研究项目
火星入侵
早期媒介效果理论：魔弹论
《人民的选择》研究：有限效果论
迪凯特研究项目：传播中的二级流动
连环漫画的弊端
电视初现
媒介效果大观
小结
关键词与关键概念
参考文献
第四章 媒介使用时间：动机与结果
使用与满足论
自我报告方法的问题
媒介使用时间
小结
关键词与关键概念
参考文献
第五章 媒介暴力的影响
暴力内容的呈现
观看暴力内容与暴力倾向行为之间的因果关系
对暴力的脱敏
电子游戏怎么了？它训练孩子杀人吗？
人们为什么喜欢媒介暴力？
媒介暴力研究在走向何方？
综述
小结
关键词与关键概念
参考文献
第六章 媒介中的性
人类的性存在导致对性内容讨论的热情
明确定义和术语
淫秽和色情问题委员会
美国淫秽和色情问题委员会报告之后的研究
米兹委员会关于色情问题的报告
电视黄金时段的性内容
性与互联网
对媒介内容的控制
小结
关键词与关键概念
参考文献
第七章 恐怖的媒介
对媒介的恐惧反应普遍存在
发展理论：对媒介恐怖的反应因人而异
《绿巨人》实验
异常的形象为什么恐怖？
表面真实性法则
为什么有些媒介恐惧会长时间持续？理论：大脑具有独特处理信息的能力
情绪应对理论：家长如何应对？
感觉恐惧有什么乐趣？
恐怖电影与坐过山车
观看恐怖电影的性动力
恐惧之外：对媒体的其他情感反应
共情：我感受到你的感觉
小结
关键词与关键概念
参考文献
第八章 媒介的说服效果
什么是说服？
媒介如何做到事半功倍
媒介中的有意说服
媒介说服的重要原则
小结
关键词与关键概念
参考文献
第九章 新闻与政治内容的影响
对新闻的检视
情感的作用
对于新闻的另一种观点：某些新闻报道真的会置更多人于死地吗？
小结
关键词与关键概念
参考文献
第十章 媒介内容中刻板印象的影响
媒介内容中的刻板印象
种族刻板印象
媒介刻板印象研究中的不平衡现象
小结
关键词与关键概念
参考文献
第十一章 新媒介技术的影响
新媒介技术革命
计算机与互联网：联系抑或疏离
小结
关键词与关键概念
参考文献
第十二章 与麦克卢汉面对面：媒介效果研究的非科学方式
讨论麦克卢汉意义何在
与麦克卢汉面对面
传播史的几个重要阶段
电子媒介对人类的影响
为什么说麦克卢汉的“理论”不够科学
麦克卢汉的影响
结束前的思考
小结
关键词与关键概念
参考文献
参考资料
附录
人名索引
主题索引
译后记
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>媒介效果研究概论-第四版
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>抄襲經濟學
引言
第一章 讓設計師恨得牙癢的Forever 21：山寨與時尚受害者的爭鋒相對
第二章 人人都可以成為茱莉亞．柴爾德：老饕界的抄襲與創意
第三章 喜劇演員的道德制裁：除了法律約束的另一種有效手段。
第四章 微創新是一種新流行：橄欖球、字體、經濟和美食的大革命
第五章 模仿所發揮的積極作用：創新的趨勢與週期
後記：被Napster翻轉的音樂產業走向
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>抄襲經濟學
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>循序渐进学Spark/大数据技术丛书
前言
第1章Spark架构与集群环境1
1.1Spark概述与架构1
1.1.1Spark概述2
1.1.2Spark生态3
1.1.3Spark架构5
1.2在Linux集群上部署Spark8
1.2.1安装OpenJDK9
1.2.2安装Scala9
1.2.3配置SSH免密码登录10
1.2.4Hadoop的安装配置10
1.2.5Spark的安装部署13
1.2.6Hadoop与Spark的集群复制14
1.3Spark集群试运行15
1.4IntellijIDEA的安装与配置17
1.4.1Intellij的安装17
1.4.2Intellij的配置17
1.5EclipseIDE的安装与配置18
1.6使用SparkShell开发运行Spark程序19
1.7本章小结20
第2章Spark编程模型21
2.1RDD弹性分布式数据集21
2.1.1RDD简介22
2.1.2深入理解RDD22
2.1.3RDD特性总结24
2.2Spark程序模型25
2.3Spark算子26
2.3.1算子简介26
2.3.2Value型Transmation算子27
2.3.3Key—Value型Transmation算子32
2.3.4Action算子34
2.4本章小结37
第3章Spark机制原理38
3.1Spark应用执行机制分析38
3.1.1Spark应用的基本概念38
3.1.2Spark应用执行机制概要39
3.1.3应用提交与执行41
3.2Spark调度机制42
3.2.1Application的调度42
3.2.2job的调度43
3.2.3stage（调度阶段）和TasksetManager的调度46
3.2.4task的调度50
3.3Spark存储与I／O52
3.3.1Spark存储系统概览52
3.3.2BlockManager中的通信54
3.4Spark通信机制54
3.4.1分布式通信方式54
3.4.2通信框架AKKA56
3.4.3Client、Master和Worker之间的通信57
3.5容错机制及依赖65
3.5.1Lineage（血统）机制66
3.5.2Checkpoint（检查点）机制68
3.6Shuffle机制70
3.6.1什么是Shuffle70
3.6.2Shuffle历史及细节72
3.7本章小结78
第4章深入Spark内核79
4.1Spark代码布局79
4.1.1Spark源码布局简介79
4.1.2SparkCore内模块概述80
4.1.3SparkCore外模块概述80
4.2Spark执行主线（RDD→Task）剖析80
4.2.1从RDD到DAGScheduler81
4.2.2从DAGScheduler到TaskScheduler82
4.2.3从TaskScheduler到Worker节点88
4.3Client、Master和Worker交互过程剖析89
4.3.1交互流程概览89
4.3.2交互过程调用90
4.4Shuffle触发96
4.4.1触发ShuffleWrite96
4.4.2触发ShuffleRead98
4.5Spark存储策略100
4.5.1CacheManager职能101
4.5.2BlockManager职能105
4.5.3DiskStore与DiskBlock——Manager类113
4.5.4MemoryStore类114
4.6本章小结117
第5章SparkonYARN118
5.1YARN概述118
5.2SparkonYARN的部署模式121
5.3SparkonYARN的配置重点125
5.3.1YARN的自身内存配置126
5.3.2SparkonYARN的重要配置127
5.4本章小结128
第6章BDAS生态主要模块129
6.1SparkSQL129
6.1.1SparkSQL概述130
6.1.2SparkSQL的架构分析132
6.1.3SparkSQL如何使用135
6.2SparkStreaming140
6.2.1SparkStreaming概述140
6.2.2SparkStreaming的架构分析143
6.2.3SparkStreaming编程模型145
6.2.4数据源DataSource147
6.2.5DStream操作149
6.3SparkR154
6.3.1R语言概述154
6.3.2SparkR简介155
6.3.3DataFrame创建156
6.3.4DataFrame操作158
6.4MLlibonSpark162
6.4.1机器学习概述162
6.4.2机器学习的研究方向与问题164
6.4.3机器学习的常见算法167
6.4.4MLlib概述210
6.4.5MLlib架构212
6.4.6MLlib使用实例——电影推荐214
6.5本章小结220
第7章Spark调优221
7.1参数配置221
7.2调优技巧223
7.2.1序列化优化223
7.2.2内存优化224
7.2.3数据本地化228
7.2.4其他优化考虑229
7.3实践中常见调优问题及思考230
7.4本章小结231
第8章Spark2.0.0232
8.1功能变化232
8.1.1删除的功能232
8.1.2Spark中发生变化的行为233
8.1.3不再建议使用的功能233
8.2Core以及SparkSQL的改变234
8.2.1编程API234
8.2.2多说些关于SparkSession234
8.2.3SQL236
8.3MLlib237
8.3.1新功能237
8.3.2速度／扩展性237
8.4SparkR238
8.5Streaming238
8.5.1初识结构化Streaming238
8.5.2结构化Streaming编程模型239
8.5.3结果输出240
8.6依赖、打包242
8.7本章小结242
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>循序渐进学Spark/大数据技术丛书
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark：大数据集群计算的生产实践
第1章  成功运行Spark job	1
安装所需组件	2
-- 原生安装Spark Standalone集群	3
分布式计算的发展史	3
-- 步入云时代	5
-- 理解资源管理	6
使用各种类型的存储格式	9
-- 文本文件	11
-- Sequence文件	13
-- Avro文件	13
-- Parquet文件	13
监控和度量的意义	14
-- Spark UI	14
-- Spark Standalone UI	17
-- Metrics REST API	17
-- Metrics System	18
-- 外部监控工具	18
总结	19
第2章  集群管理	21
背景知识	23
Spark组件	26
-- Driver	27
-- workers与executors	28
-- 配置	30
Spark Standalone	33
-- 架构	34
-- 单节点设置场景	34
-- 多节点设置	36
YARN	36
-- 架构	38
-- 动态资源分配	41
-- 场景	43
Mesos	45
-- 安装	46
-- 架构	47
-- 动态资源分配	49
-- 基本安装场景	50
比较	52
总结	56
第3章  性能调优	59
Spark 执行模型	60
分区	62
-- 控制并行度	62
-- 分区器	64
shuffle数据	65
-- shuffle与数据分区	67
-- 算子与shuffle	70
-- shuffle并不总是坏事	75
序列化	75
-- Kryo注册器	77
Spark缓存	77
-- SparkSQL 缓存	81
内存管理	82
-- 垃圾回收	83
共享变量	84
-- 广播变量	85
-- 累加器	87
数据局部性	90
总结	91
第4章  安全	93
架构	94
-- Security Manager	94
-- 设定配置	95
ACL	97
-- 配置	97
-- 提交job	98
-- Web UI	99
网络安全	107
加密	108
事件日志	113
Kerberos	114
Apache Sentry	114
总结	115
第5章  容错或job执行	117
Spark job的生命周期	118
-- Spark master	119
-- Spark driver	122
-- Spark worker	124
-- job生命周期	124
job调度	125
-- 应用程序内部调度	125
-- 用外部工具进行调度	133
容错	135
-- 内部容错与外部容错	136
-- SLA	137
-- RDD	138
-- Batch vs Streaming	145
-- 测试策略	148
-- 推荐配置	155
总结	158
第6章  超越Spark	159
数据仓库	159
-- SparkSQL CLI	161
-- Thrift JDBC/ODBC服务器	162
-- Hive on Spark	162
机器学习	164
-- DataFrame	165
-- MLlib和ML	167
-- Mahout on Spark	174
-- Hivemall On Spark	175
外部的框架	176
-- Spark Package	177
-- XGBoost	179
-- spark-jobserver	179
未来的工作	182
-- 与参数服务器集成	184
-- 深度学习	192
Spark在企业中的应用	200
-- 用Spark及Kafka收集用户活动日志	200
-- 用Spark做实时推荐	202
-- Twitter Bots的实时分类	204
总结	205
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark：大数据集群计算的生产实践
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习进阶实战/大数据技术丛书
前　言
第一篇　基础篇
第1章　机器学习概述 2
1.1　机器学习概述 2
1.1.1　理解大数据 2
1.1.2　机器学习发展过程 4
1.1.3　大数据生态环境 5
1.2　机器学习算法 6
1.2.1　传统机器学习 6
1.2.2　深度学习 8
1.2.3　其他机器学习 8
1.3　机器学习分类 9
1.3.1　监督学习 9
1.3.2　无监督学习 10
1.3.3　半监督学习 10
1.3.4　强化学习 10
1.4　机器学习综合应用 11
1.4.1　异常检测 12
1.4.2　用户画像 12
1.4.3　广告点击率预估 12
1.4.4　企业征信大数据应用 12
1.4.5　智慧交通大数据应用 13
1.5　本章小结 13
第2章　数据分析流程和方法 14
2.1　数据分析概述 14
2.2　数据分析流程 15
2.2.1　业务调研 16
2.2.2　明确目标 16
2.2.3　数据准备 16
2.2.4　特征处理 17
2.2.5　模型训练与评估 21
2.2.6　输出结论 23
2.3　数据分析的基本方法 24
2.3.1　汇总统计 24
2.3.2　相关性分析 25
2.3.3　分层抽样 26
2.3.4　假设检验 26
2.4　简单的数据分析实践 27
2.4.1　环境准备 27
2.4.2　准备数据 28
2.4.3　数据分析 29
2.5　本章小结 30
第二篇　算法篇
第3章　构建分类模型 32
3.1　分类模型概述 32
3.2　分类模型算法 34
3.2.1　逻辑回归 34
3.2.2　朴素贝叶斯模型 36
3.2.3　SVM模型 37
3.2.4　决策树模型 39
3.2.5　K-近邻 40
3.3　分类效果评估 40
3.3.1　正确率 41
3.3.2　准确率、召回率和F1值 41
3.3.3　ROC和AUC 42
3.4　App数据的分类实现 44
3.4.1　选择分类器 44
3.4.2　准备数据 45
3.4.3　训练模型 46
3.4.4　模型性能评估 48
3.4.5　模型参数调优 49
3.5　其他分类模型 50
3.5.1　随机森林 50
3.5.2　梯度提升树 51
3.5.3　因式分解机模型 51
3.6　本章小结 52
第4章　构建聚类模型 53
4.1　聚类概述 53
4.2　聚类模型 54
4.2.1　KMeans聚类 54
4.2.2　DBSCAN聚类 55
4.2.3　主题聚类 56
4.3　聚类效果评价 58
4.3.1　集中平方误差和 58
4.3.2　Purity评价法 59
4.4　使用KMeans对鸢尾花卉数据集聚类 59
4.4.1　准备数据 59
4.4.2　特征处理 60
4.4.3　聚类分析 60
4.4.4　模型性能评估 62
4.5　使用DBSCAN对GPS数据进行聚类 62
4.5.1　准备数据 63
4.5.2　特征处理 64
4.5.3　聚类分析 64
4.5.4　模型参数调优 65
4.6　其他模型 66
4.6.1　层次聚类 66
4.6.2　基于图的聚类 67
4.6.3　混合聚类模型 67
4.7　本章小结 68
第5章　构建回归模型 69
5.1　常用回归模型 69
5.1.1　线性回归模型 70
5.1.2　回归树模型 70
5.1.3　其他回归模型 71
5.2　评估指标 73
5.3　回归模型优化 74
5.3.1　特征选择 74
5.3.2　特征变换 74
5.4　构建UCI裙子销售数据回归模型 75
5.4.1　准备数据 75
5.4.2　训练模型 78
5.4.3　评估效果 79
5.4.4　模型优化 79
5.5　其他回归模型案例 80
5.5.1　GDP影响因素分析 81
5.5.2　大气污染分析 81
5.5.3　大数据比赛中的回归问题 81
5.6　本章小结 82
第6章　构建关联规则模型 83
6.1　关联规则概述 83
6.2　常用关联规则算法 84
6.2.1　Apriori算法 84
6.2.2　FP-Growth算法 85
6.3　效果评估和优化 86
6.3.1　效果评估 86
6.3.2　效果优化 87
6.4　使用FP-Growth对豆瓣评分数据进行挖掘 88
6.4.1　准备数据 89
6.4.2　训练模型 89
6.4.3　观察规则 91
6.4.4　参数调优 91
6.4.5　使用算法 92
6.5　其他应用场景 94
6.6　本章小结 96
第7章　协同过滤 97
7.1　协同过滤概述 97
7.2　常用的协同过滤算法 98
7.2.1　基于用户的协同过滤 99
7.2.2　基于物品的协同过滤 100
7.2.3　矩阵分解技术 101
7.2.4　推荐算法的选择 102
7.3　评估标准 103
7.3.1　准确率 103
7.3.2　覆盖率 103
7.3.3　多样性 104
7.3.4　其他指标 104
7.4　使用电影评分数据进行协同过滤实践 104
7.4.1　准备数据 105
7.4.2　训练模型 106
7.4.3　测试模型 109
7.4.4　使用ALS结果 111
7.5　本章小结 112
第8章　数据降维 113
8.1　降维概述 113
8.2　常用降维算法 114
8.2.1　主成分分析 114
8.2.2　奇异值分解 116
8.2.3　广义降维 117
8.2.4　文本降维 118
8.3　降维评估标准 121
8.4　使用PCA对Digits数据集进行降维 122
8.4.1　准备数据 122
8.4.2　训练模型 123
8.4.3　分析降维结果 124
8.5　其他降维方法 124
8.5.1　线性判别分析 124
8.5.2　局部线性嵌入 125
8.5.3　拉普拉斯特征映射 125
8.6　本章小结 126
第三篇　综合应用篇
第9章　异常检测 128
9.1　异常概述 128
9.1.1　异常的产生 129
9.1.2　异常检测的分类 129
9.2　异常检测方法 130
9.2.1　基于模型的方法 130
9.2.2　基于邻近度的方法 131
9.2.3　基于密度的方法 132
9.2.4　基于聚类的方法 133
9.3　异常检测系统 133
9.3.1　异常检测过程 133
9.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习进阶实战/大数据技术丛书
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析
第1章 大数据技术一览
第2章 Scala编程
第3章 SparkCore
第4章 使用Spark shell进行交互式数据分析
第5章 编写Spark应用
第6章 Spark Streaming
第7章 Spark SQL
第8章使用Spark进行机器学习
第9章 使用Spark进行图处理
第10章 集群管理员
第11章监控
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark实时大数据分析
第1章　大数据漫游指南 1
1.1　Spark前传 1
1.1.1　Web 2.0时代 2
1.1.2　无处不在的传感器 7
1.2　Spark Streaming：MapReduce和CEP的交集 9
第2章　Spark简介 10
2.1　安装 11
2.2　执行 12
2.2.1　独立集群模式（Standalone Cluster） 12
2.2.2　YARN模式 13
2.3　第一个应用程序 13
2.3.1　构建 16
2.3.2　执行 17
2.4　SparkContext 19
2.4.1　RDDs创建 19
2.4.2　处理依赖关系 20
2.4.3　创建共享变量 21
2.4.4　作业执行 22
2.5　RDD 22
2.5.1　持久化 23
2.5.2　转换 24
2.5.3　行动（Action） 28
小结 29
第3章　实时RDD：DStream 30
3.1　从连续流到离散流 30
3.2　第一个Spark Streaming应用程序 31
3.2.1　构建和执行 34
3.2.2　Streaming Context 34
3.3　DStreams 36
3.3.1　Spark Streaming应用程序剖析 38
3.3.2　转换 42
小结 52
第4章　高速流：并行化及其他 54
4.1　流数据的一大飞跃 54
4.2　并行化 56
4.2.1　Worker 56
4.2.2　执行器（Executor） 57
4.2.3　任务（Task） 59
4.3　批处理间隔 62
4.4　调度 64
4.4.1　应用程序间调度 64
4.4.2　批处理调度 64
4.4.3　作业间调度 65
4.4.4　一个行动，一个作业 65
4.5　内存 66
4.5.1　序列化 67
4.5.2　压缩（Compression） 70
4.5.3　垃圾收集 70
4.6　Shuffle 70
4.6.1　早期投影和过滤 70
4.6.2　经常使用组合器 70
4.6.3　大量运用平行化 70
4.6.4　文件合并（File Consolidation） 71
4.6.5　更多内存 71
小结 71
第5章　链接外部数据源 72
5.1　智慧城市，智慧地球，一切更智慧 72
5.2　ReceiverInputDStream 74
5.3　套接字 76
5.4　MQTT 85
5.5　Flume 89
5.5.1　基于推模式的Flume数据摄取 91
5.5.2　基于拉模式的Flume数据摄取 92
5.6　Kafka 92
5.6.1　基于接收器的Kafka消费者 95
5.6.2　直接Kafka消费者 98
5.7　Twitter 99
5.8　块间隔 100
5.9　自定义接收器 100
小结 104
第6章　边界效应 106
6.1　盘点股市 106
6.2　foreachRDD 108
6.2.1　为每条记录创建一个连接 110
6.2.2　为每个分区创建一个连接 111
6.2.3　静态连接 112
6.2.4　惰性静态连接 113
6.2.5　静态连接池 114
6.3　可扩展流存储 116
6.3.1　HBase 117
6.3.2　股市控制台（Dashboard） 118
6.3.3　SparkOnHBase 120
6.3.4　Cassandra 122
6.3.5　Spark Cassandra连接器 124
6.4　全局状态（Global State） 126
6.4.1　静态变量 126
6.4.2　updateStateByKey() 128
6.4.3　累加器 129
6.4.4　外部解决方案 131
小结 133
第7章　充分准备 134
7.1　每个点击都异乎重要 134
7.2　Tachyon（Alluxio） 135
7.3　Spark Web UI 138
7.3.1　历史分析 151
7.3.2　RESTful度量 152
7.4　日志记录 153
7.5　外部度量 154
7.6　系统度量 156
7.7　监控和报警 157
小结 159
第8章　实时ETL和分析技术 160
8.1　交易数据记录的强大功能 160
8.2　第一个流式Spark SQL应用程序 162
8.3　SQLContext 165
8.3.1　创建数据框 165
8.3.2　执行SQL 168
8.3.3　配置 169
8.3.4　用户自定义函数 169
8.3.5　Catalyst：查询执行和优化 171
8.3.6　HiveContext 171
8.4　数据框（Data Frame） 173
8.4.1　类型 173
8.4.2　查询转换 173
8.4.3　行动 180
8.4.4　RDD操作 182
8.4.5　持久化 182
8.4.6　最佳做法 183
8.5　SparkR 183
8.6　第一个SparkR应用程序 184
8.6.1　执行 185
8.6.2　流式SparkR 185
小结 188
第9章　大规模机器学习 189
9.1　传感器数据风暴 189
9.2　流式MLlib应用程序 191
9.3　MLlib 194
9.3.1　数据类型 194
9.3.2　统计分析 197
9.3.3　预处理 198
9.4　特征选择和提取 199
9.4.1　卡方选择 199
9.4.2　主成分分析 200
9.5　学习算法 201
9.5.1　分类 202
9.5.2　聚类 202
9.5.3　推荐系统 204
9.5.4　频繁模式挖掘 207
9.6　流式ML管道应用程序 208
9.7　ML 211
9.8　管道交叉验证 212
小结 213
第10章　云、Lambda及Python 215
10.1　一条好评胜过一千个广告 216
10.2　Google Dataproc 217
10.3　基于Dataproc应用程序创建的第一个Spark 220
10.4　PySpark 227
10.5　Lambda架构 229
10.6　流式图分析 238
总结 241
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark实时大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>發掘你的太陽魔力
前言：點燃夢想的火花
1. 穿越七扇白門
2. 偶然的機遇
3. 歡迎光臨太陽馬演團
4. 親自探尋太陽魔力
5. 有些祕訣，既困難又簡單
6. 是終點，也是起點
尾聲：找到自己的路
進入太陽劇團
不給你馬戲，卻給你靈魂
太陽劇團獨家專訪
只有太陽劇團一年能賣出一千萬張票－－獨家專訪總裁兼CEO拉馬爾
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>發掘你的太陽魔力
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
译者序
关于作者
前言
第1章　大规模机器学习和Spark入门 1
1.1　数据科学 2
1.2　数据科学家：21世纪最炫酷的职业 2
1.2.1　数据科学家的一天 3
1.2.2　大数据处理 4
1.2.3　分布式环境下的机器学习算法 4
1.2.4　将数据拆分到多台机器 6
1.2.5　从Hadoop MapReduce到Spark 6
1.2.6　什么是Databricks 7
1.2.7　Spark包含的内容 8
1.3　H2O.ai简介 8
1.4　H2O和Spark MLlib的区别 10
1.5　数据整理 10
1.6　数据科学：一个迭代过程 11
1.7　小结 11
第2章　探索暗物质：希格斯玻色子 12
2.1　Ⅰ型错误与Ⅱ型错误 12
2.1.1　寻找希格斯玻色子 13
2.1.2　LHC和数据的创建 13
2.1.3　希格斯玻色子背后的理论 14
2.1.4　测量希格斯玻色子 14
2.1.5　数据集 14
2.2　启动Spark与加载数据 15
2.2.1　标记点向量 22
2.2.2　创建训练和测试集合 24
2.2.3　第一个模型：决策树 26
2.2.4　下一个模型：集合树 32
2.2.5　最后一个模型：H2O深度学习 37
2.2.6　构建一个3层DNN 39
2.3　小结 45
第3章　多元分类的集成方法 46
3.1　数据 47
3.2　模型目标 48
3.2.1　挑战 48
3.2.2　机器学习工作流程 48
3.2.3　使用随机森林建模 61
3.3　小结 78
第4章　使用NLP和Spark Streaming预测电影评论 80
4.1　NLP简介 81
4.2　数据集 82
4.3　特征提取 85
4.3.1　特征提取方法：词袋模型 85
4.3.2　文本标记 86
4.4　特征化——特征哈希 89
4.5　我们来做一些模型训练吧 92
4.5.1　Spark决策树模型 93
4.5.2　Spark朴素贝叶斯模型 94
4.5.3　Spark随机森林模型 95
4.5.4　Spark GBM模型 96
4.5.5　超级学习器模型 97
4.6　超级学习器 97
4.6.1　集合所有的转换 101
4.6.2　使用超级学习器模型 105
4.7　小结 105
第5章　word2vec预测和聚类 107
5.1　词向量的动机 108
5.2　word2vec解释 108
5.2.1　什么是单词向量 108
5.2.2　CBOW模型 110
5.2.3　skip-gram模型 111
5.2.4　玩转词汇向量 112
5.2.5　余弦相似性 113
5.3　doc2vec解释 113
5.3.1　分布式内存模型 113
5.3.2　分布式词袋模型 114
5.4　应用word2vec并用向量探索数据 116
5.5　创建文档向量 118
5.6　监督学习任务 119
5.7　小结 123
第6章　从点击流数据中抽取模式 125
6.1　频繁模式挖掘 126
6.2　使用Spark MLlib进行模式挖掘 130
6.2.1　使用FP-growth进行频繁模式挖掘 131
6.2.2　关联规则挖掘 136
6.2.3　使用prefix span进行序列模式挖掘 138
6.2.4　在MSNBC点击流数据上进行模式挖掘 141
6.3　部署模式挖掘应用 147
6.4　小结 154
第7章　使用GraphX进行图分析 155
7.1　基本的图理论 156
7.1.1　图 156
7.1.2　有向和无向图 156
7.1.3　阶和度 157
7.1.4　有向无环图 158
7.1.5　连通分量 159
7.1.6　树 160
7.1.7　多重图 160
7.1.8　属性图 161
7.2　GraphX分布式图计算引擎 162
7.2.1　GraphX中图的表示 163
7.2.2　图的特性和操作 165
7.2.3　构建和加载图 170
7.2.4　使用Gephi可视化图结构 172
7.2.5　图计算进阶 178
7.2.6　GraphFrame 181
7.3　图算法及其应用 183
7.3.1　聚类 183
7.3.2　顶点重要性 185
7.4　GraphX在上下文中 188
7.5　小结 189
第8章　Lending Club借贷预测 190
8.1　动机 190
8.1.1　目标 191
8.1.2　数据 192
8.1.3　数据字典 192
8.2　环境准备 193
8.3　数据加载 193
8.4　探索——数据分析 194
8.4.1　基本清理 194
8.4.2　预测目标 200
8.4.3　使用模型评分 221
8.4.4　模型部署 224
8.5　小结 229
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark全栈数据分析
目录
前言 .................................................................................................. xiv
第Ⅰ部分　准备工作
第1章　理论 ..........................................................................................3
导论 .............................................................................................................................3
定义 .............................................................................................................................5
方法学 ................................................................................................................5
敏捷数据科学宣言 ............................................................................................6
瀑布模型的问题 .......................................................................................................10
研究与应用开发 ..............................................................................................11
敏捷软件开发的问题 ...............................................................................................14
最终质量：偿还技术债 ....................................................................................14
瀑布模型的拉力 ..............................................................................................15
数据科学过程 ...........................................................................................................16
设置预期 ..........................................................................................................17
数据科学团队的角色 ......................................................................................18
认清机遇与挑战 ..............................................................................................19
适应变化 ..........................................................................................................21
过程中的注意事项 ...................................................................................................23
代码审核与结对编程 ......................................................................................25
敏捷开发的环境：提高生产效率 ....................................................................25
用大幅打印实现想法 ......................................................................................27
第2章　敏捷工具 ................................................................................29
可伸缩性＝易用性 ...................................................................................................30
敏捷数据科学之数据处理 .......................................................................................30
搭建本地环境 ...........................................................................................................32
配置要求 ..........................................................................................................33
配置Vagrant .....................................................................................................33
下载数据 ..........................................................................................................33
搭建EC2环境 ............................................................................................................34
下载数据 ..........................................................................................................38
下载并运行代码 .......................................................................................................38
下载代码 ..........................................................................................................38
运行代码 ..........................................................................................................38
Jupyter笔记本 ...................................................................................................39
工具集概览 ...............................................................................................................39
敏捷开发工具栈的要求 ..................................................................................39
Python 3 ...........................................................................................................39
使用JSON行和Parquet序列化事件 .................................................................42
收集数据 ..........................................................................................................45
使用Spark进行数据处理 .................................................................................45
使用MongoDB发布数据 .................................................................................48
使用Elasticsearch搜索数据 .............................................................................50
使用Apache Kafka分发流数据 .......................................................................54
使用PySpark Streaming处理流数据 ...............................................................57
使用scikit-learn与Spark MLlib进行机器学习 ................................................58
使用 Apache Airflow（孵化项目）进行调度 ....................................................59
反思我们的工作流程 ......................................................................................70
轻量级网络应用 ..............................................................................................70
展示数据 ..........................................................................................................73
本章小结 ...................................................................................................................75
第3章　数据 ........................................................................................77
飞行航班数据 ...........................................................................................................77
航班准点情况数据 ..........................................................................................78
OpenFlights数据库 ...........................................................................................79
天气数据 ...................................................................................................................80
敏捷数据科学中的数据处理 ...................................................................................81
结构化数据vs.半结构化数据 ..........................................................................81
SQL vs. NoSQL .........................................................................................................82
SQL ...................................................................................................................83
NoSQL与数据流编程 ......................................................................................83
Spark: SQL + NoSQL ......................................................................................84
NoSQL中的表结构 ..........................................................................................84
数据序列化 ......................................................................................................85
动态结构表的特征提取与呈现 ......................................................................85
本章小结 ...................................................................................................................86
第Ⅱ部分　攀登金字塔
第4章　记录收集与展示 ......................................................................89
整体使用 ...................................................................................................................90
航班数据收集与序列化 ...........................................................................................91
航班记录处理与发布 ...............................................................................................94
把航班记录发布到MongoDB .........................................................................95
在浏览器中展示航班记录 .......................................................................................96
使用Flask和pymongo提供航班信息 ...............................................................97
使用Jinja2渲染HTML5页面............................................................................98
敏捷开发检查站 .....................................................................................................102
列出航班记录 .........................................................................................................103
使用MongoDB列出航班记录 .......................................................................103
数据分页 ........................................................................................................106
搜索航班数据 .........................................................................................................112
创建索引 ........................................................................................................112
发布航班数据到Elasticsearch ......................................................................113
通过网页搜索航班数据 ................................................................................114
本章小结 .................................................................................................................117
第5章　使用图表进行数据可视化 .................................................... 119
图表质量：迭代至关重要 .......................................................................................120
用发布/装饰模型伸缩数据库 ................................................................................120
一阶形式 ........................................................................................................121
二阶形式 ........................................................................................................122
三阶形式 ........................................................................................................123
选择一种形式 ................................................................................................123
探究时令性 .............................................................................................................124
查询并展示航班总数 ....................................................................................124
提取“金属”（飞机（实体）） .....................................................................................132
提取机尾编号 ................................................................................................132
评估飞机记录 ................................................................................................139
数据完善 .................................................................................................................140
网页表单逆向工程 ........................................................................................140
收集机尾编号 ................................................................................................142
自动化表单提交 ............................................................................................143
从HTML中提取数据 .....................................................................................144
评价完善后的数据 ........................................................................................147
本章小结 .................................................................................................................148
第6章　通过报表探索数据 ............................................................... 149
提取航空公司为实体 .............................................................................................150
使用PySpark把航空公司定义为飞机的分组 ...............................................150
在MongoDB中查询航空公司数据 ...............................................................151
在Flask中构建航空公司页面 ........................................................................151
添加回到航空公司页面的链接 ....................................................................152
创建一个包括所有航空公司的主页 ............................................................153
整理半结构化数据的本体关系 .............................................................................154
改进航空公司页面 .................................................................................................155
给航空公司代码加上名称 ............................................................................156
整合维基百科内容 ........................................................................................158
把扩充过的航空公司表发布到MongoDB ...................................................159
在网页上扩充航空公司信息 ........................................................................160
调查飞机（实体） .....................................................................................................162
SQL嵌套查询vs.数据流编程 ........................................................................164
不使用嵌套查询的数据流编程 ....................................................................164
Spark SQL中的子查询...................................................................................165
创建飞机主页 ................................................................................................166
在飞机页面上添加搜索 ................................................................................167
创建飞机制造商的条形图 ............................................................................172
对飞机制造商条形图进行迭代 ....................................................................174
实体解析：新一轮图表迭代 ..........................................................................177
本章小结 .................................................................................................................183
第7章　进行预测 ............................................................................. 185
预测的作用 .............................................................................................................186
预测什么 .................................................................................................................186
预测分析导论 .........................................................................................................187
进行预测 ........................................................................................................187
探索航班延误 .........................................................................................................189
使用PySpark提取特征............................................................................................193
使用scikit-learn构建回归模型 ...............................................................................198
读取数据 ........................................................................................................198
数据采样 ........................................................................................................199
向量化处理结果 ............................................................................................200
准备训练数据 ................................................................................................201
向量化处理特征 ............................................................................................201
稀疏矩阵与稠密矩阵 ....................................................................................203
准备实验 ........................................................................................................204
训练模型 ........................................................................................................204
测试模型 ........................................................................................................205
小结 ................................................................................................................207
使用Spark MLlib构建分类器.................................................................................208
使用专用结构加载训练数据 ........................................................................208
处理空值 ........................................................................................................210
用Route（路线）替代FlightNum（航班号） .....................................................210
对连续变量分桶以用于分类 ........................................................................211
使用pyspark.ml.feature向量化处理特征 ......................................................219
用Spark ML做分类 ........................................................................................221
本章小结 .................................................................................................................223
第8章　部署预测系统 ...................................................................... 225
把scikit-learn应用部署为网络服务 .......................................................................225
scikit-learn模型的保存与读取 ......................................................................226
提供预测模型的准备工作 ............................................................................227
为航班延误回归分析创建API ......................................................................228
测试API .........................................................................................................232
在产品中使用API ..........................................................................................232
使用Airflow部署批处理模式Spark ML应用 ........................................................234
在生产环境中收集训练数据 ........................................................................235
Spark ML模型的训练、存储与加载 ..............................................................237
在MongoDB中创建预测请求 .......................................................................239
从MongoDB中获取预测请求 .......................................................................245
使用Spark ML以批处理模式进行预测 ........................................................248
用MongoDB保存预测结果 ...........................................................................252
在网络应用中展示批处理预测结果 ............................................................253
用Apache Airflow（孵化项目）自动化工作流 ...............................................256
小结 ................................................................................................................264
用Spark Streaming部署流式计算模式Spark ML应用 ..........................................264
在生产环境中收集训练数据 ........................................................................265
Spark ML模型的训练、存储、读取 ................................................................265
发送预测请求到Kafka ..................................................................................266
用Spark Streaming进行预测 ..........................................................................277
测试整个系统 ................................................................................................283
本章小结 .................................................................................................................285
第9章　改进预测结果 ...................................................................... 287
解决预测的问题 .....................................................................................................287
什么时候需要改进预测 .........................................................................................288
改进预测表现 .........................................................................................................288
黏附试验法：找出黏性好的 ..........................................................................288
为试验建立严格的指标 ................................................................................289
把当日时间作为特征 ....................................................................................298
纳入飞机数据 ................................................................................................302
提取飞机特征 ................................................................................................302
在分类器模型中纳入飞机特征 ....................................................................305
纳入飞行时间 .........................................................................................................310
本章小结 .................................................................................................................313
附录A　安装手册 ............................................................................. 315
安装Hadoop ...........................................................................................................315
安装Spark ...............................................................................................................316
安装MongoDB .......................................................................................................317
安装MongoDB的Java驱动 .....................................................................................317
安装mongo-hadoop ................................................................................................318
编译mongo-hadoop .......................................................................................318
安装pymongo_spark ......................................................................................318
安装 Elasticsearch ..................................................................................................318
安装Elasticsearch的Hadoop支持库 .......................................................................319
配置我们的Spark环境 ...........................................................................................320
安装 Kafka .............................................................................................................320
安装scikit-learn ......................................................................................................320
安装Zeppelin ..........................................................................................................321
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark全栈数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据科学与大数据技术导论
目录
目 录
译者序
序
前言
致谢
关于作者
第一部分 Hadoop中的数据科学概览
第1章 数据科学概述2
1.1 数据科学究竟是什么2
1.2 示例：搜索广告3
1.3 数据科学史一瞥4
1.3.1 统计学与机器学习4
1.3.2 互联网巨头的创新5
1.3.3 现代企业中的数据科学6
1.4 数据科学家的成长之路6
1.4.1 数据工程师7
1.4.2 应用科学家7
1.4.3 过渡到数据科学家角色8
1.4.4 数据科学家的软技能9
1.5 数据科学团队的组建10
1.6 数据科学项目的生命周期11
1.6.1 问正确的问题11
1.6.2 数据摄取12
1.6.3 数据清洗：注重数据质量12
1.6.4 探索数据和设计模型特征13
1.6.5 构建和调整模型13
1.6.6 部署到生产环境14
1.7 数据科学项目的管理14
1.8 小结15
第2章 数据科学用例16
2.1 大数据—变革的驱动力16
2.1.1 容量：更多可用数据17
2.1.2 多样性：更多数据类型17
2.1.3 速度：快速数据摄取18
2.2 商业用例18
2.2.1 产品推荐18
2.2.2 客户流失分析19
2.2.3 客户细分19
2.2.4 销售线索的优先级20
2.2.5 情感分析20
2.2.6 欺诈检测21
2.2.7 预测维护22
2.2.8 购物篮分析22
2.2.9 预测医学诊断23
2.2.10 预测患者再入院23
2.2.11 检测异常访问24
2.2.12 保险风险分析24
2.2.13 预测油气井生产水平24
2.3 小结25
第3章 Hadoop与数据科学26
3.1 Hadoop 究竟为何物26
3.1.1 分布式文件系统27
3.1.2 资源管理器和调度程序28
3.1.3 分布式数据处理框架29
3.2 Hadoop的演进历史31
3.3 数据科学的Hadoop工具32
3.3.1 Apache Sqoop33
3.3.2 Apache Flume33
3.3.3 Apache Hive34
3.3.4 Apache Pig35
3.3.5 Apache Spark36
3.3.6 R37
3.3.7 Python38
3.3.8 Java机器学习软件包39
3.4 Hadoop为何对数据科学家有用39
3.4.1 成本有效的存储39
3.4.2 读取模式40
3.4.3 非结构化和半结构化数据40
3.4.4 多语言工具41
3.4.5 强大的调度和资源管理功能41
3.4.6 分布式系统抽象分层42
3.4.7 可扩展的模型创建42
3.4.8 模型的可扩展应用43
3.5 小结43
第二部分 用Hadoop准备和可视化数据
第4章 将数据导入Hadoop46
4.1 Hadoop数据湖46
4.2 Hadoop分布式文件系统47
4.3 直接传输文件到 HDFS48
4.4 将数据从文件导入Hive表49
4.5 使用Spark将数据导入Hive表52
4.5.1 使用Spark将CSV文件导入Hive52
4.5.2 使用Spark将JSON文件导入Hive54
4.6 使用Apache Sqoop获取关系数据55
4.6.1 使用Sqoop导入和导出数据55
4.6.2 Apache Sqoop版本更改56
4.6.3 使用Sqoop版本2：基本示例57
4.7 使用Apache Flume获取数据流63
4.8 使用Apache Oozie管理Hadoop工作和数据流67
4.9 Apache Falcon68
4.10 数据摄取的下一步是什么69
4.11 小结70
第5章 使用 Hadoop 进行数据再加工 71
5.1 为什么选择Hadoop做数据再加工72
5.2 数据质量72
5.2.1 什么是数据质量72
5.2.2 处理数据质量问题73
5.2.3 使用Hadoop进行数据质量控制76
5.3 特征矩阵78
5.3.1 选择“正确”的特征78
5.3.2 抽样：选择实例79
5.3.3 生成特征80
5.3.4 文本特征81
5.3.5 时间序列特征84
5.3.6 来自复杂数据类型的特征84
5.3.7 特征操作85
5.3.8 降维86
5.4 小结88
第6章 探索和可视化数据89
6.1 为什么要可视化数据89
6.1.1 示例：可视化网络吞吐量89
6.1.2 想象未曾发生的突破92
6.2 创建可视化93
6.2.1 对比图94
6.2.2 组成图96
6.2.3 分布图98
6.2.4 关系图99
6.3 针对数据科学使用可视化101
6.4 流行的可视化工具101
6.4.1 R101
6.4.2 Python：Matplotlib、Seaborn和其他102
6.4.3 SAS102
6.4.4 Matlab103
6.4.5 Julia103
6.4.6 其他可视化工具103
6.5 使用Hadoop可视化大数据103
6.6 小结104
第三部分 使用Hadoop进行数据建模
第7章 Hadoop与机器学习106
7.1 机器学习概述106
7.2 术语107
7.3 机器学习中的任务类型107
7.4 大数据和机器学习108
7.5 机器学习工具109
7.6 机器学习和人工智能的未来110
7.7 小结110
第8章 预测建模111
8.1 预测建模概述111
8.2 分类与回归112
8.3 评估预测模型113
8.3.1 评估分类器114
8.3.2 评估回归模型116
8.3.3 交叉验证117
8.4 有监督学习算法117
8.5 构建大数据预测模型的解决方案118
8.5.1 模型训练118
8.5.2 批量预测120
8.5.3 实时预测120
8.6 示例：情感分析121
8.6.1 推文数据集121
8.6.2 数据准备122
8.6.3 特征生成122
8.6.4 建立一个分类器125
8.7 小结126
第9章 聚类127
9.1 聚类概述127
9.2 聚类的使用128
9.3 设计相似性度量128
9.3.1 距离函数129
9.3.2 相似函数129
9.4 聚类算法130
9.5 示例：聚类算法131
9.5.1 k均值聚类131
9.5.2 LDA131
9.6 评估聚类和选择集群数量132
9.7 构建大数据集群解决方案133
9.8 示例：使用LDA进行主题建模134
9.8.1 特征生成135
9.8.2 运行 LDA136
9.9 小结137
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据科学与大数据技术导论
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>闪光的氰化物
第一部	罗斯玛丽
第二部	万灵节
第三部	艾丽斯
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>闪光的氰化物
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析技术与实战
第1章  Spark导论	1
1.1  Spark的发展	2
1.2  什么是Spark	3
1.3  Spark主要特征	3
1.3.1  快速	3
1.3.2  简洁易用	5
1.3.3  通用	6
1.3.4  多种运行模式	8
第2章  Spark集群部署	9
2.1  运行环境说明	9
2.1.1  软硬件环境	9
2.1.2  集群网络环境	10
2.2  安装VMware Workstation 11	10
2.3  安装CentOS 6	16
2.4  安装Hadoop	21
2.4.1  克隆并启动虚拟机	21
2.4.2  网络基本配置	24
2.4.3  安装JDK	27
2.4.4  免密钥登录配置	28
2.4.5  Hadoop配置	29
2.4.6  配置从节点	33
2.4.7  配置系统文件	33
2.4.8  启动Hadoop集群	33
2.5  安装Scala	35
2.6  安装Spark	36
2.6.1  下载并解压Spark安装包	36
2.6.2  配置Spark-env.sh	37
2.6.3  配置Spark-defaults.conf	37
2.6.4  配置Slaves	38
2.6.5  配置环境变量	38
2.6.6  发送至Slave1、Slave2	39
2.7  启动Spark	39
第3章  RDD编程	42
3.1  RDD定义	42
3.2  RDD的特性	43
3.2.1  分区	43
3.2.2  依赖	44
3.2.3  计算	45
3.2.4  分区函数	45
3.2.5  优先位置	46
3.3  创建操作	46
3.3.1  基于集合的创建操作	47
3.3.2  基于外部存储的创建操作	47
3.4  常见执行操作	49
3.5  常见转换操作	49
3.5.1  一元转换操作	50
3.5.2  二元转换操作	53
3.6  持久化操作	56
3.7  存储操作	58
第4章  Spark调度管理与应用程序开发	59
4.1  Spark调度管理基本概念	59
4.2  作业调度流程	60
4.2.1  作业的生成与提交	61
4.2.2  阶段的划分	62
4.2.3  调度阶段的提交	62
4.2.4  任务的提交与执行	62
4.3  基于IntelliJ IDEA构建Spark应用程序	64
4.3.1  安装IntelliJ IDEA	64
4.3.2  创建Spark应用程序	70
4.3.3  集群模式运行Spark应用程序	81
第5章  GraphX	87
5.1  GraphX概述	87
5.2  GraphX基本原理	89
5.2.1  图计算模型处理流程	89
5.2.2  GraphX定义	90
5.2.3  GraphX的特点	90
5.3  GraphX设计与实现	91
5.3.1  弹性分布式属性图	91
5.3.2  图的数据模型	92
5.3.3  图的存储模型	94
5.3.4  GraphX模型框架	97
5.4  GraphX操作	97
5.4.1  创建图	97
5.4.2  基本属性操作	100
5.4.3  结构操作	102
5.4.4  转换操作	103
5.4.5  连接操作	105
5.4.6  聚合操作	106
5.5  GraphX案例解析	107
5.5.1  PageRank算法与案例解析	107
5.5.2  Triangle Count算法与案例解析	110
第6章  Spark SQL	113
6.1  Spark SQL概述	113
6.2  Spark SQL逻辑架构	116
6.2.1  SQL执行流程	116
6.2.2  Catalyst	117
6.3  Spark SQL CLI	117
6.3.1  硬软件环境	117
6.3.2  集群环境	118
6.3.3  结合Hive	118
6.3.4  启动Hive	118
6.4  DataFrame编程模型	119
6.4.1  DataFrame简介	119
6.4.2  创建DataFrames	120
6.4.3  保存DataFrames	126
6.5  DataFrame常见操作	127
6.5.1  数据展示	127
6.5.2  常用列操作	128
6.5.3  过滤	131
6.5.4  排序	132
6.5.5  其他常见操作	134
6.6  基于Hive的学生信息管理系统的SQL查询案例与解析	137
6.6.1  Spark SQL整合Hive	137
6.6.2  构建数据仓库	138
6.6.3  加载数据	141
6.6.4  查询数据	142
第7章  Spark Streaming	146
7.1  Spark Streaming概述	146
7.2  Spark Streaming基础概念	147
7.2.1  批处理时间间隔	147
7.2.2  窗口时间间隔	148
7.2.3  滑动时间间隔	148
7.3  DStream基本概念	149
7.4  DStream的基本操作	150
7.4.1  无状态转换操作	150
7.4.2  有状态转换操作	152
7.4.3  输出操作	153
7.4.4  持久化操作	154
7.5  数据源	154
7.5.1  基础数据源	154
7.5.2  高级数据源	155
7.6  Spark Streaming编程模式与案例分析	156
7.6.1  Spark Streaming编程模式	156
7.6.2  文本文件数据处理案例（一）	157
7.6.3  文本文件数据处理案例（二）	160
7.6.4  网络数据处理案例（一）	164
7.6.5  网络数据处理案例（二）	171
7.6.6  stateful应用案例	175
7.6.7  window应用案例	180
7.7  性能考量	185
7.7.1  运行时间优化	185
7.7.2  内存使用与垃圾回收	186
第8章  Spark MLlib	187
8.1  Spark MLlib概述	187
8.1.1  机器学习介绍	187
8.1.2  Spark MLlib简介	189
8.2  MLlib向量与矩阵	190
8.2.1  MLlib向量	190
8.2.2  MLlib矩阵	192
8.3  Spark MLlib分类算法	196
8.3.1  贝叶斯分类算法	197
8.3.2  支持向量机算法	201
8.3.3  决策树算法	204
8.4  MLlib线性回归算法	208
8.5  MLlib聚类算法	212
8.6  MLlib协同过滤	215
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据分析技术与实战
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>快学Scala（第2版）
译者序. V
第1版序.XVII
前言.XIX
作者简介.XXIII
第1章　基础 A1  1
1.1　Scala解释器 1
1.2　声明值和变量 4
1.3　常用类型5
1.4　算术和操作符重载7
1.5　关于方法调用 8
1.6　apply方法 9
1.7　Scaladoc11
练习16
第2章　控制结构和函数 A1  19
2.1　条件表达式20
2.2　语句终止 22
2.3　块表达式和赋值 22
2.4　输入和输出23
2.5　循环25
2.6　高级for循环27
2.7　函数28
2.8　默认参数和带名参数 L1 29
2.9　变长参数 L1 29
2.10　过程 31
2.11　懒值 L1 31
2.12　异常 32
练习35
第3章　数组相关操作 A1 39
3.1　定长数组 39
3.2　变长数组：数组缓冲40
3.3　遍历数组和数组缓冲41
3.4　数组转换 42
3.5　常用算法 44
3.6　解读Scaladoc 45
3.7　多维数组 47
3.8　与Java的互操作48
练习49
第4章　映射和元组 A1 53
4.1　构造映射 53
4.2　获取映射中的值 54
4.3　更新映射中的值 55
4.4　迭代映射 56
4.5　已排序映射57
4.6　与Java的互操作57
4.7　元组58
4.8　拉链操作 59
练习60
第5章　类 A1 63
5.1　简单类和无参方法 63
5.2　带getter和setter的属性 64
5.3　只带getter的属性 67
5.4　对象私有字段68
5.5　Bean属性 L1 69
5.6　辅助构造器71
5.7　主构造器 72
5.8　嵌套类 L1 75
练习78
第6章　对象 A1  81
6.1　单例对象 81
6.2　伴生对象 82
6.3　扩展类或特质的对象83
6.4　apply方法84
6.5　应用程序对象85
6.6　枚举86
练习87
第7章　包和引入 A1  91
7.1　包 91
7.2　作用域规则93
7.3　串联式包语句95
7.4　文件顶部标记法 95
7.5　包对象 96
7.6　包可见性 97
7.7　引入97
7.8　任何地方都可以声明引入 98
7.9　重命名和隐藏方法 99
7.10　隐式引入 99
练习 100
第8章　继承 A1 103
8.1　扩展类103
8.2　重写方法 104
8.3　类型检查和转换105
8.4　受保护字段和方法 106
8.5　超类的构造 106
8.6　重写字段 107
8.7　匿名子类 109
8.8　抽象类109
8.9　抽象字段 110
8.10　构造顺序和提前定义 L3 110
8.11　Scala类继承关系 112
8.12　对象相等性 L1114
8.13　值类 L2 116
练习 117
第9章　文件和正则表达式 A1 121
9.1　读取行121
9.2　读取字符 122
9.3　读取词法单元和数字 123
9.4　从URL或其他源读取 124
9.5　读取二进制文件124
9.6　写入文本文件 124
9.7　访问目录 125
9.8　序列化125
9.9　进程控制 A2126
9.10　正则表达式129
9.11　正则表达式组 130
练习 131
第10章　特质 A1  135
10.1　为什么没有多重继承 135
10.2　当作接口使用的特质 137
10.3　带有具体实现的特质 138
10.4　带有特质的对象 139
10.5　叠加在一起的特质140
10.6　在特质中重写抽象方法 141
10.7　当作富接口使用的特质 142
10.8　特质中的具体字段143
10.9　特质中的抽象字段144
10.10　特质构造顺序145
10.11　初始化特质中的字段147
10.12　扩展类的特质148
10.13　自身类型 L2 149
10.14　背后发生了什么 151
练习 152
第11章　操作符 A1  157
11.1　标识符 157
11.2　中置操作符158
11.3　一元操作符159
11.4　赋值操作符160
11.5　优先级 161
11.6　结合性 162
11.7　apply和update方法162
11.8　提取器 L2 164
11.9　带单个参数或无参数的提取器 L2 166
11.10　unapplySeq方法 L2 167
11.11　动态调用 L2 167
练习 171
第12章　高阶函数 L1 175
12.1　作为值的函数 175
12.2　匿名函数177
12.3　带函数参数的函数178
12.4　参数（类型）推断179
12.5　一些有用的高阶函数 180
12.6　闭包 181
12.7　SAM转换182
12.8　柯里化 183
12.9　控制抽象185
12.10　return表达式 186
练习 187
第13章　集合 A2  191
13.1　主要的集合特质 192
13.2　可变和不可变集合193
13.3　序列 195
13.4　列表 196
13.5　集197
13.6　用于添加或去除元素的操作符 198
13.7　常用方法201
13.8　将函数映射到集合203
13.9　化简、折叠和扫描 A3205
13.10　拉链操作 209
13.11　迭代器 210
13.12　流 A3 211
13.13　懒视图 A3213
13.14　与Java集合的互操作 213
13.15　并行集合 215
练习 217
第14章　模式匹配和样例类 A2  221
14.1　更好的switch222
14.2　守卫 223
14.3　模式中的变量 223
14.4　类型模式224
14.5　匹配数组、列表和元组 225
14.6　提取器 227
14.7　变量声明中的模式227
14.8　for表达式中的模式229
14.9　样例类 229
14.10　copy方法和带名参数 230
14.11　case语句中的中置表示法231
14.12　匹配嵌套结构232
14.13　样例类是邪恶的吗 233
14.14　密封类 234
14.15　模拟枚举 235
14.16　Option类型 235
14.17 偏函数 L2 236
练习 238
第15章　注解 A2  243
15.1　什么是注解243
15.2　什么可以被注解 244
15.3　注解参数245
15.4　注解实现246
15.5　针对Java特性的注解247
15.5.1　Java修饰符 247
15.5.2　标记接口 248
15.5.3　受检异常 249
15.5.4　变长参数 249
15.5.5　JavaBeans 250
15.6　用于优化的注解 250
15.6.1　尾递归250
15.6.2　跳转表生成与内联 252
15.6.3　可省略方法 253
15.6.4　基本类型的特殊化 254
15.7　用于错误和警告的注解 255
练习 256
第16章　XML处理 A2 259
16.1　XML字面量 260
16.2　XML节点 260
16.3　元素属性262
16.4　内嵌表达式263
16.5　在属性中使用表达式 264
16.6　特殊节点类型 265
16.7　类XPath表达式266
16.8　模式匹配267
16.9　修改元素和属性 268
16.10　XML变换269
16.11　加载和保存 270
16.12　命名空间 273
练习 275
第17章　Future A2  277
17.1　在future中运行任务 278
17.2　等待结果280
17.3　Try类 281
17.4　回调 282
17.5　组合future任务 283
17.6　其他future变换 286
17.7　Future对象中的方法288
17.8　Promise.289
17.9　执行上下文291
练习 292
第18章　类型参数 L2 297
18.1　泛型类 298
18.2　泛型函数298
18.3　类型变量界定 298
18.4　视图界定300
18.5　上下文界定301
18.6　ClassTag上下文界定 301
18.7　多重界定302
18.8　类型约束 L3 302
18.9　型变 304
18.10　协变和逆变点305
18.11　对象不能泛型307
18.12　类型通配符 308
练习 309
第19章　高级类型 L2 313
19.1　单例类型313
19.2　类型投影315
19.3　路径 316
19.4　类型别名317
19.5　结构类型318
19.6　复合类型319
19.7　中置类型320
19.8　存在类型321
19.9　Scala类型系统 322
19.10　自身类型 323
19.11　依赖注入 325
19.12　抽象类型 L3 327
19.13　家族多态 L3 329
19.14　高等类型 L3 333
练习 336
第20章　解析 A3  341
20.1　文法 342
20.2　组合解析器操作 343
20.3　解析器结果变换 345
20.4　丢弃词法单元 347
20.5　生成解析树348
20.6　避免左递归348
20.7　更多的组合子 350
20.8　避免回溯352
20.9　记忆式解析器 353
20.10　解析器说到底是什么354
20.11　正则解析器 355
20.12　基于词法单元的解析器 356
20.13　错误处理 358
练习 359
第21章　隐式转换和隐式参数 L3 363
21.1　隐式转换363
21.2　利用隐式转换丰富现有类库的功能 364
21.3　引入隐式转换 365
21.4　隐式转换规则 367
21.5　隐式参数368
21.6　利用隐式参数进行隐式转换 370
21.7　上下文界定371
21.8　类型类 372
21.9　类型证明374
21.10　@implicitNotFound注解.376
21.11　CanBuildFrom解读376
练习 379
词汇表 381
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>快学Scala（第2版）
