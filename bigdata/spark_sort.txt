>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速数据处理
译者序
作者简介
前言
第1章　安装Spark以及构建Spark集群 / 1
1.1　单机运行Spark / 4
1.2　在EC2上运行Spark / 5
1.3　在ElasticMapReduce上部署Spark / 11
1.4　用Chef(opscode)部署Spark / 12
1.5　在Mesos上部署Spark / 14
1.6　在Yarn上部署Spark / 15
1.7　通过SSH部署集群 / 16
1.8　链接和参考 / 21
1.9　小结 / 21
第2章　Spark shell的使用 / 23
2.1　加载一个简单的text文件 / 24
2.2　用Spark shell运行逻辑回归 / 26
2.3　交互式地从S3加载数据 / 28
2.4　小结 / 30
第3章　构建并运行Spark应用 / 31
3.1　用sbt构建Spark作业 / 32
3.2　用Maven构建Spark作业 / 36
3.3　用其他工具构建Spark作业 / 39
3.4　小结 / 39
第4章　创建SparkContext / 41
4.1　Scala / 43
4.2　Java / 43
4.3　Java和Scala共享的API / 44
4.4　Python / 45
4.5　链接和参考 / 45
4.6　小结 / 46
第5章　加载与保存数据 / 47
5.1　RDD / 48
5.2　加载数据到RDD中 / 49
5.3　保存数据 / 54
5.4　连接和参考 / 55
5.5　小结 / 55
第6章　操作RDD / 57
6.1　用Scala和Java操作RDD / 58
6.2　用Python操作RDD / 79
6.3　链接和参考 / 83
6.4　小结 / 84
第7章　Shark-Hive和Spark的综合运用 / 85
7.1　为什么用Hive/Shark / 86
7.2　安装Shark / 86
7.3　运行Shark / 88
7.4　加载数据 / 88
7.5　在Spark程序中运行HiveQL查询 / 89
7.6　链接和参考 / 92
7.7　小结 / 93
第8章　测试 / 95
8.1　用Java和Scala测试 / 96
8.2　用Python测试 / 103
8.3　链接和参考 / 104
8.4　小结 / 105
第9章　技巧和窍门 / 107
9.1　日志位置 / 108
9.2　并发限制 / 108
9.3　内存使用与垃圾回收 / 109
9.4　序列化 / 110
9.5　IDE集成环境 / 111
9.6　Spark与其他语言 / 112
9.7　安全提示 / 113
9.8　邮件列表 / 113
9.9　链接和参考 / 113
9.10　小结 / 114
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速数据处理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理技术
第1章 Spark系统概述 1
1.1 大数据处理框架 1
1.2 Spark大数据处理框架 3
1.2.1 RDD表达能力 3
1.2.2 Spark子系统 4
1.3 小结 7
第2章 Spark RDD及编程接口 9
2.1 Spark程序“Hello World” 9
2.2 Spark RDD 12
2.2.1 RDD分区（partitions） 13
2.2.2 RDD优先位置（preferredLocations） 13
2.2.3 RDD依赖关系（dependencies） 15
2.2.4 RDD分区计算（compute） 19
2.2.5 RDD分区函数（partitioner） 20
2.3 创建操作 23
2.3.1 集合创建操作 23
2.3.2 存储创建操作 23
2.4 转换操作 26
2.4.1 RDD基本转换操作 26
2.4.2 键值RDD转换操作 35
2.4.3 再论RDD依赖关系 43
2.5 控制操作（control operation） 46
2.6 行动操作（action operation） 47
2.6.1 集合标量行动操作 47
2.6.2 存储行动操作 52
2.7 小结 56
第3章 Spark运行模式及原理 57
3.1 Spark运行模式概述 57
3.1.1 Spark运行模式列表 57
3.1.2 Spark基本工作流程 58
3.1.3 相关基本类 59
3.2 Local模式 62
3.2.1 部署及程序运行 62
3.2.2 内部实现原理 63
3.3 Standalone模式 64
3.3.1 部署及程序运行 64
3.3.2 内部实现原理 67
3.4 Local cluster模式 68
3.4.1 部署及程序运行 68
3.4.2 内部实现原理 69
3.5 Mesos模式 69
3.5.1 部署及程序运行 69
3.5.2 内部实现原理 70
3.6 YARN standalone / YARN cluster模式 72
3.6.1 部署及程序运行 72
3.6.2 内部实现原理 75
3.7 YARN client模式 76
3.7.1 部署及程序运行 76
3.7.2 内部实现原理 77
3.8 各种模式的实现细节比较 78
3.8.1 环境变量的传递 78
3.8.2 JAR包和各种依赖文件的分发 80
3.8.3 任务管理和序列化 82
3.8.4 用户参数配置 83
3.8.5 用户及权限控制 84
3.9 Spark 1.0版本之后的变化 85
3.10 小结 86
第4章 Spark调度管理原理 87
4.1 Spark作业调度管理概述 87
4.2 Spark调度相关基本概念 88
4.3 作业调度模块顶层逻辑概述 89
4.4 作业调度具体工作流程 92
4.4.1 调度阶段的拆分 94
4.4.2 调度阶段的提交 97
4.4.3 任务集的提交 99
4.4.4 完成状态的监控 99
4.4.5 任务结果的获取 101
4.5 任务集管理模块详解 102
4.6 调度池和调度模式分析 104
4.7 其他调度相关内容 106
4.7.1 Spark应用之间的调度关系 106
4.7.2 调度过程中的数据本地性问题 106
4.8 小结 107
第5章 Spark的存储管理 109
5.1 存储管理模块整体架构 109
5.1.1 通信层架构 110
5.1.2 通信层消息传递 112
5.1.3 注册存储管理模块 113
5.1.4 存储层架构 114
5.1.5 数据块 (Block) 116
5.2 RDD 持久化 116
5.2.1 RDD分区和数据块的关系 117
5.2.2 内存缓存 118
5.2.3 磁盘缓存 119
5.2.4 持久化选项 120
5.2.5 如何选择不同的持久化选项 122
5.3 Shuffle数据持久化 122
5.4 广播（Broadcast）变量持久化 125
5.5 小结 126
第6章 Spark监控管理 127
6.1 UI管理 127
6.1.1 实时UI管理 128
6.1.2 历史UI管理 132
6.2 Metrics管理 133
6.2.1 Metrics系统架构 133
6.2.2 Metrics系统配置 135
6.2.3 输入源（Metrics Source）介绍 136
6.2.4 输出方式（Metrics Sink）介绍 138
6.3 小结 139
第7章 Shark架构与安装配置 141
7.1 Shark架构浅析 142
7.2 Hive/Shark各功能组件对比 143
7.2.1 MetaStore 143
7.2.2 CLI/ Beeline 143
7.2.3 JDBC/ODBC 144
7.2.4 Hive Server/2 与 Shark Server/2 144
7.2.5 Driver 145
7.2.6 SQL Parser 146
7.2.7 查询优化器（Query Optimizer） 147
7.2.8 物理计划与执行 147
7.3 Shark安装配置与使用 148
7.3.1 安装前准备工作 149
7.3.2 在不同运行模式下安装Shark 149
7.4 Shark SQL命令行工具（CLI） 152
7.5 使用Shark Shell命令 155
7.6 启动Shark Server 155
7.7 Shark Server2配置与启动 156
7.8 缓存数据表 157
7.8.1 数据缓存级别 158
7.8.2 创建不同缓存级别的Shark数据表 158
7.8.3 指定数据表缓存策略 159
7.8.4 使用Tachyon 160
7.9 常见问题分析 160
7.9.1 OutOfMemory异常 160
7.9.2 数据处理吞吐量低 161
7.9.3 Shark查询比Hive慢 161
7.10 小结 162
第8章 SQL程序扩展 163
8.1 程序扩展并行运行模式 164
8.2 Evaluator和ObjectInspector 164
8.3 自定义函数扩展 168
8.3.1 自定义函数扩展分类 168
8.3.2 CLI中的用户自定义函数扩展相关命令 170
8.3.3 用户自定义函数（UDF） 171
8.3.4 通用用户自定义函数（Generic UDF） 175
8.3.5 用户自定义聚合函数（UDAF） 178
8.3.6 通用用户自定义聚合函数（Generic UDAF） 182
8.3.7 通用用户自定义表函数（Generic UDTF） 186
8.4 自定义数据存取格式 190
8.4.1 SerDe 190
8.4.2 StorageHandler 197
8.5 小结 198
第9章 Spark SQL 199
9.1 Spark SQL逻辑架构 199
9.1.1 Catalyst功能边界 200
9.1.2 SQL解析阶段 201
9.1.3 逻辑计划元数据绑定和语义分析阶段 202
9.1.4 逻辑计划优化阶段 202
9.1.5 物理计划生成阶段 202
9.1.6 Shark和Spark SQL对比 203
9.2 Catalyst上下文（Context） 204
9.2.1 SQLContext 204
9.2.2 HiveContext 205
9.3 SQL DSL API 206
9.3.1 数据源管理 206
9.3.2 SchemaRDD 208
9.3.3 Row API 210
9.3.4 数据类型 211
9.3.5 DSL API举例 213
9.3.6 表达式计算 214
9.3.7 Parquet列式存储文件 218
9.3.8 代码演示 218
9.4 Java API 221
9.5 Python API 224
9.6 Spark SQL CLI 225
9.7 Thrift服务 225
9.8 小结 225
第10章 Spark Streaming流数据处理框架 227
10.1 快速入门 227
10.2 Spark Streaming基本概念 229
10.2.1 链接和初始化 229
10.2.2 时间和窗口概念 231
10.2.3 DStream原理 232
10.2.4 DStream输入源 234
10.2.5 DStream 操作 235
10.2.6 DStream持久化 237
10.3 性能调优 238
10.3.1 运行时间优化 238
10.3.2 内存使用优化 238
10.4 容错处理 239
10.4.1 工作节点失效 239
10.4.2 驱动节点失效 240
10.5 DStream作业的产生和调度 242
10.5.1 作业产生 242
10.5.2 作业调度 243
10.5.3 Streaming作业与Spark作业之间的关系 244
10.6 DStream与RDD关系 246
10.7 数据接收原理 248
10.8 自定义数据输入源 251
10.9 自定义监控接口（StreamingListener） 253
10.10 Spark Streaming案例分析 254
10.11 小结 256
第11章 GraphX计算框架 259
11.1 图并行计算 259
11.1.1 数据并行与图并行计算 259
11.1.2 图并行计算框架简介 260
11.1.3 GraphX简介 264
11.2 GraphX模型设计 264
11.2.1 数据模型 264
11.2.2 图计算接口 265
11.3 GraphX模型实现 269
11.3.1 图的分布式存储 269
11.3.2 图操作执行策略 278
11.3.3 图操作执行优化 280
11.3.4 序列化和反序列化 283
11.3.5 GraphX内置算法库 284
11.4 GraphX应用 285
11.4.1 Pregel模型 285
11.4.2 N维邻接关系计算 288
11.5 小结 291
第12章 Tachyon存储系统 293
12.1 设计原理 294
12.1.1 高效的内存读写 294
12.1.2 无副本的可靠性实现——Lineage 297
12.2 框架设计 299
12.2.1 主节点 300
12.2.2 工作节点 304
12.2.3 客户端 306
12.2.4 读写工作流程 307
12.3 Tachyon的部署 313
12.3.1 单机部署 313
12.3.2 分布式部署 316
12.3.3 Tachyon的配置 317
12.4 Tachyon应用 321
12.4.1 Shark原始表（RawTable） 321
12.4.2 Spark的堆外RDD 325
12.4.3 Tachyon用户接口（API） 327
12.5 相关项目讨论 335
12.6 小结 336
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理技术
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速大数据分析
目录
推荐序 xi
译者序 xiv
序 xvi
前言 xvii
第1章 Spark数据分析导论 1
1.1 Spark是什么 1
1.2 一个大一统的软件栈 2
1.2.1 Spark Core 2
1.2.2 Spark SQL 3
1.2.3 Spark Streaming 3
1.2.4 MLlib 3
1.2.5 GraphX 3
1.2.6 集群管理器 4
1.3 Spark的用户和用途 4
1.3.1 数据科学任务 4
1.3.2 数据处理应用 5
1.4 Spark简史 5
1.5 Spark的版本和发布 6
1.6 Spark的存储层次 6
第2章 Spark下载与入门 7
2.1 下载Spark 7
2.2 Spark中Python和Scala的shell 9
2.3 Spark 核心概念简介 12
2.4 独立应用 14
2.4.1 初始化SparkContext 15
2.4.2 构建独立应用 16
2.5 总结 19
第3章 RDD编程 21
3.1 RDD基础 21
3.2 创建RDD 23
3.3 RDD操作 24
3.3.1 转化操作 24
3.3.2 行动操作 26
3.3.3 惰性求值 27
3.4 向Spark传递函数 27
3.4.1 Python 27
3.4.2 Scala 28
3.4.3 Java 29
3.5 常见的转化操作和行动操作 30
3.5.1 基本RDD 30
3.5.2 在不同RDD类型间转换 37
3.6 持久化( 缓存) 39
3.7 总结 40
第4章 键值对操作 41
4.1 动机 41
4.2 创建Pair RDD 42
4.3 Pair RDD的转化操作 42
4.3.1 聚合操作 45
4.3.2 数据分组 49
4.3.3 连接 50
4.3.4 数据排序 51
4.4 Pair RDD的行动操作 52
4.5 数据分区（进阶） 52
4.5.1 获取RDD的分区方式 55
4.5.2 从分区中获益的操作 56
4.5.3 影响分区方式的操作 57
4.5.4 示例：PageRank 57
4.5.5 自定义分区方式 59
4.6 总结 61
第5章 数据读取与保存 63
5.1 动机 63
5.2 文件格式 64
5.2.1 文本文件 64
5.2.2 JSON 66
5.2.3 逗号分隔值与制表符分隔值 68
5.2.4 SequenceFile 71
5.2.5 对象文件 73
5.2.6 Hadoop输入输出格式 73
5.2.7 文件压缩 77
5.3 文件系统 78
5.3.1 本地/“常规”文件系统 78
5.3.2 Amazon S3 78
5.3.3 HDFS 79
5.4 Spark SQL中的结构化数据 79
5.4.1 Apache Hive 80
5.4.2 JSON 80
5.5 数据库 81
5.5.1 Java数据库连接 81
5.5.2 Cassandra 82
5.5.3 HBase 84
5.5.4 Elasticsearch 85
5.6 总结 86
第6章 Spark编程进阶 87
6.1 简介 87
6.2 累加器 88
6.2.1 累加器与容错性 90
6.2.2 自定义累加器 91
6.3 广播变量 91
6.4 基于分区进行操作 94
6.5 与外部程序间的管道 96
6.6 数值RDD 的操作 99
6.7 总结 100
第7章 在集群上运行Spark 101
7.1 简介 101
7.2 Spark运行时架构 101
7.2.1 驱动器节点 102
7.2.2 执行器节点 103
7.2.3 集群管理器 103
7.2.4 启动一个程序 104
7.2.5 小结 104
7.3 使用spark-submit 部署应用 105
7.4 打包代码与依赖 107
7.4.1 使用Maven构建的用Java编写的Spark应用 108
7.4.2 使用sbt构建的用Scala编写的Spark应用 109
7.4.3 依赖冲突 111
7.5 Spark应用内与应用间调度 111
7.6 集群管理器 112
7.6.1 独立集群管理器 112
7.6.2 Hadoop YARN 115
7.6.3 Apache Mesos 116
7.6.4 Amazon EC2 117
7.7 选择合适的集群管理器 120
7.8 总结 121
第8章 Spark调优与调试 123
8.1 使用SparkConf配置Spark 123
8.2 Spark执行的组成部分：作业、任务和步骤 127
8.3 查找信息 131
8.3.1 Spark网页用户界面 131
8.3.2 驱动器进程和执行器进程的日志 134
8.4 关键性能考量 135
8.4.1 并行度 135
8.4.2 序列化格式 136
8.4.3 内存管理 137
8.4.4 硬件供给 138
8.5 总结 139
第9章 Spark SQL 141
9.1 连接Spark SQL 142
9.2 在应用中使用Spark SQL 144
9.2.1 初始化Spark SQL 144
9.2.2 基本查询示例 145
9.2.3 SchemaRDD 146
9.2.4 缓存 148
9.3 读取和存储数据 149
9.3.1 Apache Hive 149
9.3.2 Parquet 150
9.3.3 JSON 150
9.3.4 基于RDD 152
9.4 JDBC/ODBC服务器 153
9.4.1 使用Beeline 155
9.4.2 长生命周期的表与查询 156
9.5 用户自定义函数 156
9.5.1 Spark SQL UDF 156
9.5.2 Hive UDF 157
9.6 Spark SQL性能 158
9.7 总结 159
第10章 Spark Streaming 161
10.1 一个简单的例子 162
10.2 架构与抽象 164
10.3 转化操作 167
10.3.1 无状态转化操作 167
10.3.2 有状态转化操作 169
10.4 输出操作 173
10.5 输入源 175
10.5.1 核心数据源 175
10.5.2 附加数据源 176
10.5.3 多数据源与集群规模 179
10.6 24/7不间断运行 180
10.6.1 检查点机制 180
10.6.2 驱动器程序容错 181
10.6.3 工作节点容错 182
10.6.4 接收器容错 182
10.6.5 处理保证 183
10.7 Streaming用户界面 183
10.8 性能考量 184
10.8.1 批次和窗口大小 184
10.8.2 并行度 184
10.8.3 垃圾回收和内存使用 185
10.9 总结 185
第11章 基于MLlib的机器学习 187
11.1 概述 187
11.2 系统要求 188
11.3 机器学习基础 189
11.4 数据类型 192
11.5 算法 194
11.5.1 特征提取 194
11.5.2 统计 196
11.5.3 分类与回归 197
11.5.4 聚类 202
11.5.5 协同过滤与推荐 203
11.5.6 降维 204
11.5.7 模型评估 206
11.6 一些提示与性能考量 206
11.6.1 准备特征 206
11.6.2 配置算法 207
11.6.3 缓存RDD以重复使用 207
11.6.4 识别稀疏程度 207
11.6.5 并行度 207
11.7 流水线API 208
11.8 总结 209
作者简介 210
封面介绍 210
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark快速大数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据Spark企业级实战 : 决胜大数据时代Spark全系列书籍
第1章　Spark编程模型 1
1.1 Spark：一体化、多元化的高速
大数据通用计算平台和库 1
1.1.1 为什么需要使用Spark 5
1.1.2 Spark技术生态系统简介 9
1.2 Spark大数据处理框架 20
1.2.1 Spark速度为何如此之快 20
1.2.2 RDD：分布式函数式编程 24
1.3 Spark子框架解析 28
1.3.1 图计算框架Spark GraphX 28
1.3.2 实时流处理框架（Spark Streaming） 41
1.3.3 交互式SQL处理框架Spark SQL 46
1.3.4 机器学习框架（Spark MLlib） 49
第2章　构建Spark分布式集群 55
2.1 搭建Hadoop单机版本和伪分布式开发环境 55
2.1.1 开发Hadoop需要的基本软件 56
2.1.2 安装每个软件 58
2.1.3 配置Hadoop单机模式并运行Wordcount示例 76
2.1.3 配置Hadoop伪分布模式并运行Wordcount示例 84
2. 2 搭建 Hadoop分布式集群的 92
2.2.1 在VMWare 中准备第二、第三台运行Ubuntu系统的机器 92
2.2.2 按照配置伪分布式模式的方式配置新创建运行Ubuntu系统的机器 93
2.2.3 配置Hadoop分布式集群环境 94
2.2.4 测试Hadoop分布式集群环境 105
2.3 Spark集群的动手搭建 108
2.3.1 Spark集群需要的软件 108
2.3.2 安装每个软件 110
2.3.3 启动并查看集群的状况 116
2.4 构建Hadoop单机版本和伪分布式环境 120
2.4.1 通过Spark的shell测试Spark的工作 121
2.4.2 使用Spark的cache机制观察一下效率的提升 125
第3章　Spark开发环境及其测试 129
3.1 搭建和设置IDEA开发环境 129
3.1.1 构建Spark的IDE开发环境 129
3.1.2 配置Spark的IDE开发环境 132
3.2 测试IDEA环境 146
3.3 实战：在IDEA中开发代码，并运行在Spark集群中 148
第4章　Spark RDD与编程API实战 159
4.1 深度解析Spark RDD 159
4.2 Transformation Operations动手实战 165
4.3 Action Operations动手实战 175
4.4 Spark API综合实战 179
第5章　Spark运行模式深入解析 191
5.1 Spark运行模式概述 192
5.1.1 Spark的运行模式列表 196
5.1.2 Spark的基本工作流程 197
5.2 Standalone模式 201
5.2.1 部署及程序运行 202
5.2.2 内部实现原理 206
5.3 Yarn-Cluster模式 234
5.3.1 部署及程序运行 235
5.3.2 内部实现原理 237
5.4 Yarn-Client模式 243
5.4.1 部署及运行程序 243
5.4.2 内部实现原理 244
第6章　Spark内核解析 247
6.1 Spark内核初探 247
6.1.1 Spark内核核心术语解析 247
6.1.2 Spark集群概览 250
6.1.3 Spark核心组件 251
6.1.4 Spark任务调度系统初见 252
6.2 Spark内核核心源码解读 256
6.2.1 SparkContext核心源码解析初体验 256
6.2.2 TaskSceduler启动源码解析初体验 260
6.2.3 DAGScheduler源码解读初体验 261
6.2.4 Spark的Web监控页面 262
6.3 以RDD的count操作为例触发Job全生命周期源码研究 263
6.4 Akka驱动下的Driver、Master、Worker 276
6.4.1 Driver中的AppClient源码解析 276
6.4.2 AppClient注册Master 279
6.4.3 Worker中Executor启动过程源代码解析 282
第7章　GraphX大规模图计算与图挖掘实战 287
7.1 Spark GraphX概览 288
7.2 Spark GraphX设计实现的核心原理 291
7.3 Table operator和Graph Operator 295
7.4 Vertices、edges、triplets 296
7.5 以最原始的方式构建graph 299
7.6 动手编写第一个Graph代码实例并进行Vertices、edges、triplets操作 299
7.7 在Spark集群上使用文件中的数据加载成为graph并进行操作 310
7.8 在Spark集群上掌握比较重要的图操作 320
7.9 Spark GraphX图算法 342
7.10 淘宝对Spark GraphX的大规模使用 347
第8章　Spark SQL原理与实战 349
8.1 为什么使用Spark SQL 349
8.1.1 Spark SQL的发展历程 349
8.1.2 Spark SQL的性能 351
8.2 Spark SQL运行架构 355
8.2.1 Tree和Rule 357
8.2.2 sqlContext的运行过程 360
8.2.3 hiveContext的运行过程 362
8.2.4 catalyst优化器 365
8.3 解析Spark SQL组件 367
8.3.1 LogicalPlan 367
8.3.2 SqlParser 370
8.3.3 Analyzer 378
8.3.4 Optimizer 381
8.4 深入了解Spark SQL运行的计划 383
8.4.1 hive/console的安装过程和原理 383
8.4.2 常用操作 386
8.4.3 不同数据源的运行计划 388
8.4.4 不同查询的运行计划 391
8.4.5 查询的优化 393
8.5 搭建测试环境 396
8.5.1 搭建虚拟集群（Hadoop1、Hadoop2、Hadoop3） 397
8.5.2 搭建客户端 398
8.5.3 文件数据的准备工作 399
8.5.4 Hive数据的准备工作 399
8.6 Spark SQL之基础应用 400
8.6.1 sqlContext的基础应用 402
8.6.2 hiveContext的基础应用 405
8.6.3 混合使用 408
8.6.4 缓存的使用 409
8.6.5 DSL的使用 410
8.7 ThriftServer和CLI 411
8.7.1 令人惊讶的CLI 411
8.7.2 ThriftServer 414
8.8 Spark SQL之综合应用 418
8.8.1 店铺分类 419
8.8.2 PageRank 421
8.9 Spark SQL之调优 424
8.9.1 并行性 424
8.9.2 高效的数据格式 425
8.9.3 内存的使用 427
8.9.4 合适的Task 428
8.9.5 其他的一些建议 428
第9章　Machine Learning on Spark 431
9.1 Spark MLlib机器学习 431
9.1.1 机器学习快速入门 432
9.1.2 Spark MLlib介绍 442
9.1.3 Spark MLlib架构解析 447
9.1.4 Spark Mllib核心解析 458
9.2 MLlib经典算法解析和案例实战 462
9.2.1 Linear Regression解析和实战 462
9.2.2 K-Means解析和实战 484
9.2.3 协同过滤算法分析和案例实战 502
9.3 MLLib其他常用算法解析和代码实战 552
9.3.1 Basic Statics解析和实战 553
9.3.2 MLlib朴素贝叶斯解析和实战 560
9.3.3 MLlib决策树解析和实战 562
第10章　Tachyon文件系统 565
10.1 Tachyon文件系统概述 565
10.1.1 Tachyon文件系统简介 565
10.1.2 HDFS与Tachyon 566
10.1.3 Tachyon设计原理 568
10.2 Tachyon入门 568
10.2.1 Tachyon部署 568
10.2.2 Tachyon API的使用 570
10.2.3 在MapReduce、Spark上使用Tachyon 572
10.3 Tachyon深度解析 573
10.3.1 Tachyon整体设计概述 573
10.3.2 Tachyon Master启动流程分析 574
10.3.3 Tachyon Worker启动流程分析 577
10.3.4 客户端读写文件源码分析 577
10.4 Tachyon配置参数一览 579
10.5 小结 580
第11章　Spark Streaming原理与实战 581
11.1 Spark Streaming原理 581
11.1.1 原理和运行场景 581
11.1.2 编程模型DStream 584
11.1.3 持久化、容错和优化 588
11.2 Spark Streaming实战 589
11.2.1 源码解析 589
11.2.2 Spark Streaming实战案例 600
第12章　Spark多语言编程 605
12.1 Spark多语言编程的特点 605
12.2 Spark编程模型 609
12.3 深入Spark多语言编程 611
12.4 Spark多语言编程综合实例 622
第13章　R语言的分布式编程之SparkR 627
13.1 R语言快速入门 627
13.1.1 R语言是什么 627
13.1.2 R语言的特点 629
13.1.3 R语言的安装 630
13.1.4 R的核心概念 630
13.1.5 R动手实战 631
13.2 使用SparkR 661
13.2.1 SparkR的安装 661
13.2.2 使用SparkR编写WordCount 662
13.2.3 使用SparkR的更多代码示例 662
第14章　Spark性能调优和最佳实践 665
14.1 Spark性能调优 665
14.1.1 Spark性能优化的12大问题及其解决方法 665
14.1.2 Spark内存优化 669
14.1.3 RDD分区 672
14.1.4 Spark性能优化实例 674
14.2 Spark性能调优细节 675
14.2.1 broadcast和accumulator 675
14.2.2 reduce 和 reduceByKey 676
14.2.3 深入reduceByKey 677
第15章　Spark源码解析 679
15.1 BlockManager源码解析 679
15.2 Cache源码解析 707
15.3 Checkpoint源码解析 725
附录A　动手实战Scala三部曲 733
第一部动手体验Scala 735
第二部　动手实战Scala面向对象编程 746
第三部动手实战Scala函数式编程 761
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>大数据Spark企业级实战 : 决胜大数据时代Spark全系列书籍
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理：技术、应用与性能优化
前　言
第1章　Spark简介 1
1.1　Spark是什么 1
1.2　Spark生态系统BDAS 4
1.3　Spark架构 6
1.4　Spark分布式架构与单机多核
架构的异同 9
1.5　Spark的企业级应用 10
1.5.1　Spark在Amazon中的应用 11
1.5.2　Spark在Yahoo!的应用 15
1.5.3　Spark在西班牙电信的应用 17
1.5.4　Spark在淘宝的应用 18
1.6　本章小结 20
第2章　Spark集群的安装与部署 21
2.1　Spark的安装与部署 21
2.1.1　在Linux集群上安装与配置Spark 21
2.1.2　在Windows上安装与配置Spark 30
2.2　Spark集群初试 33
2.3　本章小结 35
第3章　Spark计算模型 36
3.1　Spark程序模型 36
3.2　弹性分布式数据集 37
3.2.1　RDD简介 38
3.2.2　RDD与分布式共享内存的异同 38
3.2.3　Spark的数据存储 39
3.3　Spark算子分类及功能 41
3.3.1　Value型Transformation算子 42
3.3.2　Key-Value型Transformation算子 49
3.3.3　Actions算子 53
3.4　本章小结 59
第4章　Spark工作机制详解 60
4.1　Spark应用执行机制 60
4.1.1　Spark执行机制总览 60
4.1.2　Spark应用的概念 62
4.1.3　应用提交与执行方式 63
4.2　Spark调度与任务分配模块 65
4.2.1　Spark应用程序之间的调度 66
4.2.2　Spark应用程序内Job的调度 67
4.2.3　Stage和TaskSetManager调度方式 72
4.2.4　Task调度 74
4.3　Spark I/O机制 77
4.3.1　序列化 77
4.3.2　压缩 78
4.3.3　Spark块管理 80
4.4　Spark通信模块 93
4.4.1　通信框架AKKA 94
4.4.2　Client、Master和Worker间的通信 95
4.5　容错机制 104
4.5.1　Lineage机制 104
4.5.2　Checkpoint机制 108
4.6　Shuffle机制 110
4.7　本章小结 119
第5章　Spark开发环境配置及流程 120
5.1　Spark应用开发环境配置 120
5.1.1　使用Intellij开发Spark程序 120
5.1.2　使用Eclipse开发Spark程序 125
5.1.3　使用SBT构建Spark程序 129
5.1.4　使用Spark Shell开发运行Spark程序 130
5.2　远程调试Spark程序 130
5.3　Spark编译 132
5.4　配置Spark源码阅读环境 135
5.5　本章小结 135
第6章　Spark编程实战 136
6.1　WordCount 136
6.2　Top K 138
6.3　中位数 140
6.4　倒排索引 141
6.5　CountOnce 143
6.6　倾斜连接 144
6.7　股票趋势预测 146
6.8　本章小结 153
第7章　Benchmark使用详解 154
7.1　Benchmark简介 154
7.1.1　Intel Hibench与Berkeley BigDataBench 155
7.1.2　Hadoop GridMix 157
7.1.3　Bigbench、BigDataBenchmark与TPC-DS 158
7.1.4　其他Benchmark 161
7.2　Benchmark的组成 162
7.2.1　数据集 162
7.2.2　工作负载 163
7.2.3　度量指标 167
7.3　Benchmark的使用 168
7.3.1　使用Hibench 168
7.3.2　使用TPC-DS 170
7.3.3　使用BigDataBench 172
7.4　本章小结 176
第8章　BDAS简介 177
8.1　SQL on Spark 177
8.1.1　使用Spark SQL的原因 178
8.1.2　Spark SQL架构分析 179
8.1.3　Shark简介 182
8.1.4　Hive on Spark 184
8.1.5　未来展望 185
8.2　Spark Streaming 185
8.2.1　Spark Streaming简介 186
8.2.2　Spark Streaming架构 188
8.2.3　Spark Streaming原理剖析 189
8.2.4　Spark Streaming调优 198
8.2.5　Spark Streaming 实例 198
8.3　GraphX 205
8.3.1　GraphX简介 205
8.3.2　GraphX的使用 206
8.3.3　GraphX架构 209
8.3.4　运行实例 211
8.4　MLlib 215
8.4.1　MLlib简介 217
8.4.2　MLlib的数据存储 219
8.4.3　数据转换为向量（向量空间模型VSM） 222
8.4.4　MLlib中的聚类和分类 223
8.4.5　算法应用实例 228
8.4.6　利用MLlib进行电影推荐 230
8.5　本章小结 237
第9章　Spark性能调优 238
9.1　配置参数 238
9.2　调优技巧 239
9.2.1　调度与分区优化 240
9.2.2　内存存储优化 243
9.2.3　网络传输优化 249
9.2.4　序列化与压缩 251
9.2.5　其他优化方法 253
9.3　本章小结 255
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark大数据处理：技术、应用与性能优化
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>PySpark实战指南 : 利用Python和Spark构建数据密集型应用并规模化部署
Contents?目 录
译者序
序
前言
关于作者
第1章 了解Spark 1
1.1 什么是Apache Spark 1
1.2 Spark作业和API 2
1.2.1 执行过程 2
1.2.2 弹性分布式数据集 3
1.2.3 DataFrame 4
1.2.4 Dataset 5
1.2.5 Catalyst优化器 5
1.2.6 钨丝计划 5
1.3 Spark 2.0的架构 6
1.3.1 统一Dataset和DataFrame 7
1.3.2 SparkSession介绍 8
1.3.3 Tungsten Phase 2 8
1.3.4 结构化流 10
1.3.5 连续应用 10
1.4 小结 11
第2章 弹性分布式数据集 12
2.1 RDD的内部运行方式 12
2.2 创建RDD 13
2.2.1 Schema 14
2.2.2 从文件读取 14
2.2.3 Lambda表达式 15
2.3 全局作用域和局部作用域 16
2.4 转换 17
2.4.1 .map(...)转换 17
2.4.2 .filter(...)转换 18
2.4.3 .flatMap(...)转换 18
2.4.4 .distinct(...)转换 18
2.4.5 .sample(...)转换 19
2.4.6 .leftOuterJoin(...)转换 19
2.4.7 .repartition(...)转换 20
2.5 操作 20
2.5.1 .take(...)方法 21
2.5.2 .collect(...)方法 21
2.5.3 .reduce(...)方法 21
2.5.4 .count(...)方法 22
2.5.5 .saveAsTextFile(...)方法 22
2.5.6 .foreach(...)方法 23
2.6 小结 23
第3章 DataFrame 24
3.1 Python到RDD之间的通信 24
3.2 Catalyst优化器刷新 25
3.3 利用DataFrame加速PySpark 27
3.4 创建DataFrame 28
3.4.1 生成自己的JSON数据 29
3.4.2 创建一个DataFrame 29
3.4.3 创建一个临时表 30
3.5 简单的DataFrame查询 31
3.5.1 DataFrame API查询 32
3.5.2 SQL查询 32
3.6 RDD的交互操作 33
3.6.1 使用反射来推断模式 33
3.6.2 编程指定模式 34
3.7 利用DataFrame API查询 35
3.7.1 行数 35
3.7.2 运行筛选语句 35
3.8 利用SQL查询 36
3.8.1 行数 36
3.8.2 利用where子句运行筛选语句 36
3.9 DataFrame场景——实时飞行性能 38
3.9.1 准备源数据集 38
3.9.2 连接飞行性能和机场 39
3.9.3 可视化飞行性能数据 40
3.10 Spark数据集（Dataset）API 41
3.11 小结 42
第4章 准备数据建模 43
4.1 检查重复数据、未观测数据和异常数据（离群值） 43
4.1.1 重复数据 43
4.1.2 未观测数据 46
4.1.3 离群值 50
4.2 熟悉你的数据 51
4.2.1 描述性统计 52
4.2.2 相关性 54
4.3 可视化 55
4.3.1 直方图 55
4.3.2 特征之间的交互 58
4.4 小结 60
第5章 MLlib介绍 61
5.1 包概述 61
5.2 加载和转换数据 62
5.3 了解你的数据 65
5.3.1 描述性统计 66
5.3.2 相关性 67
5.3.3 统计测试 69
5.4 创建最终数据集 70
5.4.1 创建LabeledPoint形式的RDD 70
5.4.2 分隔培训和测试数据 71
5.5 预测婴儿生存机会 71
5.5.1 MLlib中的逻辑回归 71
5.5.2 只选择最可预测的特征 72
5.5.3 MLlib中的随机森林 73
5.6 小结 74
第6章 ML包介绍 75
6.1 包的概述 75
6.1.1 转换器 75
6.1.2 评估器 78
6.1.3 管道 80
6.2 使用ML预测婴儿生存几率 80
6.2.1 加载数据 80
6.2.2 创建转换器 81
6.2.3 创建一个评估器 82
6.2.4 创建一个管道 82
6.2.5 拟合模型 83
6.2.6 评估模型的性能 84
6.2.7 保存模型 84
6.3 超参调优 85
6.3.1 网格搜索法 85
6.3.2 Train-validation 划分 88
6.4 使用PySpark ML的其他功能 89
6.4.1 特征提取 89
6.4.2 分类 93
6.4.3 聚类 95
6.4.4 回归 98
6.5 小结 99
第7章 GraphFrames 100
7.1 GraphFrames介绍 102
7.2 安装GraphFrames 102
7.2.1 创建库 103
7.3 准备你的航班数据集 105
7.4 构建图形 107
7.5 执行简单查询 108
7.5.1 确定机场和航班的数量 108
7.5.2 确定这个数据集中的最长延误时间 108
7.5.3 确定延误和准点/早到航班的数量对比 109
7.5.4 哪一班从西雅图出发的航班最有可能出现重大延误 109
7.5.5 西雅图出发到哪个州的航班最有可能出现重大延误 110
7.6 理解节点的度 110
7.7 确定最大的中转机场 112
7.8 理解Motif 113
7.9 使用PageRank确定机场排名 114
……
第8章 TensorFrames 120
8.1 深度学习是什么 120
8.1.1 神经网络和深度学习的必要性 123
8.1.2 特征工程是什么 125
8.1.3 桥接数据和算法 125
8.2 TensorFlow是什么 127
8.2.1 安装PIP 129
8.2.2 安装TensorFlow 129
8.2.3 使用常量进行矩阵乘法 130
8.2.4 使用placeholder进行矩阵乘法 131
8.2.5 讨论 132
8.3 TensorFrames介绍 133
8.4 TensorFrames快速入门 134
8.4.1 配置和设置 134
8.4.2 使用TensorFlow向已有列添加常量 136
8.4.3 Blockwise reducing操作示例 137
8.5 小结 139
第9章 使用Blaze实现混合持久化
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>PySpark实战指南 : 利用Python和Spark构建数据密集型应用并规模化部署
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法 : Hadoop/Spark大数据处理技巧
序 1
前言 3
第1章二次排序：简介 19
二次排序问题解决方案 21
MapReduce/Hadoop的二次排序解决方案 25
Spark的二次排序解决方案 29
第2章二次排序：详细示例 42
二次排序技术 43
二次排序的完整示例 46
运行示例——老版本Hadoop API 50
运行示例——新版本Hadoop API 52
第3章 Top 10 列表 54
Top N 设计模式的形式化描述 55
MapReduce/Hadoop实现：唯一键 56
Spark实现：唯一键 62
Spark实现：非唯一键 73
使用takeOrdered()的Spark Top 10 解决方案 84
MapReduce/Hadoop Top 10 解决方案：非唯一键 91
第4章左外连接 96
左外连接示例 96
MapReduce左外连接实现 99
Spark左外连接实现 105
使用leftOuterJoin()的Spark实现 117
第5章反转排序 127
反转排序模式示例 128
反转排序模式的MapReduce/Hadoop实现 129
运行示例 134
第6章移动平均 137
示例1：时间序列数据（股票价格） 137
示例2：时间序列数据（URL访问数） 138
形式定义 139
POJO移动平均解决方案 140
MapReduce/Hadoop移动平均解决方案 143
第7章购物篮分析 155
MBA目标 155
MBA的应用领域 157
使用MapReduce的购物篮分析 157
Spark解决方案 166
运行Spark实现的YARN 脚本 179
第8章共同好友 182
输入 183
POJO共同好友解决方案 183
MapReduce算法 184
解决方案1: 使用文本的Hadoop实现 187
解决方案2: 使用ArrayListOfLongsWritable 的Hadoop实现 189
Spark解决方案 191
第9章使用MapReduce实现推荐引擎 201
购买过该商品的顾客还购买了哪些商品 202
经常一起购买的商品 206
推荐连接 210
第10章基于内容的电影推荐 225
输入 226
MapReduce阶段1 226
MapReduce阶段2和阶段3 227
Spark电影推荐实现 234
第11章使用马尔可夫模型的智能邮件营销 .253
马尔可夫链基本原理 254
使用MapReduce的马尔可夫模型 256
Spark解决方案 269
第12章 K-均值聚类 282
什么是K-均值聚类? 285
聚类的应用领域 285
K-均值聚类方法非形式化描述：分区方法 286
K-均值距离函数 286
K-均值聚类形式化描述 287
K-均值聚类的MapReduce解决方案 288
K-均值算法Spark实现 292
第13章 k-近邻 296
kNN分类 297
距离函数 297
kNN示例 298
kNN算法非形式化描述 299
kNN算法形式化描述 299
kNN的类Java非MapReduce 解决方案 299
Spark的kNN算法实现 301
第14章朴素贝叶斯 315
训练和学习示例 316
条件概率 319
深入分析朴素贝叶斯分类器 319
朴素贝叶斯分类器：符号数据的MapReduce解决方案 322
朴素贝叶斯分类器Spark实现 332
使用Spark和Mahout 347
第15章情感分析 349
情感示例 350
情感分数：正面或负面 350
一个简单的MapReduce情感分析示例 351
真实世界的情感分析 353
第16章查找、统计和列出大图中的所有三角形 354
基本的图概念 355
三角形计数的重要性 356
MapReduce/Hadoop解决方案 357
Spark解决方案 364
第17章 K-mer计数 375
K-mer计数的输入数据 376
K-mer计数应用 376
K-mer计数MapReduce/Hadoop解决方案 377
K-mer计数Spark解决方案 378
第18章 DNA测序 390
DNA测序的输入数据 392
输入数据验证 393
DNA序列比对 393
DNA测试的MapReduce算法 394
第19章 Cox回归 413
Cox模型剖析 414
使用R的Cox回归 415
Cox回归应用 416
Cox回归 POJO解决方案 417
MapReduce输入 418
使用MapReduce的Cox回归 419
第20章 Cochran-Armitage趋势检验 426
Cochran-Armitage算法 427
Cochran-Armitage应用 432
MapReduce解决方案 435
第21章等位基因频率 443
基本定义 444
形式化问题描述 448
等位基因频率分析的MapReduce解决方案 449
MapReduce解决方案, 阶段1 449
MapReduce解决方案，阶段2 459
MapReduce解决方案, 阶段3 463
染色体X 和Y的特殊处理 466
第22章 T检验 468
对bioset完成T检验 469
MapReduce问题描述 472
输入 472
期望输出 473
MapReduce解决方案 473
Spark实现 476
第23章皮尔逊相关系数 488
皮尔逊相关系数公式 489
皮尔逊相关系数示例 491
皮尔逊相关系数数据集 492
皮尔逊相关系数POJO 解决方案 492
皮尔逊相关系数MapReduce解决方案 493
皮尔逊相关系数的Spark 解决方案 496
运行Spark程序的YARN 脚本 516
使用Spark计算斯皮尔曼相关系数 517
第24章 DNA碱基计数 520
FASTA 格式 521
FASTQ 格式 522
MapReduce解决方案：FASTA 格式 522
运行示例 524
MapReduce解决方案: FASTQ 格式 528
Spark 解决方案: FASTA 格式 533
Spark解决方案: FASTQ 格式 537
第25章 RNA测序 543
数据大小和格式 543
MapReduce工作流 544
RNA测序分析概述 544
RNA测序MapReduce算法 548
第26章基因聚合 553
输入 554
输出 554
MapReduce解决方案（按单个值过滤和按平均值过滤） 555
基因聚合的Spark解决方案 567
Spark解决方案：按单个值过滤 567
Spark解决方案：按平均值过滤 576
第27章线性回归 586
基本定义 587
简单示例 587
问题描述 588
输入数据 589
期望输出 590
使用SimpleRegression的MapReduce解决方案 590
Hadoop实现类 593
使用R线性模型的MapReduce解决方案 593
第28章 MapReduce和幺半群 600
概述 600
幺半群的定义 602
幺半群和非幺半群示例 603
MapReduce示例：非幺半群 606
MapReduce示例：幺半群 608
使用幺半群的Spark示例 612
使用幺半群的结论 618
函子和幺半群 619
第29章小文件问题 622
解决方案1：在客户端合并小文件 623
解决方案2：用CombineFileInputFormat解决小文件问题 629
其他解决方案 634
第30章 MapReduce的大容量缓存 635
实现方案 636
缓存问题形式化描述 637
一个精巧、可伸缩的解决方案 637
实现LRUMap缓存 640
使用LRUMap的MapReduce解决方案 646
第31章 Bloom过滤器 651Bloom
过滤器性质 651
一个简单的Bloom过滤器示例 653
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>数据算法 : Hadoop/Spark大数据处理技巧
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解SPARK : 核心思想与源码分析
前言
准　备　篇
第1章　环境准备 2
1.1　运行环境准备 2
1.1.1　安装JDK 3
1.1.2　安装Scala 3
1.1.3　安装Spark 4
1.2　Spark初体验 4
1.2.1　运行spark-shell 4
1.2.2　执行word count 5
1.2.3　剖析spark-shell 7
1.3　阅读环境准备 11
1.4　Spark源码编译与调试 13
1.5　小结 17
第2章　Spark设计理念与基本架构 18
2.1　初识Spark 18
2.1.1　Hadoop MRv1的局限 18
2.1.2　Spark使用场景 20
2.1.3　Spark的特点 20
2.2　Spark基础知识 20
2.3　Spark基本设计思想 22
2.3.1　Spark模块设计 22
2.3.2　Spark模型设计 24
2.4　Spark基本架构 25
2.5　小结 26
核心设计篇
第3章　SparkContext的初始化 28
3.1　SparkContext概述 28
3.2　创建执行环境SparkEnv 30
3.2.1　安全管理器SecurityManager 31
3.2.2　基于Akka的分布式消息系统ActorSystem 31
3.2.3　map任务输出跟踪器mapOutputTracker 32
3.2.4　实例化ShuffleManager 34
3.2.5　shuffle线程内存管理器ShuffleMemoryManager 34
3.2.6　块传输服务BlockTransferService 35
3.2.7　BlockManagerMaster介绍 35
3.2.8　创建块管理器BlockManager 36
3.2.9　创建广播管理器Broadcast-Manager 36
3.2.10　创建缓存管理器CacheManager 37
3.2.11　HTTP文件服务器HttpFile-Server 37
3.2.12　创建测量系统MetricsSystem 39
3.2.13　创建SparkEnv 40
3.3　创建metadataCleaner 41
3.4　SparkUI详解 42
3.4.1　listenerBus详解 43
3.4.2　构造JobProgressListener 46
3.4.3　SparkUI的创建与初始化 47
3.4.4　Spark UI的页面布局与展示 49
3.4.5　SparkUI的启动 54
3.5　Hadoop相关配置及Executor环境变量 54
3.5.1　Hadoop相关配置信息 54
3.5.2　Executor环境变量 54
3.6　创建任务调度器TaskScheduler 55
3.6.1　创建TaskSchedulerImpl 55
3.6.2　TaskSchedulerImpl的初始化 57
3.7　创建和启动DAGScheduler 57
3.8　TaskScheduler的启动 60
3.8.1　创建LocalActor 60
3.8.2　ExecutorSource的创建与注册 62
3.8.3　ExecutorActor的构建与注册 64
3.8.4　Spark自身ClassLoader的创建 64
3.8.5　启动Executor的心跳线程 66
3.9　启动测量系统MetricsSystem 69
3.9.1　注册Sources 70
3.9.2　注册Sinks 70
3.9.3　给Sinks增加Jetty的Servlet-ContextHandler 71
3.10　创建和启动ExecutorAllocation-Manager 72
3.11　ContextCleaner的创建与启动 73
3.12　Spark环境更新 74
3.13　创建DAGSchedulerSource和BlockManagerSource 76
3.14　将SparkContext标记为激活 77
3.15　小结 78
第4章　存储体系 79
4.1　存储体系概述 79
4.1.1　块管理器BlockManager的实现 79
4.1.2　Spark存储体系架构 81
4.2　shuffle服务与客户端 83
4.2.1　Block的RPC服务 84
4.2.2　构造传输上下文Transpor-tContext 85
4.2.3　RPC客户端工厂Transport-ClientFactory 86
4.2.4　Netty服务器TransportServer 87
4.2.5　获取远程shuffle文件 88
4.2.6　上传shuffle文件 89
4.3　BlockManagerMaster对Block-Manager的管理 90
4.3.1　BlockManagerMasterActor 90
4.3.2　询问Driver并获取回复方法 92
4.3.3　向BlockManagerMaster注册BlockManagerId 93
4.4　磁盘块管理器DiskBlockManager 94
4.4.1　DiskBlockManager的构造过程 94
4.4.2　获取磁盘文件方法getFile 96
4.4.3　创建临时Block方法create-TempShuffleBlock 96
4.5　磁盘存储DiskStore 97
4.5.1　NIO读取方法getBytes 97
4.5.2　NIO写入方法putBytes 98
4.5.3　数组写入方法putArray 98
4.5.4　Iterator写入方法putIterator 98
4.6　内存存储MemoryStore 99
4.6.1　数据存储方法putBytes 101
4.6.2　Iterator写入方法putIterator详解 101
4.6.3　安全展开方法unrollSafely 102
4.6.4　确认空闲内存方法ensureFreeSpace 105
4.6.5　内存写入方法putArray 107
4.6.6　尝试写入内存方法tryToPut 108
4.6.7　获取内存数据方法getBytes 109
4.6.8　获取数据方法getValues 110
4.7　Tachyon存储TachyonStore 110
4.7.1　Tachyon简介 111
4.7.2　TachyonStore的使用 112
4.7.3　写入Tachyon内存的方法putIntoTachyonStore 113
4.7.4　获取序列化数据方法getBytes 113
4.8　块管理器BlockManager 114
4.8.1　移出内存方法dropFrom-Memory 114
4.8.2　状态报告方法reportBlockStatus 116
4.8.3　单对象块写入方法putSingle 117
4.8.4　序列化字节块写入方法putBytes 118
4.8.5　数据写入方法doPut 118
4.8.6　数据块备份方法replicate 121
4.8.7　创建DiskBlockObjectWriter的方法getDiskWriter 125
4.8.8　获取本地Block数据方法getBlockData 125
4.8.9　获取本地shuffle数据方法doGetLocal 126
4.8.10　获取远程Block数据方法doGetRemote 127
4.8.11　获取Block数据方法get 128
4.8.12　数据流序列化方法dataSerializeStream 129
4.9　metadataCleaner和broadcastCleaner 129
4.10　缓存管理器CacheManager 130
4.11　压缩算法 133
4.12　磁盘写入实现DiskBlockObjectWriter 133
4.13　块索引shuffle管理器IndexShuffleBlockManager 135
4.14　shuffle内存管理器ShuffleMemoryManager 137
4.15　小结 138
第5章　任务提交与执行 139
5.1　任务概述 139
5.2　广播Hadoop的配置信息 142
5.3　RDD转换及DAG构建 144
5.3.1　为什么需要RDD 144
5.3.2　RDD实现分析 146
5.4　任务提交 152
5.4.1　任务提交的准备 152
5.4.2　finalStage的创建与Stage的划分 157
5.4.3　创建Job 163
5.4.4　提交Stage 164
5.4.5　提交Task 165
5.5　执行任务 176
5.5.1　状态更新 176
5.5.2　任务还原 177
5.5.3　任务运行 178
5.6　任务执行后续处理 179
5.6.1　计量统计与执行结果序列化 179
5.6.2　内存回收 180
5.6.3　执行结果处理 181
5.7　小结 187
第6章　计算引擎 188
6.1　迭代计算 188
6.2　什么是shuffle 192
6.3　map端计算结果缓存处理 194
6.3.1　map端计算结果缓存聚合 195
6.3.2　map端计算结果简单缓存 200
6.3.3　容量限制 201
6.4　map端计算结果持久化 204
6.4.1　溢出分区文件 205
6.4.2排序与分区分组 207
6.4.3　分区索引文件 209
6.5　reduce端读取中间计算结果 210
6.5.1　获取map任务状态 213
6.5.2　划分本地与远程Block 215
6.5.3　获取远程Block 217
6.5.4　获取本地Block 218
6.6　reduce端计算 219
6.6.1　如何同时处理多个map任务的中间结果 219
6.6.2　reduce端在缓存中对中间计算结果执行聚合和排序 220
6.7　map端与reduce端组合分析 221
6.7.1　在map端溢出分区文件，在reduce端合并组合 221
6.7.2　在map端简单缓存、排序分组，在reduce端合并组合 222
6.7.3　在map端缓存中聚合、排序分组，在reduce端组合 222
6.8　小结 223
第7章　部署模式 224
7.1　local部署模式 225
7.2　local-cluster部署模式 225
7.2.1　LocalSparkCluster的启动 226
7.2.2　CoarseGrainedSchedulerBackend的启动 236
7.2.3　启动AppClient 237
7.2.4　资源调度 242
7.2.5　local-cluster模式的任务执行 253
7.3　Standalone部署模式 255
7.3.1　启动Standalone模式 255
7.3.2　启动Master分析 257
7.3.3　启动Worker分析 259
7.3.4　启动Driver Application分析 261
7.3.5　Standalone模式的任务执行 263
7.3.6　资源回收 263
7.4　容错机制 266
7.4.1　Executor异常退出 266
7.4.2　Worker异常退出 268
7.4.3　Master异常退出 269
7.5　其他部署方案 276
7.5.1　YARN 277
7.5.2　Mesos 280
7.6　小结 282
扩　展　篇
第8章　Spark SQL 284
8.1　Spark SQL总体设计 284
8.1.1　传统关系型数据库SQL运行原理 285
8.1.2　Spark SQL运行架构 286
8.2　字典表Catalog 288
8.3　Tree和TreeNode 289
8.4　词法解析器Parser的设计与实现 293
8.4.1　SQL语句解析的入口 294
8.4.2　建表语句解析器DDLParser 295
8.4.3　SQL语句解析器SqlParser 296
8.4.4　Spark代理解析器SparkSQLParser 299
8.5　Rule和RuleExecutor 300
8.6　Analyzer与Optimizer的设计与实现 302
8.6.1　语法分析器Analyzer 304
8.6.2　优化器Optimizer 305
8.7　生成物理执行计划 306
8.8　执行物理执行计划 308
8.9　Hive 311
8.9.1　Hive SQL语法解析器 311
8.9.2　Hive SQL元数据分析 313
8.9.3　Hive SQL物理执行计划 314
8.10　应用举例：JavaSparkSQL 314
8.11　小结 320
第9章　流式计算 321
9.1　Spark Streaming总体设计 321
9.2　StreamingContext初始化 323
9.3　输入流接收器规范Receiver 324
9.4　数据流抽象DStream 325
9.4.1　Dstream的离散化 326
9.4.2　数据源输入流InputDStream 327
9.4.3　Dstream转换及构建DStream Graph 329
9.5　流式计算执行过程分析 330
9.5.1　流式计算例子CustomReceiver 331
9.5.2　Spark Streaming执行环境构建 335
9.5.3　任务生成过程 347
9.6　窗口操作 355
9.7　应用举例 357
9.7.1　安装mosquitto 358
9.7.2　启动mosquitto 358
9.7.3　MQTTWordCount 359
9.8　小结 361
第10章　图计算 362
10.1　Spark GraphX总体设计 362
10.1.1　图计算模型 363
10.1.2　属性图 365
10.1.3　GraphX的类继承体系 367
10.2　图操作 368
10.2.1　属性操作 368
10.2.2　结构操作 368
10.2.3　连接操作 369
10.2.4　聚合操作 370
10.3　Pregel API 371
10.3.1　Dijkstra算法 373
10.3.2　Dijkstra的实现 376
10.4　Graph的构建 377
10.4.1　从边的列表加载Graph 377
10.4.2　在Graph中创建图的方法 377
10.5　顶点集合抽象VertexRDD 378
10.6　边集合抽象EdgeRDD 379
10.7　图分割 380
10.8　常用算法 382
10.8.1　网页排名 382
10.8.2　Connected Components的应用 386
10.8.3　三角关系统计 388
10.9　应用举例 390
10.10　小结 391
第11章　机器学习 392
11.1机器学习概论 392
11.2　Spark MLlib总体设计 394
11.3　数据类型 394
11.3.1　局部向量 394
11.3.2标记点 395
11.3.3局部矩阵 396
11.3.4分布式矩阵 396
11.4基础统计 398
11.4.1摘要统计 398
11.4.2相关统计 399
11.4.3分层抽样 401
11.4.4假设检验 401
11.4.5随机数生成 402
11.5分类和回归 405
11.5.1数学公式 405
11.5.2线性回归 407
11.5.3分类 407
11.5.4回归 410
11.6决策树 411
11.6.1基本算法 411
11.6.2使用例子 412
11.7随机森林 413
11.7.1基本算法 414
11.7.2使用例子 414
11.8梯度提升决策树 415
11.8.1基本算法 415
11.8.2使用例子 416
11.9朴素贝叶斯 416
11.9.1算法原理 416
11.9.2使用例子 418
11.10保序回归 418
11.10.1算法原理 418
11.10.2使用例子 419
11.11协同过滤 419
11.12聚类 420
11.12.1K-means 420
11.12.2高斯混合 422
11.12.3快速迭代聚类 422
11.12.4latent Dirichlet allocation 422
11.12.5流式K-means 423
11.13维数减缩 424
11.13.1奇异值分解 424
11.13.2主成分分析 425
11.14特征提取与转型 425
11.14.1术语频率反转 425
11.14.2单词向量转换 426
11.14.3标准尺度 427
11.14.4正规化尺度 428
11.14.5卡方特征选择器 428
11.14.6Hadamard积 429
11.15频繁模式挖掘 429
11.16预言模型标记语言 430
11.17管道 431
11.17.1管道工作原理 432
11.17.2管道API介绍 433
11.17.3交叉验证 435
11.18小结 436
附录A　Utils 437
附录B　Akka 446
附录C　Jetty 450
附录D　Metrics 453
附录E　Hadoop word count 456
附录F　CommandUtils 458
附录G　Netty 461
附录H　源码编译错误 465
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>深入理解SPARK : 核心思想与源码分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark技术内幕 : 深入解析Spark内核架构设计与实现原理
序
前言
第1章　Spark简介1
1.1Spark的技术背景1
1.2Spark的优点2
1.3Spark架构综述4
1.4Spark核心组件概述5
1.4.1Spark Streaming5
1.4.2MLlib6
1.4.3Spark SQL7
1.4.4　GraphX8
1.5Spark的整体代码结构规模8
第2章　Spark学习环境的搭建9
2.1源码的获取与编译9
2.1.1源码获取9
2.1.2源码编译10
2.2构建Spark的源码阅读环境11
2.3小结15
第3章　RDD实现详解16
3.1概述16
3.2什么是RDD17
3.2.1RDD的创建19
3.2.2RDD的转换20
3.2.3　RDD的动作22
3.2.4RDD的缓存23
3.2.5RDD的检查点24
3.3RDD的转换和DAG的生成25
3.3.1RDD的依赖关系26
3.3.2DAG的生成30
3.3.3Word Count的RDD转换和DAG划分的逻辑视图30
3.4RDD的计算33
3.4.1Task简介33
3.4.2Task的执行起点33
3.4.3缓存的处理35
3.4.4checkpoint的处理37
3.4.5RDD的计算逻辑39
3.5RDD的容错机制39
3.6小结40
第4章　Scheduler 模块详解41
4.1模块概述41
4.1.1整体架构41
4.1.2Scheduler的实现概述43
4.2DAGScheduler实现详解45
4.2.1DAGScheduler的创建46
4.2.2Job的提交48
4.2.3Stage的划分49
4.2.4任务的生成54
4.3任务调度实现详解57
4.3.1TaskScheduler的创建57
4.3.2Task的提交概述58
4.3.3任务调度具体实现61
4.3.4Task运算结果的处理65
4.4Word Count调度计算过程详解72
4.5小结74
第5章　Deploy模块详解76
5.1　Spark运行模式概述76
5.1.1　local77
5.1.2Mesos78
5.1.3YARN82
5.2模块整体架构86
5.3消息传递机制详解87
5.3.1Master和Worker87
5.3.2Master和Client89
5.3.3Client和Executor91
5.4集群的启动92
5.4.1Master的启动92
5.4.2Worker的启动96
5.5集群容错处理98
5.5.1Master 异常退出98
5.5.2Worker异常退出99
5.5.3Executor异常退出101
5.6Master HA实现详解102
5.6.1Master启动的选举和数据恢复策略103
5.6.2集群启动参数的配置105
5.6.3Curator Framework简介 106
5.6.4ZooKeeperLeaderElectionAgent的实现109
5.7小结110
第6章　Executor模块详解112
6.1Standalone模式的Executor分配详解113
6.1.1SchedulerBackend创建AppClient114
6.1.2AppClient向Master注册Application116
6.1.3Master根据AppClient的提交选择Worker119
6.1.4Worker根据Master的资源分配结果创建Executor121
6.2Task的执行122
6.2.1依赖环境的创建和分发123
6.2.2任务执行125
6.2.3任务结果的处理128
6.2.4Driver端的处理130
6.3　参数设置131
6.3.1　spark.executor.memory131
6.3.2日志相关132
6.3.3spark.executor.heartbeatInterval132
6.4小结133
第7章　Shuffle模块详解134
7.1Hash Based Shuffle Write135
7.1.1Basic Shuffle Writer实现解析136
7.1.2存在的问题138
7.1.3Shuffle Consolidate Writer139
7.1.4小结140
7.2Shuffle Pluggable 框架141
7.2.1org.apache.spark.shuffle.ShuffleManager141
7.2.2org.apache.spark.shuffle.ShuffleWriter143
7.2.3org.apache.spark.shuffle.ShuffleBlockManager143
7.2.4org.apache.spark.shuffle.ShuffleReader144
7.2.5如何开发自己的Shuffle机制144
7.3Sort Based Write144
7.4Shuffle Map Task运算结果的处理148
7.4.1Executor端的处理148
7.4.2Driver端的处理150
7.5Shuffle Read152
7.5.1整体流程152
7.5.2数据读取策略的划分155
7.5.3本地读取156
7.5.4远程读取158
7.6性能调优160
7.6.1spark.shuffle.manager160
7.6.2spark.shuffle.spill162
7.6.3spark.shuffle.memoryFraction和spark.shuffle.safetyFraction162
7.6.4spark.shuffle.sort.bypassMergeThreshold 163
7.6.5spark.shuffle.blockTransferService 163
7.6.6spark.shuffle.consolidateFiles 163
7.6.7spark.shuffle.compress和 spark.shuffle.spill.compress164
7.6.8spark.reducer.maxMbInFlight165
7.7小结165
第8章　Storage模块详解167
8.1模块整体架构167
8.1.1整体架构167
8.1.2源码组织结构170
8.1.3Master 和Slave的消息传递详解173
8.2存储实现详解181
8.2.1存储级别181
8.2.2模块类图184
8.2.3org.apache.spark.storage.DiskStore实现详解186
8.2.4org.apache.spark.storage.MemoryStore实现详解188
8.2.5org.apache.spark.storage.TachyonStore实现详解189
8.2.6Block存储的实现190
8.3性能调优194
8.3.1spark.local.dir194
8.3.2spark.executor.memory194
8.3.3spark.storage.memoryFraction194
8.3.4spark.streaming.blockInterval195
8.4小结195
第9章　企业应用概述197
9.1Spark在百度197
9.1.1现状197
9.1.2百度开放云BMR的Spark198
9.1.3在Spark中使用Tachyon199
9.2Spark在阿里200
9.3Spark在腾讯200
9.4小结201
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark技术内幕 : 深入解析Spark内核架构设计与实现原理
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
第1章　Spark的环境搭建与运行　　1
1.1　Spark的本地安装与配置　　2
1.2　Spark集群　　3
1.3　Spark编程模型　　4
1.3.1　SparkContext类与SparkConf 类　　4
1.3.2　Spark shell　　5
1.3.3　弹性分布式数据集　　6
1.3.4　广播变量和累加器　　10
1.4　Spark Scala编程入门　　11
1.5　Spark Java编程入门　　14
1.6　Spark Python编程入门　　17
1.7　在Amazon EC2上运行Spark　　18
1.8　小结　　23
第2章　设计机器学习系统　　24
2.1　MovieStream介绍　　24
2.2　机器学习系统商业用例　　25
2.2.1　个性化　　26
2.2.2　目标营销和客户细分　　26
2.2.3　预测建模与分析　　26
2.3　机器学习模型的种类　　27
2.4　数据驱动的机器学习系统的组成　　27
2.4.1　数据获取与存储　　28
2.4.2　数据清理与转换　　28
2.4.3　模型训练与测试回路　　29
2.4.4　模型部署与整合　　30
2.4.5　模型监控与反馈　　30
2.4.6　批处理或实时方案的选择　　31
2.5　机器学习系统架构　　31
2.6　小结　　33
第3章　Spark上数据的获取、处理与准备　　34
3.1　获取公开数据集　　35
3.2　探索与可视化数据　　37
3.2.1　探索用户数据　　38
3.2.2　探索电影数据　　41
3.2.3　探索评级数据　　43
3.3　处理与转换数据　　46
3.4　从数据中提取有用特征　　48
3.4.1　数值特征　　48
3.4.2　类别特征　　49
3.4.3　派生特征　　50
3.4.4　文本特征　　51
3.4.5　正则化特征　　55
3.4.6　用软件包提取特征　　56
3.5　小结　　57
第4章　构建基于Spark的推荐引擎　　58
4.1　推荐模型的分类　　59
4.1.1　基于内容的过滤　　59
4.1.2　协同过滤　　59
4.1.3　矩阵分解　　60
4.2　提取有效特征　　64
4.3　训练推荐模型　　67
4.3.1　使用MovieLens 100k数据集训练模型　　67
4.3.2　使用隐式反馈数据训练模型　　68
4.4　使用推荐模型　　69
4.4.1　用户推荐　　69
4.4.2　物品推荐　　72
4.5　推荐模型效果的评估　　75
4.5.1　均方差　　75
4.5.2　K值平均准确率　　77
4.5.3　使用MLlib内置的评估函数　　81
4.6　小结　　82
第5章　Spark构建分类模型　　83
5.1　分类模型的种类　　85
5.1.1　线性模型　　85
5.1.2　朴素贝叶斯模型　　89
5.1.3　决策树　　90
5.2　从数据中抽取合适的特征　　91
5.3　训练分类模型　　93
5.4　使用分类模型　　95
5.5　评估分类模型的性能　　96
5.5.1　预测的正确率和错误率　　96
5.5.2　准确率和召回率　　97
5.5.3　ROC曲线和AUC　　99
5.6　改进模型性能以及参数调优　　101
5.6.1　特征标准化　　101
5.6.2　其他特征　　104
5.6.3　使用正确的数据格式　　106
5.6.4　模型参数调优　　107
5.7　小结　　115
第6章　Spark构建回归模型　　116
6.1　回归模型的种类　　116
6.1.1　最小二乘回归　　117
6.1.2　决策树回归　　117
6.2　从数据中抽取合适的特征　　118
6.3　回归模型的训练和应用　　123
6.4　评估回归模型的性能　　125
6.4.1　均方误差和均方根误差　　125
6.4.2　平均绝对误差　　126
6.4.3　均方根对数误差　　126
6.4.4　R-平方系数　　126
6.4.5　计算不同度量下的性能　　126
6.5　改进模型性能和参数调优　　127
6.5.1　变换目标变量　　128
6.5.2　模型参数调优　　132
6.6　小结　　140
第7章　Spark构建聚类模型　　141
7.1　聚类模型的类型　　142
7.1.1　K-均值聚类　　142
7.1.2　混合模型　　146
7.1.3　层次聚类　　146
7.2　从数据中提取正确的特征　　146
7.3　训练聚类模型　　150
7.4　使用聚类模型进行预测　　151
7.5　评估聚类模型的性能　　155
7.5.1　内部评价指标　　155
7.5.2　外部评价指标　　156
7.5.3　在MovieLens数据集计算性能　　156
7.6　聚类模型参数调优　　156
7.7　小结　　158
第8章　Spark应用于数据降维　　159
8.1　降维方法的种类　　160
8.1.1　主成分分析　　160
8.1.2　奇异值分解　　160
8.1.3　和矩阵分解的关系　　161
8.1.4　聚类作为降维的方法　　161
8.2　从数据中抽取合适的特征　　162
8.3　训练降维模型　　169
8.4　使用降维模型　　172
8.4.1　在LFW数据集上使用PCA投影数据　　172
8.4.2　PCA和SVD模型的关系　　173
8.5　评价降维模型　　174
8.6　小结　　176
第9章　Spark高级文本处理技术　　177
9.1　处理文本数据有什么特别之处　　177
9.2　从数据中抽取合适的特征　　177
9.2.1　短语加权表示　　178
9.2.2　特征哈希　　179
9.2.3　从20新闻组数据集中提取TF-IDF特征　　180
9.3　使用TF-IDF模型　　192
9.3.1　20 Newsgroups数据集的文本相似度和TF-IDF特征　　192
9.3.2　基于20 Newsgroups数据集使用TF-IDF训练文本分类器　　194
9.4　评估文本处理技术的作用　　196
9.5　Word2Vec 模型　　197
9.6　小结　　200
第10章　Spark Streaming在实时机器学习上的应用　　201
10.1　在线学习　　201
10.2　流处理　　202
10.2.1　Spark Streaming介绍　　202
10.2.2　使用Spark Streaming缓存和容错　　205
10.3　创建Spark Streaming应用　　206
10.3.1　消息生成端　　207
10.3.2　创建简单的流处理程序　　209
10.3.3　流式分析　　211
10.3.4　有状态的流计算　　213
10.4　使用Spark Streaming进行在线学习　　215
10.4.1　流回归　　215
10.4.2　一个简单的流回归程序　　216
10.4.3　流K-均值　　220
10.5　在线模型评估　　221
10.6　小结　　224
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark机器学习
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析 : 基于Storm、Spark等Hadoop替代技术的实时应用
目录
前言
致谢
关于作者
1 引言：为什么要超越 Hadoop Map-Reduce 1
Hadoop的适用范围 3
大数据分析之机器学习实现的革命 10
第一代机器学习工具 /范式 11
第二代机器学习工具 /范式 11
第三代机器学习工具 /范式 14
小结 18
参考文献 19
2 何为伯克利数据分析栈（BDAS） 23
实现 BDAS的动机 24
Spark：动机 25
Shark：动机 26
Mesos：动机 28
BDAS的设计及架构 29
Spark：高效的集群数据处理的范式 34
Spark的弹性分布式数据集 36
Spark的实现 40
Spark VS. 分布式共享内存系统 42
RDD的表达性 44
类似 Spark的系统 45
Shark：分布式系统上的 SQL接口 46
Spark为 Shark提供的扩展 47
列内存存储 49
分布式数据加载 50
完全分区智能连接 50
分区修剪 50
机器学习的支持 51
Mesos：集群调度及管理系统 51
Mesos组件 52
资源分配 54
隔离 55
容错性 57
小结 58
参考文献 59
3 使用 Spark实现机器学习算法 66
机器学习基础知识 66
机器学习：随机森林示例 68
逻辑回归：概述 72
二元形式的逻辑回归 73
逻辑回归估计 75
多元逻辑回归 76
Spark中的逻辑回归算法 77
支持向量机 80
复杂决策面 81
支持向量机背后的数学原理 82
Spark中的支持向量机 84
Spark对 PMML的支持 85
PMML结构 87
PMML的生产者及消费者 92
Spark对朴素贝叶斯的 PMML支持 94
Spark对线性回归的 PMML支持 95
在 Spark中使用 MLbase进行机器学习 97
参考文献 99
4 实现实时的机器学习算法 101
Storm简介 101
数据流 103
拓扑 104
Storm集群 105
简单的实时计算例子 106
数据流组 108
Storm的消息处理担保 109
基于 Storm的设计模式 111
分布式远程过程调用 111
Trident：基于 Storm的实时聚合 115
实现基于 Storm的逻辑回归算法 116
实现基于 Storm的支持向量机算法 120
Storm对朴素贝叶斯 PMML的支持 122
实时分析的应用 126
工业日志分类 126
互联网流量过滤器 130
Storm的替代品 131
Spark流 133
D-Streams的动机 133
参考文献 135
5 图处理范式 138
Pregel：基于 BSP的图处理框架 139
类似的做法 141
开源的 Pregel实现 143
Giraph 143
GoldenORB 145
Phoebus 145
Apache Hama 146
Stanford GPS 146
GraphLab 147
GraphLab：多核版本 148
分布式的 GraphLab 150
PowerGraph 152
通过 GraphLab实现网页排名算法 156
顶点程序 158
基于 GraphLab实现随机梯度下降算法 163
参考文献 167
6 结论：超越Hadoop Map-Reduce的大数据分析 171
Hadoop YARN概览 172
Hadoop YARN的动机 172
作为资源调度器的 YARN 174
YARN上的其他框架 175
大数据分析的未来是怎样的 177
参考文献 180
附录A 代码笔记 182
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>颠覆大数据分析 : 基于Storm、Spark等Hadoop替代技术的实时应用
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark高级数据分析
推荐序　　ix
译者序　　xi
序　　xiii
前言　　xv
第1章　大数据分析　　1
1.1　数据科学面临的挑战　　2
1.2　认识Apache Spark　　4
1.3　关于本书　　5
第2章　用Scala和Spark进行数据分析　　7
2.1　数据科学家的Scala　　8
2.2　Spark 编程模型　　9
2.3　记录关联问题　　9
2.4　小试牛刀：Spark shell和SparkContext　　10
2.5　把数据从集群上获取到客户端　　15
2.6　把代码从客户端发送到集群　　18
2.7　用元组和case class对数据进行结构化　　19
2.8　聚合　　23
2.9　创建直方图　　24
2.10　连续变量的概要统计　　25
2.11　为计算概要信息创建可重用的代码　　26
2.12　变量的选择和评分简介　　30
2.13　小结　　31
第3章　音乐推荐和Audioscrobbler数据集　　33
3.1　数据集　　34
3.2　交替最小二乘推荐算法　　35
3.3　准备数据　　37
3.4　构建第一个模型　　39
3.5　逐个检查推荐结果　　42
3.6　评价推荐质量　　43
3.7　计算AUC　　44
3.8　选择超参数　　46
3.9　产生推荐　　48
3.10　小结　　49
第4章　 用决策树算法预测森林植被　　51
4.1　回归简介　　52
4.2　向量和特征　　52
4.3　样本训练　　53
4.4　决策树和决策森林　　54
4.5　Covtype数据集　　56
4.6　准备数据　　57
4.7　第一棵决策树　　58
4.8　决策树的超参数　　62
4.9　决策树调优　　63
4.10　重谈类别型特征　　65
4.11　随机决策森林　　67
4.12　进行预测　　69
4.13　小结　　69
第5章　基于K均值聚类的网络流量异常检测　　71
5.1　异常检测　　72
5.2　K均值聚类　　72
5.3　网络入侵　　73
5.4　KDD Cup 1999数据集　　73
5.5　初步尝试聚类　　74
5.6　K 的选择　　76
5.7　基于R的可视化　　79
5.8　特征的规范化　　81
5.9　类别型变量　　83
5.10　利用标号的熵信息　　84
5.11　聚类实战　　85
5.12　小结　　86
第6章　基于潜在语义分析算法分析维基百科　　89
6.1　词项-文档矩阵　　90
6.2　获取数据　　91
6.3　分析和准备数据　　92
6.4　词形归并　　93
6.5　计算TF-IDF　　94
6.6　奇异值分解　　97
6.7　找出重要的概念　　98
6.8　基于低维近似的查询和评分　　101
6.9　词项-词项相关度　　102
6.10　文档-文档相关度　　103
6.11　词项-文档相关度　　105
6.12　多词项查询　　106
6.13　小结　　107
第7章　用GraphX分析伴生网络　　109
7.1　对MEDLINE文献引用索引的网络分析　　110
7.2　获取数据　　111
7.3　用Scala XML工具解析XML文档　　113
7.4　分析MeSH主要主题及其伴生关系　　114
7.5　用GraphX来建立一个伴生网络　　116
7.6　理解网络结构　　119
7.6.1　连通组件　　119
7.6.2　度的分布　　122
7.7　过滤噪声边　　124
7.7.1　处理EdgeTriplet　　125
7.7.2　分析去掉噪声边的子图　　126
7.8　小世界网络　　127
7.8.1　系和聚类系数　　128
7.8.2　用Pregel计算平均路径长度　　129
7.9　小结　　133
第8章　纽约出租车轨迹的空间和时间数据分析　　135
8.1　数据的获取　　136
8.2　基于Spark的时间和空间数据分析　　136
8.3　基于JodaTime和NScalaTime的时间数据处理　　137
8.4　基于Esri Geometry API和Spray的地理空间数据处理　　138
8.4.1　认识Esri Geometry API　　139
8.4.2　GeoJSON简介　　140
8.5　纽约市出租车客运数据的预处理　　142
8.5.1　大规模数据中的非法记录处理　　143
8.5.2　地理空间分析　　147
8.6　基于Spark的会话分析　　149
8.7　小结　　153
第9章　基于蒙特卡罗模拟的金融风险评估　　155
9.1　术语　　156
9.2　VaR计算方法　　157
9.2.1　方差-协方差法　　157
9.2.2　历史模拟法　　157
9.2.3　蒙特卡罗模拟法　　157
9.3　我们的模型　　158
9.4　获取数据　　158
9.5　数据预处理　　159
9.6　确定市场因素的权重　　162
9.7　采样　　164
9.8　运行试验　　167
9.9　回报分布的可视化　　170
9.10　结果的评估　　171
9.11　小结　　173
第10章　基因数据分析和BDG项目　　175
10.1　分离存储与模型　　176
10.2　用ADAM CLI导入基因学数据　　178
10.3　从ENCODE数据预测转录因子结合位点　　185
10.4　查询1000 Genomes项目中的基因型　　191
10.5　小结　　193
第11章　基于PySpark和Thunder的神经图像数据分析　　195
11.1　PySpark简介　　196
11.2　Thunder工具包概况和安装　　199
11.3　用Thunder加载数据　　200
11.4　用Thunder对神经元进行分类　　207
11.5　小结　　211
附录A　Spark进阶　　213
附录B　即将发布的MLlib Pipelines API　　221
作者介绍　　226
封面介绍　　226
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Spark高级数据分析
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Apache Spark源码剖析
第一部分Spark概述1
第1章初识Spark 3
1.1 大数据和Spark 3
1.1.1 大数据的由来4
1.1.2 大数据的分析4
1.1.3 Hadoop 5
1.1.4 Spark简介6
1.2 与Spark的第一次亲密接触7
1.2.1 环境准备7
1.2.2 下载安装Spark 8
1.2.3 Spark下的WordCount 8
第二部分Spark核心概念13
第2章Spark整体框架 15
2.1 编程模型15
2.1.1 RDD 17
2.1.2 Operation 17
2.2 运行框架18
2.2.1 作业提交18
2.2.2 集群的节点构成18
2.2.3 容错处理19
2.2.4 为什么是Scala 19
2.3 源码阅读环境准备19
2.3.1 源码下载及编译19
2.3.2 源码目录结构21
2.3.3 源码阅读工具21
2.3.4 本章小结22
第3章SparkContext初始化 23
3.1 spark-shell 23
3.2 SparkContext的初始化综述27
3.3 Spark Repl综述30
3.3.1 Scala Repl执行过程31
3.3.2 Spark Repl 32
第4章Spark作业提交 33
4.1 作业提交33
4.2 作业执行38
4.2.1 依赖性分析及Stage划分39
4.2.2 Actor Model和Akka 46
4.2.3 任务的创建和分发47
4.2.4 任务执行53
4.2.5 Checkpoint和Cache 62
4.2.6 WebUI和Metrics 62
4.3 存储机制71
4.3.1 Shuffle结果的写入和读取71
4.3.2 Memory Store 80
4.3.3 存储子模块启动过程分析81
4.3.4 数据写入过程分析82
4.3.5 数据读取过程分析84
4.3.6 TachyonStore 88
第5章部署方式分析 91
5.1 部署模型91
5.2 单机模式local 92
5.3 伪集群部署local-cluster 93
5.4 原生集群Standalone Cluster 95
5.4.1 启动Master 96
5.4.2 启动Worker 97
5.4.3 运行spark-shell 102
5.4.4 容错性分析106
5.5 Spark On YARN 112
5.5.1 YARN的编程模型112
5.5.2 YARN中的作业提交112
5.5.3 Spark On YARN实现详解113
5.5.4 SparkPi on YARN 122
第三部分Spark Lib 129
第6章Spark Streaming 131
6.1 Spark Streaming整体架构131
6.1.1 DStream 132
6.1.2 编程接口133
6.1.3 Streaming WordCount 134
6.2 Spark Streaming执行过程135
6.2.1 StreamingContext初始化过程136
6.2.2 数据接收141
6.2.3 数据处理146
6.2.4 BlockRDD 155
6.3 窗口操作158
6.4 容错性分析159
6.5 Spark Streaming vs. Storm 165
6.5.1 Storm简介165
6.5.2 Storm和Spark Streaming对比168
6.6 应用举例168
6.6.1 搭建Kafka Cluster 168
6.6.2 KafkaWordCount 169
第7章SQL 173
7.1 SQL语句的通用执行过程分析175
7.2 SQL On Spark的实现分析178
7.2.1 SqlParser 178
7.2.2 Analyzer 184
7.2.3 Optimizer 191
7.2.4 SparkPlan 192
7.3 Parquet 文件和JSON数据集196
7.4 Hive简介197
7.4.1 Hive 架构197
7.4.2 HiveQL On MapReduce执行过程分析199
7.5 HiveQL On Spark详解200
7.5.1 Hive On Spark环境搭建206
7.5.2 编译支持Hadoop 2.x的Spark 211
7.5.3 运行Hive On Spark测试用例213
第8章GraphX 215
8.1 GraphX简介215
8.1.1 主要特点216
8.1.2 版本演化216
8.1.3 应用场景217
8.2 分布式图计算处理技术介绍218
8.2.1 属性图218
8.2.2 图数据的存储与分割219
8.3 Pregel计算模型220
8.3.1 BSP 220
8.3.2 像顶点一样思考220
8.4 GraphX图计算框架实现分析223
8.4.1 基本概念223
8.4.2 图的加载与构建226
8.4.3 图数据存储与分割227
8.4.4 操作接口228
8.4.5 Pregel在GraphX中的源码实现230
8.5 PageRank 235
8.5.1 什么是PageRank 235
8.5.2 PageRank核心思想235
第9章MLLib 239
9.1 线性回归239
9.1.1 数据和估计240
9.1.2 线性回归参数求解方法240
9.1.3 正则化245
9.2 线性回归的代码实现246
9.2.1 简单示例246
9.2.2 入口函数train 247
9.2.3 最优化算法optimizer 249
9.2.4 权重更新update 256
9.2.5 结果预测predict 257
9.3 分类算法257
9.3.1 逻辑回归258
9.3.2 支持向量机260
9.4 拟牛顿法261
9.4.1 数学原理261
9.4.2 代码实现265
9.5 MLLib与其他应用模块间的整合268
第四部分附录271
附录A Spark源码调试 273
附录B 源码阅读技巧 283
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Apache Spark源码剖析
